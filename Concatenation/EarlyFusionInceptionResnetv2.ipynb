{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf1cf7d-e12a-469e-b3c6-c37dee98db7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)  # If you are using CUDA\n",
    "torch.backends.cudnn.deterministic = True  # For deterministic results\n",
    "torch.backends.cudnn.benchmark = False  # For consistency across different environments\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "IMAGE_DIR = 'D:\\\\PAD-UFES\\\\images'  \n",
    "METADATA_PATH = 'D:\\\\PAD-UFES\\\\metadata.csv'\n",
    "\n",
    "metadata = pd.read_csv(METADATA_PATH)\n",
    "\n",
    "def preprocess_metadata(metadata):\n",
    "    metadata = metadata.fillna('UNK')\n",
    "\n",
    "    boolean_cols = [\n",
    "        'smoke',\n",
    "        'drink',\n",
    "        'pesticide',\n",
    "        'skin_cancer_history',\n",
    "        'cancer_history',\n",
    "        'has_piped_water',\n",
    "        'has_sewage_system',\n",
    "        'itch',\n",
    "        'grew',\n",
    "        'hurt',\n",
    "        'changed',\n",
    "        'bleed',\n",
    "        'elevation',\n",
    "        'biopsed',\n",
    "    ]\n",
    "    # Ensure columns are strings and lowercase\n",
    "    for col in boolean_cols:\n",
    "        metadata[col] = metadata[col].astype(str).str.lower()\n",
    "    \n",
    "    # Map boolean columns to 1/0/-1\n",
    "    boolean_mapping = {'true': 1, 'false': 0, 'unk': -1}\n",
    "    for col in boolean_cols:\n",
    "        metadata[col] = metadata[col].map(boolean_mapping)\n",
    "    \n",
    "    # Handle categorical variables\n",
    "    categorical_cols = [\n",
    "        'background_father',\n",
    "        'background_mother',\n",
    "        'gender',\n",
    "        'region',\n",
    "        'diagnostic',\n",
    "    ]\n",
    "    # Convert categorical columns to string\n",
    "    for col in categorical_cols:\n",
    "        metadata[col] = metadata[col].astype(str)\n",
    "    \n",
    "    # One-hot encode categorical variables\n",
    "    metadata_encoded = pd.get_dummies(metadata[categorical_cols])\n",
    "    \n",
    "    # Normalize numerical variables\n",
    "    numerical_cols = ['age', 'fitspatrick', 'diameter_1', 'diameter_2']\n",
    "    # Ensure numerical columns are numeric\n",
    "    for col in numerical_cols:\n",
    "        metadata[col] = pd.to_numeric(metadata[col], errors='coerce')\n",
    "    # Fill NaNs in numerical columns with the mean\n",
    "    metadata[numerical_cols] = metadata[numerical_cols].fillna(metadata[numerical_cols].mean())\n",
    "    # Scale numerical columns\n",
    "    scaler = StandardScaler()\n",
    "    metadata_numeric = metadata[numerical_cols]\n",
    "    metadata_numeric_scaled = pd.DataFrame(\n",
    "        scaler.fit_transform(metadata_numeric), columns=numerical_cols\n",
    "    )\n",
    "    \n",
    "    # Combine all metadata features\n",
    "    metadata_processed = pd.concat(\n",
    "        [metadata_numeric_scaled.reset_index(drop=True),\n",
    "         metadata_encoded.reset_index(drop=True),\n",
    "         metadata[boolean_cols].reset_index(drop=True)], axis=1\n",
    "    )\n",
    "    \n",
    "    return metadata_processed\n",
    "\n",
    "# Preprocess metadata\n",
    "metadata_processed = preprocess_metadata(metadata)\n",
    "\n",
    "def get_image_paths(metadata, image_dir):\n",
    "    image_paths = []\n",
    "    for idx, row in metadata.iterrows():\n",
    "        filename = row['img_id']\n",
    "        # Ensure filename is a string\n",
    "        filename = str(filename)\n",
    "        # Check if filename has an extension\n",
    "        if not filename.endswith(('.jpg', '.jpeg', '.png')):\n",
    "            # Try common extensions\n",
    "            possible_extensions = ['.jpg', '.jpeg', '.png']\n",
    "            found = False\n",
    "            for ext in possible_extensions:\n",
    "                filepath = os.path.join(image_dir, filename + ext)\n",
    "                if os.path.isfile(filepath):\n",
    "                    image_paths.append(filepath)\n",
    "                    found = True\n",
    "                    break\n",
    "            if not found:\n",
    "                print(f\"Image file not found for ID: {filename}\")\n",
    "                image_paths.append(None)\n",
    "        else:\n",
    "            filepath = os.path.join(image_dir, filename)\n",
    "            if os.path.isfile(filepath):\n",
    "                image_paths.append(filepath)\n",
    "            else:\n",
    "                print(f\"Image file not found: {filepath}\")\n",
    "                image_paths.append(None)\n",
    "    metadata['ImagePath'] = image_paths\n",
    "    return metadata\n",
    "\n",
    "metadata = get_image_paths(metadata, IMAGE_DIR)\n",
    "\n",
    "# Remove entries with missing images\n",
    "metadata = metadata[metadata['ImagePath'].notnull()]\n",
    "metadata_processed = metadata_processed.loc[metadata.index].reset_index(drop=True)\n",
    "metadata = metadata.reset_index(drop=True)\n",
    "\n",
    "# Drop diagnostic-related columns from features\n",
    "diagnostic_cols = ['diagnostic_ACK', 'diagnostic_BCC', 'diagnostic_MEL', 'diagnostic_NEV', 'diagnostic_SCC', 'diagnostic_SEK']\n",
    "metadata_processed = metadata_processed.drop(columns=diagnostic_cols)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(metadata['diagnostic'])\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# Split data into features and labels\n",
    "X_meta = metadata_processed.reset_index(drop=True)\n",
    "X_img_paths = metadata['ImagePath'].reset_index(drop=True)\n",
    "y = pd.Series(y_encoded)\n",
    "\n",
    "X_train_meta, X_temp_meta, X_train_img_paths, X_temp_img_paths, y_train, y_temp = train_test_split(\n",
    "    X_meta,\n",
    "    X_img_paths,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "X_val_meta, X_test_meta, X_val_img_paths, X_test_img_paths, y_val, y_test = train_test_split(\n",
    "    X_temp_meta,\n",
    "    X_temp_img_paths,\n",
    "    y_temp,\n",
    "    test_size=0.5,\n",
    "    random_state=42,\n",
    "    stratify=y_temp\n",
    ")\n",
    "\n",
    "# Load augmented metadata + image paths\n",
    "aug_meta_df   = pd.read_csv(\"D:/PAD-UFES/augmented_metadata.csv\")\n",
    "aug_labels_df = pd.read_csv(\"D:/PAD-UFES/augmented_labels.csv\")\n",
    "\n",
    "# Combine augmented samples with training set\n",
    "X_train_meta_final = pd.concat([X_train_meta, aug_meta_df], ignore_index=True)\n",
    "X_train_img_paths_final = pd.concat([X_train_img_paths.reset_index(drop=True),\n",
    "                                     aug_labels_df['ImagePath']], ignore_index=True)\n",
    "y_train_final = pd.concat([y_train.reset_index(drop=True),\n",
    "                           aug_labels_df['Label']], ignore_index=True)\n",
    "\n",
    "class PADUFESDataset(Dataset):\n",
    "    def __init__(self, img_paths, meta_data, labels, transform=None):\n",
    "        self.img_paths = img_paths.reset_index(drop=True)\n",
    "        self.meta_data = meta_data.reset_index(drop=True)\n",
    "        self.labels = pd.Series(labels).reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        meta = torch.tensor(self.meta_data.iloc[idx].values.astype(np.float32))\n",
    "        label = torch.tensor(self.labels.iloc[idx], dtype=torch.long)\n",
    "        return image, meta, label\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # Random horizontal flip\n",
    "    transforms.RandomRotation(70),          \n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Color jitter\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = PADUFESDataset(X_train_img_paths_final, X_train_meta_final, y_train_final, transform=train_transform)\n",
    "val_dataset = PADUFESDataset(X_val_img_paths, X_val_meta, y_val, transform=val_test_transform)\n",
    "test_dataset = PADUFESDataset(X_test_img_paths, X_test_meta, y_test, transform=val_test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"\\nâœ… Loaded Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a339d5-2fb3-4f98-9bae-828fb9723443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode class labels\n",
    "train_labels_decoded = label_encoder.inverse_transform(y_train_final)\n",
    "val_labels_decoded = label_encoder.inverse_transform(y_val)\n",
    "test_labels_decoded = label_encoder.inverse_transform(y_test)\n",
    "\n",
    "# Count class instances\n",
    "train_class_counts = pd.Series(train_labels_decoded).value_counts()\n",
    "val_class_counts = pd.Series(val_labels_decoded).value_counts()\n",
    "test_class_counts = pd.Series(test_labels_decoded).value_counts()\n",
    "\n",
    "# Print distributions\n",
    "print(\"\\nðŸ“Š Class distribution in Augmented Training Set:\")\n",
    "print(train_class_counts)\n",
    "\n",
    "print(\"\\nðŸ“Š Class distribution in Validation Set:\")\n",
    "print(val_class_counts)\n",
    "\n",
    "print(\"\\nðŸ“Š Class distribution in Test Set:\")\n",
    "print(test_class_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64ef42d-7600-41d1-9614-b289b6199865",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Data\n",
    "optimizers = ['Adam', 'SGD', 'RMSprop', 'Adagrad']\n",
    "val_accuracy = [87.35, 68.97, 85.38, 78.26]\n",
    "val_f1 = [87.27, 68.24, 85.40, 77.90]\n",
    "\n",
    "# Bar positions\n",
    "x = np.arange(len(optimizers))\n",
    "bar_width = 0.35\n",
    "\n",
    "# Viridis color palette for all bars (2 bars per optimizer)\n",
    "cmap = plt.cm.viridis\n",
    "colors = [cmap(i / (len(optimizers) * 2)) for i in range(len(optimizers) * 2)]\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Draw bars with unique colors\n",
    "bars1 = ax.bar(x - bar_width/2, val_accuracy, width=bar_width, label='Val Accuracy', color=colors[0::2])\n",
    "bars2 = ax.bar(x + bar_width/2, val_f1, width=bar_width, label='Val F1 Score', color=colors[1::2])\n",
    "\n",
    "# Axes and title\n",
    "ax.set_xlabel('Optimizers', fontweight='bold')\n",
    "ax.set_ylabel('Validation Score (%)', fontweight='bold')\n",
    "ax.set_title('Optimizer Comparison: Validation Accuracy vs F1 Score', fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(optimizers, fontweight='bold')\n",
    "ax.tick_params(axis='y', labelsize=10)\n",
    "ax.tick_params(axis='x', labelsize=10)\n",
    "# ax.legend()\n",
    "\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "custom_legend = [\n",
    "    Patch(color=colors[0], label='Adam - Acc'),\n",
    "    Patch(color=colors[1], label='Adam - F1'),\n",
    "    Patch(color=colors[2], label='SGD - Acc'),\n",
    "    Patch(color=colors[3], label='SGD - F1'),\n",
    "    Patch(color=colors[4], label='RMSprop - Acc'),\n",
    "    Patch(color=colors[5], label='RMSprop - F1'),\n",
    "    Patch(color=colors[6], label='Adagrad - Acc'),\n",
    "    Patch(color=colors[7], label='Adagrad - F1')\n",
    "]\n",
    "ax.legend(handles=custom_legend, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height:.2f}',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "# Layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807d1368-2281-48e7-b46d-29e1d8dfcbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "# Data\n",
    "batch_sizes = ['16', '32', '64']\n",
    "val_accuracy = [85.38, 87.15, 85.77]\n",
    "val_f1 = [85.31, 87.10, 85.46]\n",
    "\n",
    "# Bar positions\n",
    "x = np.arange(len(batch_sizes))\n",
    "bar_width = 0.35\n",
    "\n",
    "# Use magma colormap for 6 unique bars\n",
    "cmap = plt.cm.magma\n",
    "colors = [cmap(i / 6) for i in range(6)]\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bars1 = ax.bar(x - bar_width/2, val_accuracy, width=bar_width, label='Val Accuracy', color=colors[0::2])\n",
    "bars2 = ax.bar(x + bar_width/2, val_f1, width=bar_width, label='Val F1 Score', color=colors[1::2])\n",
    "\n",
    "# Axis formatting\n",
    "ax.set_xlabel('Batch Size', fontweight='bold')\n",
    "ax.set_ylabel('Validation Score (%)', fontweight='bold')\n",
    "ax.set_title('Batch Size vs Validation Accuracy and F1 Score', fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(batch_sizes, fontweight='bold')\n",
    "ax.tick_params(axis='x', labelsize=10)\n",
    "ax.tick_params(axis='y', labelsize=10)\n",
    "\n",
    "# Custom Legend\n",
    "custom_legend = [\n",
    "    Patch(color=colors[0], label='Batch Size 16 - Acc'),\n",
    "    Patch(color=colors[1], label='Batch Size 16 - F1'),\n",
    "    Patch(color=colors[2], label='Batch Size 32 - Acc'),\n",
    "    Patch(color=colors[3], label='Batch Size 32 - F1'),\n",
    "    Patch(color=colors[4], label='Batch Size 64 - Acc'),\n",
    "    Patch(color=colors[5], label='Batch Size 64 - F1'),\n",
    "]\n",
    "ax.legend(handles=custom_legend, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# Annotate exact values\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height:.2f}',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "# Layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1286fc50-d249-462e-8878-d01d6549aaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data\n",
    "temperature = [1, 3, 6, 9]\n",
    "accuracy = [0.875494, 0.859684, 0.865613, 0.877470]\n",
    "f1_score = [0.874773, 0.859024, 0.864872, 0.876967]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(temperature, accuracy, marker='o', label='Validation Accuracy', linewidth=4)\n",
    "plt.plot(temperature, f1_score, marker='s', label='Validation F1 Score', linewidth=4)\n",
    "\n",
    "# Axis and Title Formatting\n",
    "plt.xlabel('Temperature', fontweight='bold')\n",
    "plt.ylabel('Validation Score (%)', fontweight='bold')\n",
    "plt.title('Effect of Temperature on Accuracy and F1 Score', fontweight='bold')\n",
    "\n",
    "# Axis Ticks and Grid\n",
    "plt.xticks(temperature, fontweight='bold')\n",
    "plt.yticks(fontweight='bold')\n",
    "\n",
    "# Legend\n",
    "plt.legend()\n",
    "\n",
    "# Optional: Annotate data points\n",
    "for t, acc, f1 in zip(temperature, accuracy, f1_score):\n",
    "    plt.text(t, acc + 0.001, f'{acc:.3f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "    plt.text(t, f1 - 0.002, f'{f1:.3f}', ha='center', va='top', fontsize=9, fontweight='bold')\n",
    "\n",
    "# Optional: Annotate data points\n",
    "for t, acc, f1 in zip(temperature, accuracy, f1_score):\n",
    "    plt.text(t, acc + 0.001, f'{acc:.3f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "    plt.text(t, f1 - 0.002, f'{f1:.3f}', ha='center', va='top', fontsize=9, fontweight='bold')\n",
    "\n",
    "# Adjust y-axis limits to keep annotations visible\n",
    "min_y = min(min(accuracy), min(f1_score)) - 0.005\n",
    "max_y = max(max(accuracy), max(f1_score)) + 0.005\n",
    "plt.ylim(min_y, max_y)\n",
    "\n",
    "# Layout and show\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b646ca0b-056a-47a6-bf91-3e234db56195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data\n",
    "alpha = [0.2, 0.4, 0.6, 0.8]\n",
    "accuracy = [0.871542, 0.867589, 0.865613, 0.867589]\n",
    "f1_score = [0.869651, 0.868283, 0.865297, 0.865305]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(alpha, accuracy, marker='o', label='Val Accuracy', linewidth=3)\n",
    "plt.plot(alpha, f1_score, marker='s', label='Val F1 Score', linewidth=3)\n",
    "\n",
    "# Connect Accuracy and F1 points with lines per alpha\n",
    "for a, acc, f1 in zip(alpha, accuracy, f1_score):\n",
    "    plt.plot([a, a], [acc, f1], color='gray', linestyle='--', linewidth=1.5)\n",
    "\n",
    "# Annotate values\n",
    "for a, acc, f1 in zip(alpha, accuracy, f1_score):\n",
    "    plt.text(a, acc + 0.001, f'{acc:.3f}', ha='center', fontsize=9, fontweight='bold')\n",
    "    plt.text(a, f1 - 0.002, f'{f1:.3f}', ha='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "# Axis formatting\n",
    "plt.xlabel('Alpha', fontweight='bold')\n",
    "plt.ylabel('Validation Score', fontweight='bold')\n",
    "plt.title('Accuracy vs F1 Score for Different Alpha Values', fontweight='bold')\n",
    "plt.xticks(alpha, fontweight='bold')\n",
    "plt.yticks(fontweight='bold')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959df813-d9a3-4fbc-b9f7-1b0f5cc8e4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data\n",
    "alpha = [0.2, 0.4, 0.6, 0.8]\n",
    "accuracy = [0.871542, 0.867589, 0.865613, 0.867589]\n",
    "f1_score = [0.869651, 0.868283, 0.865297, 0.865305]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot lines\n",
    "plt.plot(alpha, accuracy, marker='o', label='Val Accuracy', linewidth=3, color='purple')\n",
    "plt.plot(alpha, f1_score, marker='s', label='Val F1 Score', linewidth=3, color='magenta')\n",
    "\n",
    "# Fill area between the two lines\n",
    "plt.fill_between(alpha, accuracy, f1_score, color='violet', alpha=0.3, label='Difference Area')\n",
    "\n",
    "# Annotate points\n",
    "for a, acc, f1 in zip(alpha, accuracy, f1_score):\n",
    "    plt.text(a, acc + 0.001, f'{acc:.3f}', ha='center', fontsize=9, fontweight='bold')\n",
    "    plt.text(a, f1 - 0.002, f'{f1:.3f}', ha='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "# Axis formatting\n",
    "plt.xlabel('Alpha', fontweight='bold')\n",
    "plt.ylabel('Validation Score', fontweight='bold')\n",
    "plt.title('Validation Accuracy vs F1 Score with Alpha', fontweight='bold')\n",
    "plt.xticks(alpha, fontweight='bold')\n",
    "plt.yticks(fontweight='bold')\n",
    "plt.legend()\n",
    "\n",
    "# Adjust y-axis limits to keep annotations visible\n",
    "min_y = min(min(accuracy), min(f1_score)) - 0.005\n",
    "max_y = max(max(accuracy), max(f1_score)) + 0.005\n",
    "plt.ylim(min_y, max_y)\n",
    "\n",
    "# Layout and show\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b82d7e8-2d5d-412f-83f8-05b72f3cd330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert encoded labels back to original class names\n",
    "y_train_labels = label_encoder.inverse_transform(y_train)\n",
    "y_val_labels = label_encoder.inverse_transform(y_val)\n",
    "y_test_labels = label_encoder.inverse_transform(y_test)\n",
    "\n",
    "# Count instances of each class\n",
    "train_class_counts = pd.Series(y_train_labels).value_counts()\n",
    "val_class_counts = pd.Series(y_val_labels).value_counts()\n",
    "test_class_counts = pd.Series(y_test_labels).value_counts()\n",
    "\n",
    "# Print counts\n",
    "print(\"Class distribution in Training Set:\")\n",
    "print(train_class_counts)\n",
    "\n",
    "print(\"\\nClass distribution in Validation Set:\")\n",
    "print(val_class_counts)\n",
    "\n",
    "print(\"\\nClass distribution in Test Set:\")\n",
    "print(test_class_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948f75c2-ea9b-4ab3-b6af-910f0d213eb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T16:31:24.501259Z",
     "iopub.status.busy": "2025-03-29T16:31:24.500259Z",
     "iopub.status.idle": "2025-03-29T16:31:24.511322Z",
     "shell.execute_reply": "2025-03-29T16:31:24.511322Z"
    }
   },
   "outputs": [],
   "source": [
    "metadata_processed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd643e33-86a3-4de9-b06f-9c702a124a69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T16:31:24.513324Z",
     "iopub.status.busy": "2025-03-29T16:31:24.513324Z",
     "iopub.status.idle": "2025-03-29T16:31:24.515825Z",
     "shell.execute_reply": "2025-03-29T16:31:24.515825Z"
    }
   },
   "outputs": [],
   "source": [
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360bfa12-b774-4b8a-bae9-c0c698db5f98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T16:31:24.517827Z",
     "iopub.status.busy": "2025-03-29T16:31:24.517827Z",
     "iopub.status.idle": "2025-03-29T16:31:28.583115Z",
     "shell.execute_reply": "2025-03-29T16:31:28.583115Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm  \n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "\n",
    "class EarlyFusionModel(nn.Module):\n",
    "    def __init__(self, input_dim_meta, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embed metadata to smaller spatial dimensions\n",
    "        self.meta_embed = nn.Sequential(\n",
    "            nn.Linear(input_dim_meta, 56 * 56),  # Smaller initial dimension\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(56 * 56),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        # Load the InceptionResNetV2 model from timm\n",
    "        self.inception_resnet = timm.create_model('inception_resnet_v2', pretrained=True)\n",
    "        \n",
    "        # Modify the first convolutional block (conv2d_1a) to accept 4 channels\n",
    "        first_conv = self.inception_resnet.conv2d_1a.conv\n",
    "        self.inception_resnet.conv2d_1a.conv = nn.Conv2d(\n",
    "            4, 32, kernel_size=(3, 3), stride=(2, 2), bias=False\n",
    "        )\n",
    "        \n",
    "        # Copy weights for the first 3 channels and initialize the 4th channel\n",
    "        with torch.no_grad():\n",
    "            self.inception_resnet.conv2d_1a.conv.weight[:, :3] = first_conv.weight\n",
    "            # Initialize the new channel with the mean of existing weights\n",
    "            self.inception_resnet.conv2d_1a.conv.weight[:, 3:] = first_conv.weight.mean(dim=1, keepdim=True) * 0.1\n",
    "        \n",
    "        # Modify the final classification head to match the number of classes\n",
    "        in_features = self.inception_resnet.classif.in_features\n",
    "        self.inception_resnet.classif = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(in_features, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, img, meta):\n",
    "        # Reshape metadata to image-like format\n",
    "        batch_size = img.shape[0]\n",
    "        meta_reshaped = self.meta_embed(meta).view(batch_size, 1, 56, 56)\n",
    "        \n",
    "        # Upsample metadata to match image dimensions\n",
    "        meta_upsampled = F.interpolate(meta_reshaped, \n",
    "                                       size=(224, 224), \n",
    "                                       mode='bilinear', \n",
    "                                       align_corners=False)\n",
    "        \n",
    "        # Concatenate image and metadata along the channel dimension\n",
    "        combined_input = torch.cat([img, meta_upsampled], dim=1)\n",
    "        \n",
    "        # Process through the modified InceptionResNetV2\n",
    "        out = self.inception_resnet(combined_input)\n",
    "        return out\n",
    "        \n",
    "input_dim_meta = X_train_meta.shape[1]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = EarlyFusionModel(input_dim_meta, num_classes).to(device)\n",
    "\n",
    "from torchinfo import summary\n",
    "summary(model=model, \n",
    "        input_size=[(16, 3, 224, 224), (16, input_dim_meta)],  \n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a8b844-ef1a-44cd-8ad1-9b6d630b935a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T16:31:28.585116Z",
     "iopub.status.busy": "2025-03-29T16:31:28.585116Z",
     "iopub.status.idle": "2025-03-29T16:31:28.587632Z",
     "shell.execute_reply": "2025-03-29T16:31:28.587632Z"
    }
   },
   "outputs": [],
   "source": [
    "print(input_dim_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7279bf2a-d008-4abe-b887-366efe15d700",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T16:31:28.589634Z",
     "iopub.status.busy": "2025-03-29T16:31:28.589634Z",
     "iopub.status.idle": "2025-03-29T16:31:37.178926Z",
     "shell.execute_reply": "2025-03-29T16:31:37.178926Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt'):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_accuracy_max = -np.Inf\n",
    "        \n",
    "    def __call__(self, val_acc, model):\n",
    "        score = val_acc\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_acc, model)\n",
    "        elif score <= self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_acc, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_acc, model):\n",
    "        if self.verbose:\n",
    "            print(f'Validation accuracy increased ({self.val_accuracy_max:.6f} --> {val_acc:.6f}). Saving model...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_accuracy_max = val_acc\n",
    "\n",
    "from PIL import Image\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "def test(model, loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, metas, labels in loader:\n",
    "            imgs, metas, labels = imgs.to(device), metas.to(device), labels.to(device)\n",
    "            outputs = model(imgs, metas)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    return all_labels, all_preds\n",
    "\n",
    "true_labels, pred_labels = test(model, test_loader, device)\n",
    "\n",
    "def train(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc='Training')\n",
    "    for images, meta, labels in pbar:\n",
    "        images, meta, labels = images.to(device), meta.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images, meta)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Optional: Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{running_loss/total:.4f}',\n",
    "            'acc': f'{100.*correct/total:.2f}%'\n",
    "        })\n",
    "    \n",
    "    return running_loss/len(train_loader), correct/total\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, meta, labels in val_loader:\n",
    "            images, meta, labels = images.to(device), meta.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images, meta)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    return running_loss/len(val_loader), correct/total\n",
    "\n",
    "def train_model_with_scheduler_and_checkpoint(\n",
    "    model, train_loader, val_loader, optimizer, criterion, device, \n",
    "    epochs=20, patience=5, scheduler_patience=5, checkpoint_dir='checkpoints'):\n",
    "    \n",
    "    # Create checkpoint directory if it doesn't exist\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, 'inceptionresnetv2.pt')\n",
    "    \n",
    "    early_stopping = EarlyStopping(\n",
    "        patience=patience, \n",
    "        verbose=True, \n",
    "        path=checkpoint_path\n",
    "    )\n",
    "    scheduler = ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='max',  # Changed to max since we're monitoring accuracy\n",
    "        patience=scheduler_patience, \n",
    "        verbose=True,\n",
    "        factor=0.1,\n",
    "        min_lr=1e-6\n",
    "    )\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [], 'val_loss': [],\n",
    "        'train_acc': [], 'val_acc': [],\n",
    "        'lr': []\n",
    "    }\n",
    "    \n",
    "    best_model_epoch = None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f'\\nEpoch {epoch+1}/{epochs}')\n",
    "        \n",
    "        # Training phase\n",
    "        train_loss, train_acc = train(model, train_loader, optimizer, criterion, device)\n",
    "        \n",
    "        # Validation phase\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        # Update history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['lr'].append(optimizer.param_groups[0]['lr'])\n",
    "        \n",
    "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
    "        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "        \n",
    "        # Update scheduler based on validation accuracy\n",
    "        scheduler.step(val_acc)\n",
    "        \n",
    "        # Early stopping check\n",
    "        early_stopping(val_acc, model)\n",
    "        if val_acc > early_stopping.val_accuracy_max:\n",
    "            best_model_epoch = epoch + 1\n",
    "            \n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load(checkpoint_path))\n",
    "    \n",
    "    # Plot training curves\n",
    "    plot_training_curves_with_checkpoint(history, best_model_epoch)\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "def plot_training_curves_with_checkpoint(history, best_model_epoch):\n",
    "    epochs_range = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Loss curves\n",
    "    ax1.plot(epochs_range, history['train_loss'], label='Training Loss')\n",
    "    ax1.plot(epochs_range, history['val_loss'], label='Validation Loss')\n",
    "    if best_model_epoch:\n",
    "        ax1.axvline(best_model_epoch, color='r', linestyle='--', label='Best Model')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Training and Validation Loss')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Accuracy curves\n",
    "    ax2.plot(epochs_range, history['train_acc'], label='Training Accuracy')\n",
    "    ax2.plot(epochs_range, history['val_acc'], label='Validation Accuracy')\n",
    "    if best_model_epoch:\n",
    "        ax2.axvline(best_model_epoch, color='r', linestyle='--', label='Best Model')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.set_title('Training and Validation Accuracy')\n",
    "    ax2.legend()\n",
    "    \n",
    "    # Learning rate curve\n",
    "    ax3.plot(epochs_range, history['lr'], label='Learning Rate')\n",
    "    if best_model_epoch:\n",
    "        ax3.axvline(best_model_epoch, color='r', linestyle='--', label='Best Model')\n",
    "    ax3.set_xlabel('Epoch')\n",
    "    ax3.set_ylabel('Learning Rate')\n",
    "    ax3.set_title('Learning Rate Schedule')\n",
    "    ax3.set_yscale('log')\n",
    "    ax3.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c58d0f6-1109-4424-8d25-10152fbd18f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c647a86-b320-4cc8-93dd-da03b25c788b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T16:31:37.181928Z",
     "iopub.status.busy": "2025-03-29T16:31:37.180927Z",
     "iopub.status.idle": "2025-03-29T17:39:38.818465Z",
     "shell.execute_reply": "2025-03-29T17:39:38.818465Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "def set_random_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# Function to evaluate test metrics\n",
    "def evaluate_test_metrics(model, test_loader, device):\n",
    "    true_labels, pred_labels = test(model, test_loader, device)\n",
    "    acc = accuracy_score(true_labels, pred_labels)\n",
    "    precision = precision_score(true_labels, pred_labels, average='macro')\n",
    "    recall = recall_score(true_labels, pred_labels, average='macro')\n",
    "    f1 = f1_score(true_labels, pred_labels, average='macro')\n",
    "    return acc, precision, recall, f1\n",
    "\n",
    "# Placeholder for results\n",
    "results = {\n",
    "    \"accuracy\": [],\n",
    "    \"precision\": [],\n",
    "    \"recall\": [],\n",
    "    \"f1_score\": []\n",
    "}\n",
    "\n",
    "best_accuracy = 0.0\n",
    "best_model_state = None\n",
    "\n",
    "# Run experiment for 3 random seeds\n",
    "seeds = [42, 123, 569]  # Example random seeds\n",
    "for seed in seeds:\n",
    "    print(f\"\\nTraining with random seed: {seed}\")\n",
    "    set_random_seed(seed)\n",
    "    \n",
    "    # Reinitialize model, optimizer, and criterion\n",
    "    model = EarlyFusionModel(input_dim_meta, num_classes).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Train the model\n",
    "    model, history = train_model_with_scheduler_and_checkpoint(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        device=device,\n",
    "        epochs=100,\n",
    "        patience=7,\n",
    "        scheduler_patience=3,\n",
    "        checkpoint_dir='D:\\\\PAD-UFES\\\\checkpoints'\n",
    "    )\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    acc, precision, recall, f1 = evaluate_test_metrics(model, test_loader, device)\n",
    "    print(f\"Seed {seed}: Accuracy={acc:.4f}, Precision={precision:.4f}, Recall={recall:.4f}, F1 Score={f1:.4f}\")\n",
    "    \n",
    "    # Save metrics\n",
    "    results[\"accuracy\"].append(acc)\n",
    "    results[\"precision\"].append(precision)\n",
    "    results[\"recall\"].append(recall)\n",
    "    results[\"f1_score\"].append(f1)\n",
    "    \n",
    "    # Update the best model\n",
    "    if acc > best_accuracy:\n",
    "        best_accuracy = acc\n",
    "        best_model_state = model.state_dict()\n",
    "\n",
    "# Compute average and standard deviation\n",
    "metrics_summary = {}\n",
    "for metric, values in results.items():\n",
    "    avg = np.mean(values)\n",
    "    std_dev = np.std(values)\n",
    "    metrics_summary[metric] = (avg, std_dev)\n",
    "    print(f\"{metric.capitalize()}: Mean={avg:.4f}, StdDev={std_dev:.4f}\")\n",
    "\n",
    "# Save the best model\n",
    "print(f\"Best model achieved an accuracy of {best_accuracy:.4f}\")\n",
    "torch.save(best_model_state, 'D:\\\\PAD-UFES\\\\best_early_fusion_inceptionresnetv2smoteDA.pth')\n",
    "\n",
    "model = EarlyFusionModel(input_dim_meta, num_classes).to(device)\n",
    "model.load_state_dict(torch.load('D:\\\\PAD-UFES\\\\best_early_fusion_inceptionresnetv2smoteDA.pth'))\n",
    "model.eval()\n",
    "print(\"Best model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e445513-187f-4d9a-a981-8a88908ab80c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T17:39:38.820467Z",
     "iopub.status.busy": "2025-03-29T17:39:38.820467Z",
     "iopub.status.idle": "2025-03-29T17:39:44.664811Z",
     "shell.execute_reply": "2025-03-29T17:39:44.664811Z"
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "def test(model, loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, metas, labels in loader:\n",
    "            imgs, metas, labels = imgs.to(device), metas.to(device), labels.to(device)\n",
    "            outputs = model(imgs, metas)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    return all_labels, all_preds\n",
    "\n",
    "true_labels, pred_labels = test(model, test_loader, device)\n",
    "\n",
    "class_names = label_encoder.classes_\n",
    "report = classification_report(true_labels, pred_labels, digits=4,target_names=class_names)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "cm = confusion_matrix(true_labels, pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cd483f-39d8-4f5b-82fe-a8756484ad73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T17:39:44.666808Z",
     "iopub.status.busy": "2025-03-29T17:39:44.666808Z",
     "iopub.status.idle": "2025-03-29T17:39:44.880045Z",
     "shell.execute_reply": "2025-03-29T17:39:44.880045Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt=\".0f\", xticklabels = class_names, yticklabels = class_names)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c59dcc-4a74-43c5-804e-9cb0835d533b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T17:39:44.882047Z",
     "iopub.status.busy": "2025-03-29T17:39:44.882047Z",
     "iopub.status.idle": "2025-03-29T17:39:44.992994Z",
     "shell.execute_reply": "2025-03-29T17:39:44.992994Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "n_class = 6\n",
    "\n",
    "for i in range(n_class):\n",
    "    fpr[i], tpr[i], _ = roc_curve(np.array(true_labels) == i, np.array(pred_labels) == i)\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "colors = ['orange', 'green', 'red', 'blue', 'purple', 'pink']\n",
    "\n",
    "for i in range(n_class):\n",
    "    plt.plot(fpr[i], tpr[i], color=colors[i], lw=2, label=f'ROC curve (AUC = {roc_auc[i]:.2f}) for {class_names[i]}')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('(ROC)Receiver Operating Characteristic')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef91a5a5-07a3-4d3a-9c14-71799071074b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
