{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf1cf7d-e12a-469e-b3c6-c37dee98db7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)  # If you are using CUDA\n",
    "torch.backends.cudnn.deterministic = True  # For deterministic results\n",
    "torch.backends.cudnn.benchmark = False  # For consistency across different environments\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "IMAGE_DIR = 'D:\\\\PAD-UFES\\\\images'  \n",
    "METADATA_PATH = 'D:\\\\PAD-UFES\\\\metadata.csv'\n",
    "\n",
    "metadata = pd.read_csv(METADATA_PATH)\n",
    "\n",
    "\n",
    "def preprocess_metadata(metadata):\n",
    "    metadata = metadata.fillna('UNK')\n",
    "\n",
    "    boolean_cols = [\n",
    "        'smoke',\n",
    "        'drink',\n",
    "        'pesticide',\n",
    "        'skin_cancer_history',\n",
    "        'cancer_history',\n",
    "        'has_piped_water',\n",
    "        'has_sewage_system',\n",
    "        'itch',\n",
    "        'grew',\n",
    "        'hurt',\n",
    "        'changed',\n",
    "        'bleed',\n",
    "        'elevation',\n",
    "        'biopsed',\n",
    "    ]\n",
    "    # Ensure columns are strings and lowercase\n",
    "    for col in boolean_cols:\n",
    "        metadata[col] = metadata[col].astype(str).str.lower()\n",
    "    \n",
    "    # Map boolean columns to 1/0/-1\n",
    "    boolean_mapping = {'true': 1, 'false': 0, 'unk': -1}\n",
    "    for col in boolean_cols:\n",
    "        metadata[col] = metadata[col].map(boolean_mapping)\n",
    "    \n",
    "    # Handle categorical variables\n",
    "    categorical_cols = [\n",
    "        'background_father',\n",
    "        'background_mother',\n",
    "        'gender',\n",
    "        'region',\n",
    "        'diagnostic',\n",
    "    ]\n",
    "    # Convert categorical columns to string\n",
    "    for col in categorical_cols:\n",
    "        metadata[col] = metadata[col].astype(str)\n",
    "    \n",
    "    # One-hot encode categorical variables\n",
    "    metadata_encoded = pd.get_dummies(metadata[categorical_cols])\n",
    "    \n",
    "    # Normalize numerical variables\n",
    "    numerical_cols = ['age', 'fitspatrick', 'diameter_1', 'diameter_2']\n",
    "    # Ensure numerical columns are numeric\n",
    "    for col in numerical_cols:\n",
    "        metadata[col] = pd.to_numeric(metadata[col], errors='coerce')\n",
    "    # Fill NaNs in numerical columns with the mean\n",
    "    metadata[numerical_cols] = metadata[numerical_cols].fillna(metadata[numerical_cols].mean())\n",
    "    # Scale numerical columns\n",
    "    scaler = StandardScaler()\n",
    "    metadata_numeric = metadata[numerical_cols]\n",
    "    metadata_numeric_scaled = pd.DataFrame(\n",
    "        scaler.fit_transform(metadata_numeric), columns=numerical_cols\n",
    "    )\n",
    "    \n",
    "    # Combine all metadata features\n",
    "    metadata_processed = pd.concat(\n",
    "        [metadata_numeric_scaled.reset_index(drop=True),\n",
    "         metadata_encoded.reset_index(drop=True),\n",
    "         metadata[boolean_cols].reset_index(drop=True)], axis=1\n",
    "    )\n",
    "    \n",
    "    return metadata_processed\n",
    "\n",
    "# Preprocess metadata\n",
    "metadata_processed = preprocess_metadata(metadata)\n",
    "\n",
    "def get_image_paths(metadata, image_dir):\n",
    "    image_paths = []\n",
    "    for idx, row in metadata.iterrows():\n",
    "        filename = row['img_id']\n",
    "        # Ensure filename is a string\n",
    "        filename = str(filename)\n",
    "        # Check if filename has an extension\n",
    "        if not filename.endswith(('.jpg', '.jpeg', '.png')):\n",
    "            # Try common extensions\n",
    "            possible_extensions = ['.jpg', '.jpeg', '.png']\n",
    "            found = False\n",
    "            for ext in possible_extensions:\n",
    "                filepath = os.path.join(image_dir, filename + ext)\n",
    "                if os.path.isfile(filepath):\n",
    "                    image_paths.append(filepath)\n",
    "                    found = True\n",
    "                    break\n",
    "            if not found:\n",
    "                print(f\"Image file not found for ID: {filename}\")\n",
    "                image_paths.append(None)\n",
    "        else:\n",
    "            filepath = os.path.join(image_dir, filename)\n",
    "            if os.path.isfile(filepath):\n",
    "                image_paths.append(filepath)\n",
    "            else:\n",
    "                print(f\"Image file not found: {filepath}\")\n",
    "                image_paths.append(None)\n",
    "    metadata['ImagePath'] = image_paths\n",
    "    return metadata\n",
    "\n",
    "metadata = get_image_paths(metadata, IMAGE_DIR)\n",
    "\n",
    "# Remove entries with missing images\n",
    "metadata = metadata[metadata['ImagePath'].notnull()]\n",
    "metadata_processed = metadata_processed.loc[metadata.index].reset_index(drop=True)\n",
    "metadata = metadata.reset_index(drop=True)\n",
    "\n",
    "# Drop diagnostic-related columns from features\n",
    "diagnostic_cols = ['diagnostic_ACK', 'diagnostic_BCC', 'diagnostic_MEL', 'diagnostic_NEV', 'diagnostic_SCC', 'diagnostic_SEK']\n",
    "metadata_processed = metadata_processed.drop(columns=diagnostic_cols)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(metadata['diagnostic'])\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# Split data into features and labels\n",
    "X_meta = metadata_processed.reset_index(drop=True)\n",
    "X_img_paths = metadata['ImagePath'].reset_index(drop=True)\n",
    "y = pd.Series(y_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a560b78-d821-406f-a203-3c44038e42f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "# Count original labels\n",
    "original_counts = Counter(y)\n",
    "class_labels = label_encoder.classes_\n",
    "class_indices = list(range(len(class_labels)))\n",
    "counts = [original_counts[i] for i in class_indices]\n",
    "\n",
    "# Print counts\n",
    "print(\"ðŸ“Š Original Class Distribution:\")\n",
    "for cls, count in zip(class_labels, counts):\n",
    "    print(f\"{cls:<10} â†’ {count} samples\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(x=class_labels, y=counts)\n",
    "plt.title(\"Original Class Distribution\")\n",
    "plt.ylabel(\"Number of Samples\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaffe06-a2fd-477b-ac58-4eb3f19105e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "counts = Counter(y)\n",
    "ir = max(counts.values()) / min(counts.values())\n",
    "print(f'Imbalance Ratio (IR): {ir:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4910cfc8-a01f-4b9d-b2be-47d54bbb34c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(array):\n",
    "    array = np.sort(np.array(array))\n",
    "    index = np.arange(1, array.shape[0]+1)\n",
    "    return (2 * np.sum(index * array)) / (array.sum() * array.shape[0]) - (array.shape[0] + 1) / array.shape[0]\n",
    "\n",
    "gini_value = gini(list(Counter(y).values()))\n",
    "print(f'Gini Imbalance Score: {gini_value:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e309589-3bbf-4943-bc77-7f37c804a324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import random\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.combine import SMOTETomek\n",
    "import imagehash\n",
    "\n",
    "# === Configuration ===\n",
    "AUGMENTED_DIR = 'D:/PAD-UFES/augmented_images_balanced_800'\n",
    "\n",
    "# Recreate output directory\n",
    "if os.path.exists(AUGMENTED_DIR):\n",
    "    shutil.rmtree(AUGMENTED_DIR)\n",
    "os.makedirs(AUGMENTED_DIR, exist_ok=True)\n",
    "\n",
    "# === Augmentation Transform for saving (NO Normalize or ToTensor) ===\n",
    "offline_aug_transform_save = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomApply([\n",
    "        transforms.ColorJitter(0.3, 0.3, 0.3, 0.1),\n",
    "        transforms.RandomRotation(45),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "    ], p=1.0),  # Force transformation\n",
    "])\n",
    "\n",
    "# === Label Encoding ===\n",
    "label_encoder = LabelEncoder()\n",
    "metadata['diagnostic'] = metadata['diagnostic'].astype(str)\n",
    "y_all = label_encoder.fit_transform(metadata['diagnostic'])\n",
    "metadata['EncodedLabel'] = y_all\n",
    "metadata_processed = metadata_processed.reset_index(drop=True)\n",
    "\n",
    "# === Ensure metadata is numeric for SMOTETomek ===\n",
    "metadata_processed = metadata_processed.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "\n",
    "# === SMOTETomek Augmentation on Metadata ===\n",
    "print(\"\\nâš™ï¸ Applying SMOTETomek on metadata...\")\n",
    "smote_tomek = SMOTETomek(random_state=42)\n",
    "X_resampled, y_resampled = smote_tomek.fit_resample(metadata_processed, y_all)\n",
    "\n",
    "# Identify synthetic samples\n",
    "original_len = len(metadata_processed)\n",
    "X_synthetic = X_resampled[original_len:]\n",
    "y_synthetic = y_resampled[original_len:]\n",
    "\n",
    "X_synthetic = np.array(X_synthetic)\n",
    "\n",
    "# Group real image paths by label\n",
    "class_to_img_paths = metadata.groupby('EncodedLabel')['ImagePath'].apply(list).to_dict()\n",
    "\n",
    "# === Initialize augmented data holders ===\n",
    "augmented_metadata = []\n",
    "augmented_img_paths = []\n",
    "augmented_labels = []\n",
    "image_hashes = []  # for perceptual hashes\n",
    "meta_set = set()\n",
    "\n",
    "# === Generate Augmented Image + Metadata ===\n",
    "print(f\"\\nðŸŽ¨ Creating {len(X_synthetic)} synthetic imageâ€“metadata pairs...\")\n",
    "for i, (meta_row, label_id) in enumerate(zip(X_synthetic, y_synthetic)):\n",
    "    meta_row = np.array(meta_row, dtype=np.float64)\n",
    "    meta_tuple = tuple(np.round(meta_row, 5))\n",
    "    if meta_tuple in meta_set:\n",
    "        continue  # Skip duplicate metadata\n",
    "\n",
    "    # Randomly pick an image from the same class\n",
    "    img_candidates = class_to_img_paths.get(label_id, [])\n",
    "    if not img_candidates:\n",
    "        continue\n",
    "\n",
    "    chosen_img_path = random.choice(img_candidates)\n",
    "    try:\n",
    "        image = Image.open(chosen_img_path).convert('RGB')\n",
    "    except:\n",
    "        print(f\"âš ï¸ Failed to load: {chosen_img_path}\")\n",
    "        continue\n",
    "\n",
    "    aug_img = offline_aug_transform_save(image)\n",
    "    img_hash = imagehash.phash(aug_img)\n",
    "\n",
    "    if any(img_hash - h <= 5 for h in image_hashes):\n",
    "        continue  # Skip visually similar image\n",
    "\n",
    "    aug_filename = f\"aug_{label_id}_{i}.jpg\"\n",
    "    aug_path = os.path.join(AUGMENTED_DIR, aug_filename)\n",
    "    aug_img.save(aug_path, format='JPEG', quality=95)\n",
    "\n",
    "    image_hashes.append(img_hash)\n",
    "    meta_set.add(meta_tuple)\n",
    "    augmented_img_paths.append(aug_path)\n",
    "    augmented_metadata.append(meta_row)\n",
    "    augmented_labels.append(label_id)\n",
    "\n",
    "# === Combine into DataFrames ===\n",
    "aug_meta_df = pd.DataFrame(augmented_metadata, columns=metadata_processed.columns)\n",
    "aug_img_paths_series = pd.Series(augmented_img_paths)\n",
    "aug_labels_series = pd.Series(augmented_labels)\n",
    "\n",
    "print(f\"\\nâœ… Finished generating {len(aug_labels_series)} clean synthetic samples.\")\n",
    "print(f\"Images saved to: '{AUGMENTED_DIR}'\")\n",
    "\n",
    "# Save outputs\n",
    "augmented_df = pd.DataFrame({\n",
    "    'ImagePath': aug_img_paths_series,\n",
    "    'Label': aug_labels_series\n",
    "})\n",
    "aug_meta_df.to_csv(\"D:/PAD-UFES/augmented_metadata.csv\", index=False)\n",
    "augmented_df.to_csv(\"D:/PAD-UFES/augmented_labels.csv\", index=False)\n",
    "\n",
    "# Debug summary\n",
    "from collections import Counter\n",
    "print(\"\\nðŸ“Š Synthetic class distribution:\")\n",
    "print(Counter(aug_labels_series))\n",
    "print(\"âœ… No duplicate metadata rows?\", aug_meta_df.duplicated().sum() == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11750826-9e90-40f8-a3a0-22809bd93d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# First: Train + Temp\n",
    "X_train_meta, X_temp_meta, X_train_img_paths, X_temp_img_paths, y_train, y_temp = train_test_split(\n",
    "    X_meta,\n",
    "    X_img_paths,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Then: Temp â†’ Validation + Test\n",
    "X_val_meta, X_test_meta, X_val_img_paths, X_test_img_paths, y_val, y_test = train_test_split(\n",
    "    X_temp_meta,\n",
    "    X_temp_img_paths,\n",
    "    y_temp,\n",
    "    test_size=0.5,\n",
    "    stratify=y_temp,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Load augmented metadata + image paths\n",
    "aug_meta_df = pd.read_csv(\"D:/PAD-UFES/augmented_metadata.csv\")\n",
    "aug_labels_df = pd.read_csv(\"D:/PAD-UFES/augmented_labels.csv\")\n",
    "\n",
    "# Combine augmented samples with training set\n",
    "X_train_meta_final = pd.concat([X_train_meta, aug_meta_df], ignore_index=True)\n",
    "X_train_img_paths_final = pd.concat([X_train_img_paths.reset_index(drop=True),\n",
    "                                     aug_labels_df['ImagePath']], ignore_index=True)\n",
    "y_train_final = pd.concat([y_train.reset_index(drop=True),\n",
    "                           aug_labels_df['Label']], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92713716-aed5-4523-8c76-d59003651a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import imagehash\n",
    "\n",
    "# === Paths ===\n",
    "AUG_IMG_DIR = \"D:/PAD-UFES/augmented_images_balanced_800\"\n",
    "AUG_META_CSV = \"D:/PAD-UFES/augmented_metadata.csv\"\n",
    "AUG_LABELS_CSV = \"D:/PAD-UFES/augmented_labels.csv\"\n",
    "\n",
    "# Load augmented CSVs\n",
    "aug_meta_df = pd.read_csv(AUG_META_CSV)\n",
    "aug_labels_df = pd.read_csv(AUG_LABELS_CSV)\n",
    "\n",
    "# Extract augmented image paths\n",
    "aug_img_paths = aug_labels_df['ImagePath'].tolist()\n",
    "\n",
    "# Validation and test paths (define these if not already available)\n",
    "val_paths = X_val_img_paths.tolist()\n",
    "test_paths = X_test_img_paths.tolist()\n",
    "\n",
    "# === Compute perceptual hashes ===\n",
    "def compute_hashes(paths):\n",
    "    hash_dict = {}\n",
    "    for p in paths:\n",
    "        try:\n",
    "            img = Image.open(p).resize((224, 224)).convert('RGB')\n",
    "            img_hash = imagehash.phash(img)\n",
    "            hash_dict[p] = img_hash\n",
    "        except:\n",
    "            continue\n",
    "    return hash_dict\n",
    "\n",
    "val_hashes = compute_hashes(val_paths)\n",
    "test_hashes = compute_hashes(test_paths)\n",
    "aug_hashes = compute_hashes(aug_img_paths)\n",
    "\n",
    "# === Find and delete leaks ===\n",
    "hash_threshold = 5\n",
    "to_delete = []\n",
    "\n",
    "for aug_path, aug_hash in aug_hashes.items():\n",
    "    for ref_hash in list(val_hashes.values()) + list(test_hashes.values()):\n",
    "        if aug_hash - ref_hash <= hash_threshold:\n",
    "            to_delete.append(aug_path)\n",
    "            break  # no need to check further if one match is found\n",
    "\n",
    "print(f\"\\nðŸ§¹ Found {len(to_delete)} augmented images to delete due to potential leakage.\")\n",
    "\n",
    "# === Delete images and remove entries from CSVs ===\n",
    "aug_labels_df_clean = aug_labels_df[~aug_labels_df['ImagePath'].isin(to_delete)].reset_index(drop=True)\n",
    "aug_meta_df_clean = aug_meta_df.loc[aug_labels_df_clean.index].reset_index(drop=True)\n",
    "\n",
    "for img_path in to_delete:\n",
    "    try:\n",
    "        os.remove(img_path)\n",
    "    except:\n",
    "        print(f\"âš ï¸ Couldn't delete: {img_path}\")\n",
    "\n",
    "# Save cleaned CSVs\n",
    "aug_labels_df_clean.to_csv(AUG_LABELS_CSV, index=False)\n",
    "aug_meta_df_clean.to_csv(AUG_META_CSV, index=False)\n",
    "\n",
    "print(\"âœ… Cleaned CSVs and removed leaked images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3067a01-c02a-4c56-a0af-50d4757787c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === Separate distances ===\n",
    "val_dists = []\n",
    "test_dists = []\n",
    "\n",
    "for aug_hash in aug_hashes.values():\n",
    "    for ref_hash in val_hashes.values():\n",
    "        val_dists.append(aug_hash - ref_hash)\n",
    "    for ref_hash in test_hashes.values():\n",
    "        test_dists.append(aug_hash - ref_hash)\n",
    "\n",
    "# === Plotting ===\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(val_dists, bins=30, alpha=0.6, label='Augmented vs Validation', color='blue', edgecolor='black')\n",
    "plt.hist(test_dists, bins=30, alpha=0.6, label='Augmented vs Test', color='green', edgecolor='black')\n",
    "plt.axvline(x=5, color='red', linestyle='--', label='Threshold (5)')\n",
    "\n",
    "plt.title(\"Hamming Distance Between Augmented and Val/Test Sets\")\n",
    "plt.xlabel(\"Hamming Distance\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7a3d74-4c75-4360-9631-ecf6a6c34eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set the style and color palette\n",
    "plt.style.use('seaborn-v0_8-paper')\n",
    "sns.set_palette(\"deep\")\n",
    "\n",
    "# Create the figure and axes\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot histograms with enhanced aesthetics\n",
    "sns.histplot(val_dists, bins=30, alpha=0.7, label='Augmented vs Validation', \n",
    "             color=sns.color_palette(\"deep\")[0], \n",
    "             edgecolor='white', \n",
    "             linewidth=1)\n",
    "sns.histplot(test_dists, bins=30, alpha=0.7, label='Augmented vs Test', \n",
    "             color=sns.color_palette(\"deep\")[1], \n",
    "             edgecolor='white', \n",
    "             linewidth=1)\n",
    "\n",
    "# Add vertical line with improved styling\n",
    "plt.axvline(x=5, color='red', linestyle='--', linewidth=2, label='Threshold (5)')\n",
    "\n",
    "# Enhance title and labels\n",
    "plt.title(\"Hamming Distance Distribution\", fontsize=16, fontweight='bold')\n",
    "plt.xlabel(\"Hamming Distance\", fontsize=12)\n",
    "plt.ylabel(\"Frequency\", fontsize=12)\n",
    "\n",
    "# Improve legend\n",
    "plt.legend(frameon=True, framealpha=0.8, facecolor='white', edgecolor='gray')\n",
    "\n",
    "# Add grid with subtler styling\n",
    "plt.grid(True, linestyle='--', linewidth=0.5, color='gray', alpha=0.7)\n",
    "\n",
    "# Adjust layout and save\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ff071e-acbc-4f3a-9801-669106ba2930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save validation and test metadata for future distance checks\n",
    "X_val_meta.to_csv(\"D:/PAD-UFES/val_metadata.csv\", index=False)\n",
    "X_test_meta.to_csv(\"D:/PAD-UFES/test_metadata.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7004d8-dfc6-4565-bfaf-30fbdaf70ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances, cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === Load metadata ===\n",
    "aug_meta_df = pd.read_csv(\"D:/PAD-UFES/augmented_metadata.csv\")\n",
    "val_meta_df = pd.read_csv(\"D:/PAD-UFES/val_metadata.csv\")     # Save this when splitting\n",
    "test_meta_df = pd.read_csv(\"D:/PAD-UFES/test_metadata.csv\")   # Save this when splitting\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X_aug = aug_meta_df.values\n",
    "X_val = val_meta_df.values\n",
    "X_test = test_meta_df.values\n",
    "\n",
    "# === 1. Euclidean Distance (Val & Test) ===\n",
    "eucl_val = euclidean_distances(X_aug, X_val).min(axis=1)\n",
    "eucl_test = euclidean_distances(X_aug, X_test).min(axis=1)\n",
    "\n",
    "# === 2. Cosine Distance (1 - similarity) ===\n",
    "cos_val = 1 - cosine_similarity(X_aug, X_val).max(axis=1)\n",
    "cos_test = 1 - cosine_similarity(X_aug, X_test).max(axis=1)\n",
    "\n",
    "# === Plotting ===\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(eucl_val, bins=30, alpha=0.7, label=\"Aug vs Val\", color='teal', edgecolor='black')\n",
    "plt.hist(eucl_test, bins=30, alpha=0.7, label=\"Aug vs Test\", color='purple', edgecolor='black')\n",
    "plt.axvline(x=0.3, color='red', linestyle='--', label='Threshold')\n",
    "plt.title(\"Euclidean Distance: Augmented â†’ Val/Test\")\n",
    "plt.xlabel(\"Distance\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(cos_val, bins=30, alpha=0.7, label=\"Aug vs Val\", color='orange', edgecolor='black')\n",
    "plt.hist(cos_test, bins=30, alpha=0.7, label=\"Aug vs Test\", color='gray', edgecolor='black')\n",
    "plt.axvline(x=0.1, color='red', linestyle='--', label='Threshold')\n",
    "plt.title(\"Cosine Distance: Augmented â†’ Val/Test\")\n",
    "plt.xlabel(\"Distance\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8c8187-d7f1-4660-9461-a7967270e918",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import euclidean_distances, cosine_similarity\n",
    "\n",
    "# === Load metadata ===\n",
    "aug_meta_df = pd.read_csv(\"D:/PAD-UFES/augmented_metadata.csv\")\n",
    "val_meta_df = pd.read_csv(\"D:/PAD-UFES/val_metadata.csv\")\n",
    "test_meta_df = pd.read_csv(\"D:/PAD-UFES/test_metadata.csv\")\n",
    "\n",
    "# === Convert to arrays ===\n",
    "X_aug = aug_meta_df.values\n",
    "X_val = val_meta_df.values\n",
    "X_test = test_meta_df.values\n",
    "\n",
    "# === Euclidean Distance ===\n",
    "eucl_val_matrix = euclidean_distances(X_aug, X_val)\n",
    "eucl_test_matrix = euclidean_distances(X_aug, X_test)\n",
    "eucl_val_dists = eucl_val_matrix.min(axis=1)\n",
    "eucl_test_dists = eucl_test_matrix.min(axis=1)\n",
    "\n",
    "# === Cosine Distance ===\n",
    "cos_val_sim = cosine_similarity(X_aug, X_val)\n",
    "cos_test_sim = cosine_similarity(X_aug, X_test)\n",
    "cos_val_dists = 1 - cos_val_sim.max(axis=1)\n",
    "cos_test_dists = 1 - cos_test_sim.max(axis=1)\n",
    "\n",
    "# === Summary function ===\n",
    "def summarize_distances(name, distances, threshold):\n",
    "    return {\n",
    "        'Type': name,\n",
    "        'Mean': round(np.mean(distances), 4),\n",
    "        'Min': round(np.min(distances), 4),\n",
    "        'Max': round(np.max(distances), 4),\n",
    "        'Samples Below Threshold': int(np.sum(distances < threshold))\n",
    "    }\n",
    "\n",
    "# === Thresholds ===\n",
    "eucl_thresh = 0.3\n",
    "cos_thresh = 0.1\n",
    "\n",
    "# === Collect statistics ===\n",
    "summary_data = [\n",
    "    summarize_distances(\"Euclidean (Aug vs Val)\", eucl_val_dists, eucl_thresh),\n",
    "    summarize_distances(\"Euclidean (Aug vs Test)\", eucl_test_dists, eucl_thresh),\n",
    "    summarize_distances(\"Cosine (Aug vs Val)\", cos_val_dists, cos_thresh),\n",
    "    summarize_distances(\"Cosine (Aug vs Test)\", cos_test_dists, cos_thresh),\n",
    "]\n",
    "\n",
    "# === Create and display DataFrame ===\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\nðŸ“Š Distance Summary Table:\")\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108fa952-cc32-4d9a-9562-8c851b5ab5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# === Style ===\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_context(\"notebook\", font_scale=1.2)\n",
    "\n",
    "# === Euclidean Distance Plot ===\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.hist(eucl_val, bins=30, alpha=0.7, label=\"Augmented vs Validation\", color='#1E88E5', edgecolor='white')\n",
    "plt.hist(eucl_test, bins=30, alpha=0.7, label=\"Augmented vs Test\", color='#FFB6C1', edgecolor='white')\n",
    "plt.axvline(x=0.3, color='red', linestyle='--', linewidth=2, label='Threshold')\n",
    "plt.title(\"Euclidean Distance Distribution\", fontsize=15, fontweight='bold')\n",
    "plt.xlabel(\"Euclidean Distance\", fontsize=12)\n",
    "plt.ylabel(\"Frequency\", fontsize=12)\n",
    "plt.legend(title='Dataset Comparison')\n",
    "plt.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.savefig('euclidean_distance_plot.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# === Cosine Distance Plot ===\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.hist(cos_val, bins=30, alpha=0.7, label=\"Augmented vs Validation\", color='#1E88E5', edgecolor='white')\n",
    "plt.hist(cos_test, bins=30, alpha=0.7, label=\"Augmented vs Test\", color='#FFB6C1', edgecolor='white')\n",
    "plt.axvline(x=0.1, color='red', linestyle='--', linewidth=2, label='Threshold')\n",
    "plt.title(\"Cosine Distance Distribution\", fontsize=15, fontweight='bold')\n",
    "plt.xlabel(\"Cosine Distance\", fontsize=12)\n",
    "plt.ylabel(\"Frequency\", fontsize=12)\n",
    "plt.legend(title='Dataset Comparison')\n",
    "plt.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.savefig('cosine_distance_plot.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39d8852-27d0-464a-a7f2-ffad18a41a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import euclidean_distances, cosine_similarity\n",
    "\n",
    "# === Paths ===\n",
    "AUG_IMG_DIR = \"D:/PAD-UFES/augmented_images_balanced_800\"\n",
    "AUG_META_CSV = \"D:/PAD-UFES/augmented_metadata.csv\"\n",
    "AUG_LABELS_CSV = \"D:/PAD-UFES/augmented_labels.csv\"\n",
    "VAL_META_CSV = \"D:/PAD-UFES/val_metadata.csv\"\n",
    "TEST_META_CSV = \"D:/PAD-UFES/test_metadata.csv\"\n",
    "\n",
    "# === Load metadata ===\n",
    "aug_meta_df = pd.read_csv(AUG_META_CSV)\n",
    "aug_labels_df = pd.read_csv(AUG_LABELS_CSV)\n",
    "val_meta_df = pd.read_csv(VAL_META_CSV)\n",
    "test_meta_df = pd.read_csv(TEST_META_CSV)\n",
    "\n",
    "# === Convert to arrays ===\n",
    "X_aug = aug_meta_df.values\n",
    "X_val = val_meta_df.values\n",
    "X_test = test_meta_df.values\n",
    "\n",
    "# === Compute distances ===\n",
    "eucl_val_dists = euclidean_distances(X_aug, X_val).min(axis=1)\n",
    "eucl_test_dists = euclidean_distances(X_aug, X_test).min(axis=1)\n",
    "\n",
    "cos_val_dists = 1 - cosine_similarity(X_aug, X_val).max(axis=1)\n",
    "cos_test_dists = 1 - cosine_similarity(X_aug, X_test).max(axis=1)\n",
    "\n",
    "# === Define thresholds ===\n",
    "eucl_thresh = 0.3\n",
    "cos_thresh = 0.1\n",
    "\n",
    "# === Get indices to delete ===\n",
    "to_delete_mask = (\n",
    "    (eucl_val_dists < eucl_thresh) | (eucl_test_dists < eucl_thresh) |\n",
    "    (cos_val_dists < cos_thresh) | (cos_test_dists < cos_thresh)\n",
    ")\n",
    "to_delete_indices = np.where(to_delete_mask)[0]\n",
    "\n",
    "print(f\"\\nðŸ§¹ Removing {len(to_delete_indices)} augmented samples due to metadata similarity...\")\n",
    "\n",
    "# === Delete image files ===\n",
    "img_paths_to_delete = aug_labels_df.iloc[to_delete_indices]['ImagePath'].tolist()\n",
    "\n",
    "for path in img_paths_to_delete:\n",
    "    try:\n",
    "        os.remove(path)\n",
    "    except:\n",
    "        print(f\"âš ï¸ Couldn't delete image: {path}\")\n",
    "\n",
    "# === Keep only safe samples ===\n",
    "aug_meta_df_clean = aug_meta_df.drop(index=to_delete_indices).reset_index(drop=True)\n",
    "aug_labels_df_clean = aug_labels_df.drop(index=to_delete_indices).reset_index(drop=True)\n",
    "\n",
    "# === Save cleaned files ===\n",
    "aug_meta_df_clean.to_csv(AUG_META_CSV, index=False)\n",
    "aug_labels_df_clean.to_csv(AUG_LABELS_CSV, index=False)\n",
    "\n",
    "print(\"âœ… Metadata + image cleanup complete. Cleaned CSVs saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6385e766-6a18-4cc2-82bf-96357d42777c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import euclidean_distances, cosine_similarity\n",
    "\n",
    "# === Load metadata ===\n",
    "aug_meta_df = pd.read_csv(\"D:/PAD-UFES/augmented_metadata.csv\")\n",
    "val_meta_df = pd.read_csv(\"D:/PAD-UFES/val_metadata.csv\")\n",
    "test_meta_df = pd.read_csv(\"D:/PAD-UFES/test_metadata.csv\")\n",
    "\n",
    "# === Convert to arrays ===\n",
    "X_aug = aug_meta_df.values\n",
    "X_val = val_meta_df.values\n",
    "X_test = test_meta_df.values\n",
    "\n",
    "# === Euclidean Distance ===\n",
    "eucl_val_matrix = euclidean_distances(X_aug, X_val)\n",
    "eucl_test_matrix = euclidean_distances(X_aug, X_test)\n",
    "eucl_val_dists = eucl_val_matrix.min(axis=1)\n",
    "eucl_test_dists = eucl_test_matrix.min(axis=1)\n",
    "\n",
    "# === Cosine Distance ===\n",
    "cos_val_sim = cosine_similarity(X_aug, X_val)\n",
    "cos_test_sim = cosine_similarity(X_aug, X_test)\n",
    "cos_val_dists = 1 - cos_val_sim.max(axis=1)\n",
    "cos_test_dists = 1 - cos_test_sim.max(axis=1)\n",
    "\n",
    "# === Summary function ===\n",
    "def summarize_distances(name, distances, threshold):\n",
    "    return {\n",
    "        'Type': name,\n",
    "        'Mean': round(np.mean(distances), 4),\n",
    "        'Min': round(np.min(distances), 4),\n",
    "        'Max': round(np.max(distances), 4),\n",
    "        'Samples Below Threshold': int(np.sum(distances < threshold))\n",
    "    }\n",
    "\n",
    "# === Thresholds ===\n",
    "eucl_thresh = 0.3\n",
    "cos_thresh = 0.1\n",
    "\n",
    "# === Collect statistics ===\n",
    "summary_data = [\n",
    "    summarize_distances(\"Euclidean (Aug vs Val)\", eucl_val_dists, eucl_thresh),\n",
    "    summarize_distances(\"Euclidean (Aug vs Test)\", eucl_test_dists, eucl_thresh),\n",
    "    summarize_distances(\"Cosine (Aug vs Val)\", cos_val_dists, cos_thresh),\n",
    "    summarize_distances(\"Cosine (Aug vs Test)\", cos_test_dists, cos_thresh),\n",
    "]\n",
    "\n",
    "# === Create and display DataFrame ===\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\nðŸ“Š Distance Summary Table:\")\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21664316-6086-4e85-afb1-0efe8695abf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import imagehash\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === Paths ===\n",
    "val_paths = X_val_img_paths.tolist()\n",
    "test_paths = X_test_img_paths.tolist()\n",
    "train_aug_paths = X_train_img_paths_final.tolist()  # original + augmented\n",
    "\n",
    "# === Hash all val/test images ===\n",
    "def compute_hashes(img_paths, label=\"set\"):\n",
    "    hashes = {}\n",
    "    for path in tqdm(img_paths, desc=f\"Hashing {label}\"):\n",
    "        try:\n",
    "            img = Image.open(path).resize((224, 224)).convert(\"RGB\")\n",
    "            img_hash = imagehash.phash(img)\n",
    "            hashes[path] = img_hash\n",
    "        except:\n",
    "            print(f\"âš ï¸ Failed to load {path}\")\n",
    "    return hashes\n",
    "\n",
    "val_hashes = compute_hashes(val_paths, label=\"validation\")\n",
    "test_hashes = compute_hashes(test_paths, label=\"test\")\n",
    "train_hashes = compute_hashes(train_aug_paths, label=\"train+augmented\")\n",
    "\n",
    "# === Compare hashes for leakage ===\n",
    "def check_leakage(query_hashes, reference_hashes, threshold=5):\n",
    "    leaked = []\n",
    "    for q_path, q_hash in tqdm(query_hashes.items(), desc=\"Checking leakage\"):\n",
    "        for r_path, r_hash in reference_hashes.items():\n",
    "            if q_hash - r_hash <= threshold:\n",
    "                leaked.append((q_path, r_path, q_hash - r_hash))\n",
    "                break\n",
    "    return leaked\n",
    "\n",
    "# === Run leakage detection ===\n",
    "print(\"\\nðŸ” Checking for data leakage in validation set...\")\n",
    "val_leaks = check_leakage(val_hashes, train_hashes)\n",
    "\n",
    "print(\"\\nðŸ” Checking for data leakage in test set...\")\n",
    "test_leaks = check_leakage(test_hashes, train_hashes)\n",
    "\n",
    "# === Summary ===\n",
    "print(f\"\\nðŸš¨ Potential leaks in validation set: {len(val_leaks)}\")\n",
    "print(f\"ðŸš¨ Potential leaks in test set: {len(test_leaks)}\")\n",
    "\n",
    "if val_leaks:\n",
    "    print(\"\\nSome examples from validation set leakage:\")\n",
    "    for leak in val_leaks[:3]:\n",
    "        print(f\" - Val: {leak[0]}  â†”  Train: {leak[1]}  [Dist={leak[2]}]\")\n",
    "\n",
    "if test_leaks:\n",
    "    print(\"\\nSome examples from test set leakage:\")\n",
    "    for leak in test_leaks[:3]:\n",
    "        print(f\" - Test: {leak[0]}  â†”  Train: {leak[1]}  [Dist={leak[2]}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1aab1d-0017-4768-b9e3-d151c4a6de68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imagehash\n",
    "from PIL import Image\n",
    "\n",
    "# === Compute perceptual hashes ===\n",
    "def compute_hash_dict(paths):\n",
    "    hash_dict = {}\n",
    "    for path in paths:\n",
    "        try:\n",
    "            img = Image.open(path).resize((224, 224)).convert('RGB')\n",
    "            img_hash = imagehash.phash(img)\n",
    "            hash_dict[path] = img_hash\n",
    "        except:\n",
    "            continue\n",
    "    return hash_dict\n",
    "\n",
    "# Compute hashes for all sets\n",
    "train_hashes = compute_hash_dict(X_train_img_paths)\n",
    "val_hashes = compute_hash_dict(X_val_img_paths)\n",
    "test_hashes = compute_hash_dict(X_test_img_paths)\n",
    "\n",
    "# === Compare hashes and flag train samples to remove ===\n",
    "hash_threshold = 5\n",
    "leak_train_indices = set()\n",
    "\n",
    "# Compare against validation set\n",
    "for val_path, val_hash in val_hashes.items():\n",
    "    for i, train_path in enumerate(X_train_img_paths):\n",
    "        if train_path in train_hashes:\n",
    "            dist = val_hash - train_hashes[train_path]\n",
    "            if dist <= hash_threshold:\n",
    "                leak_train_indices.add(i)\n",
    "\n",
    "# Compare against test set\n",
    "for test_path, test_hash in test_hashes.items():\n",
    "    for i, train_path in enumerate(X_train_img_paths):\n",
    "        if train_path in train_hashes:\n",
    "            dist = test_hash - train_hashes[train_path]\n",
    "            if dist <= hash_threshold:\n",
    "                leak_train_indices.add(i)\n",
    "\n",
    "print(f\"\\nðŸ” Found {len(leak_train_indices)} potential leakage samples in train set.\")\n",
    "\n",
    "# === Remove leaked train samples ===\n",
    "mask = ~X_train_img_paths.index.isin(leak_train_indices)\n",
    "X_train_img_paths = X_train_img_paths[mask].reset_index(drop=True)\n",
    "X_train_meta = X_train_meta[mask].reset_index(drop=True)\n",
    "y_train = y_train[mask].reset_index(drop=True)\n",
    "\n",
    "print(f\"âœ… Train set cleaned. New size: {len(X_train_img_paths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b983f99-a38a-4449-b4ff-dca717680c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "hash_array = np.array(hash_distances)\n",
    "print(\"ðŸ“Š Hamming Distance Summary:\")\n",
    "print(f\"Mean:    {np.mean(hash_array):.2f}\")\n",
    "print(f\"Std Dev: {np.std(hash_array):.2f}\")\n",
    "print(f\"Min:     {np.min(hash_array)}\")\n",
    "print(f\"Max:     {np.max(hash_array)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd4c99f-a49f-498f-b50f-8fffb5fbb24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "tsne_dists = pdist(X_embedded, metric='euclidean')\n",
    "print(\"ðŸ“Š t-SNE Embedding Pairwise Distance Summary:\")\n",
    "print(f\"Mean:    {np.mean(tsne_dists):.2f}\")\n",
    "print(f\"Std Dev: {np.std(tsne_dists):.2f}\")\n",
    "print(f\"Min:     {np.min(tsne_dists):.2f}\")\n",
    "print(f\"Max:     {np.max(tsne_dists):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd700973-d4ec-4846-a530-015948088497",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_embedded = pd.DataFrame(X_embedded, columns=['tsne_1', 'tsne_2'])\n",
    "df_embedded['label'] = aug_labels.values\n",
    "\n",
    "centroids = df_embedded.groupby('label')[['tsne_1', 'tsne_2']].mean().values\n",
    "centroid_dists = pdist(centroids, metric='euclidean')\n",
    "\n",
    "print(\"ðŸ“Š Class Centroid Distances in t-SNE space:\")\n",
    "print(f\"Mean distance between class centroids: {np.mean(centroid_dists):.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "085881ebbcda4def929a7afeba1586a2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "22ac965251184c9d92878d0fc141f9bf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "528ccd9132af44b3a2d08c17a371a74f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b525369342324ae79912f44225a5684f",
       "max": 22058321,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_8713a5c8e429475f80d66c0a3b0a4c5f",
       "tabbable": null,
       "tooltip": null,
       "value": 22058321
      }
     },
     "6eb4d407a2e14d3398ca31e8ef765ac9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_dcc805abc57648118fa371add11e6e53",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_085881ebbcda4def929a7afeba1586a2",
       "tabbable": null,
       "tooltip": null,
       "value": "model.safetensors:â€‡100%"
      }
     },
     "8713a5c8e429475f80d66c0a3b0a4c5f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "98428bdcd8904e79aebaebf1a0adcf75": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a9cc0e6fdbd04830970252fecda17487": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_6eb4d407a2e14d3398ca31e8ef765ac9",
        "IPY_MODEL_528ccd9132af44b3a2d08c17a371a74f",
        "IPY_MODEL_b94952efad8c4fefbc83bd28ec9e1c51"
       ],
       "layout": "IPY_MODEL_22ac965251184c9d92878d0fc141f9bf",
       "tabbable": null,
       "tooltip": null
      }
     },
     "ac7f57fd95414ddaa4e393f2a78ee5d9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b525369342324ae79912f44225a5684f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b94952efad8c4fefbc83bd28ec9e1c51": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ac7f57fd95414ddaa4e393f2a78ee5d9",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_98428bdcd8904e79aebaebf1a0adcf75",
       "tabbable": null,
       "tooltip": null,
       "value": "â€‡22.1M/22.1Mâ€‡[00:05&lt;00:00,â€‡3.77MB/s]"
      }
     },
     "dcc805abc57648118fa371add11e6e53": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
