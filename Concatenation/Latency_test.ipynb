{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7ebcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)  # If you are using CUDA\n",
    "torch.backends.cudnn.deterministic = True  # For deterministic results\n",
    "torch.backends.cudnn.benchmark = False  # For consistency across different environments\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "IMAGE_DIR = 'D:\\\\PAD-UFES\\\\images'  \n",
    "METADATA_PATH = 'D:\\\\PAD-UFES\\\\metadata.csv'\n",
    "\n",
    "metadata = pd.read_csv(METADATA_PATH)\n",
    "\n",
    "def preprocess_metadata(metadata):\n",
    "    metadata = metadata.fillna('UNK')\n",
    "\n",
    "    boolean_cols = [\n",
    "        'smoke',\n",
    "        'drink',\n",
    "        'pesticide',\n",
    "        'skin_cancer_history',\n",
    "        'cancer_history',\n",
    "        'has_piped_water',\n",
    "        'has_sewage_system',\n",
    "        'itch',\n",
    "        'grew',\n",
    "        'hurt',\n",
    "        'changed',\n",
    "        'bleed',\n",
    "        'elevation',\n",
    "        'biopsed',\n",
    "    ]\n",
    "    # Ensure columns are strings and lowercase\n",
    "    for col in boolean_cols:\n",
    "        metadata[col] = metadata[col].astype(str).str.lower()\n",
    "    \n",
    "    # Map boolean columns to 1/0/-1\n",
    "    boolean_mapping = {'true': 1, 'false': 0, 'unk': -1}\n",
    "    for col in boolean_cols:\n",
    "        metadata[col] = metadata[col].map(boolean_mapping)\n",
    "    \n",
    "    # Handle categorical variables\n",
    "    categorical_cols = [\n",
    "        'background_father',\n",
    "        'background_mother',\n",
    "        'gender',\n",
    "        'region',\n",
    "        'diagnostic',\n",
    "    ]\n",
    "    # Convert categorical columns to string\n",
    "    for col in categorical_cols:\n",
    "        metadata[col] = metadata[col].astype(str)\n",
    "    \n",
    "    # One-hot encode categorical variables\n",
    "    metadata_encoded = pd.get_dummies(metadata[categorical_cols])\n",
    "    \n",
    "    # Normalize numerical variables\n",
    "    numerical_cols = ['age', 'fitspatrick', 'diameter_1', 'diameter_2']\n",
    "    # Ensure numerical columns are numeric\n",
    "    for col in numerical_cols:\n",
    "        metadata[col] = pd.to_numeric(metadata[col], errors='coerce')\n",
    "    # Fill NaNs in numerical columns with the mean\n",
    "    metadata[numerical_cols] = metadata[numerical_cols].fillna(metadata[numerical_cols].mean())\n",
    "    # Scale numerical columns\n",
    "    scaler = StandardScaler()\n",
    "    metadata_numeric = metadata[numerical_cols]\n",
    "    metadata_numeric_scaled = pd.DataFrame(\n",
    "        scaler.fit_transform(metadata_numeric), columns=numerical_cols\n",
    "    )\n",
    "    \n",
    "    # Combine all metadata features\n",
    "    metadata_processed = pd.concat(\n",
    "        [metadata_numeric_scaled.reset_index(drop=True),\n",
    "         metadata_encoded.reset_index(drop=True),\n",
    "         metadata[boolean_cols].reset_index(drop=True)], axis=1\n",
    "    )\n",
    "    \n",
    "    return metadata_processed\n",
    "\n",
    "# Preprocess metadata\n",
    "metadata_processed = preprocess_metadata(metadata)\n",
    "\n",
    "def get_image_paths(metadata, image_dir):\n",
    "    image_paths = []\n",
    "    for idx, row in metadata.iterrows():\n",
    "        filename = row['img_id']\n",
    "        # Ensure filename is a string\n",
    "        filename = str(filename)\n",
    "        # Check if filename has an extension\n",
    "        if not filename.endswith(('.jpg', '.jpeg', '.png')):\n",
    "            # Try common extensions\n",
    "            possible_extensions = ['.jpg', '.jpeg', '.png']\n",
    "            found = False\n",
    "            for ext in possible_extensions:\n",
    "                filepath = os.path.join(image_dir, filename + ext)\n",
    "                if os.path.isfile(filepath):\n",
    "                    image_paths.append(filepath)\n",
    "                    found = True\n",
    "                    break\n",
    "            if not found:\n",
    "                print(f\"Image file not found for ID: {filename}\")\n",
    "                image_paths.append(None)\n",
    "        else:\n",
    "            filepath = os.path.join(image_dir, filename)\n",
    "            if os.path.isfile(filepath):\n",
    "                image_paths.append(filepath)\n",
    "            else:\n",
    "                print(f\"Image file not found: {filepath}\")\n",
    "                image_paths.append(None)\n",
    "    metadata['ImagePath'] = image_paths\n",
    "    return metadata\n",
    "\n",
    "metadata = get_image_paths(metadata, IMAGE_DIR)\n",
    "\n",
    "# Remove entries with missing images\n",
    "metadata = metadata[metadata['ImagePath'].notnull()]\n",
    "metadata_processed = metadata_processed.loc[metadata.index].reset_index(drop=True)\n",
    "metadata = metadata.reset_index(drop=True)\n",
    "\n",
    "# Drop diagnostic-related columns from features\n",
    "diagnostic_cols = ['diagnostic_ACK', 'diagnostic_BCC', 'diagnostic_MEL', 'diagnostic_NEV', 'diagnostic_SCC', 'diagnostic_SEK']\n",
    "metadata_processed = metadata_processed.drop(columns=diagnostic_cols)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(metadata['diagnostic'])\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# Split data into features and labels\n",
    "X_meta = metadata_processed.reset_index(drop=True)\n",
    "X_img_paths = metadata['ImagePath'].reset_index(drop=True)\n",
    "y = pd.Series(y_encoded)\n",
    "\n",
    "X_train_meta, X_temp_meta, X_train_img_paths, X_temp_img_paths, y_train, y_temp = train_test_split(\n",
    "    X_meta,\n",
    "    X_img_paths,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "X_val_meta, X_test_meta, X_val_img_paths, X_test_img_paths, y_val, y_test = train_test_split(\n",
    "    X_temp_meta,\n",
    "    X_temp_img_paths,\n",
    "    y_temp,\n",
    "    test_size=0.5,\n",
    "    random_state=42,\n",
    "    stratify=y_temp\n",
    ")\n",
    "\n",
    "# Load augmented metadata + image paths\n",
    "aug_meta_df   = pd.read_csv(\"D:/PAD-UFES/augmented_metadata.csv\")\n",
    "aug_labels_df = pd.read_csv(\"D:/PAD-UFES/augmented_labels.csv\")\n",
    "\n",
    "# Combine augmented samples with training set\n",
    "X_train_meta_final = pd.concat([X_train_meta, aug_meta_df], ignore_index=True)\n",
    "X_train_img_paths_final = pd.concat([X_train_img_paths.reset_index(drop=True),\n",
    "                                     aug_labels_df['ImagePath']], ignore_index=True)\n",
    "y_train_final = pd.concat([y_train.reset_index(drop=True),\n",
    "                           aug_labels_df['Label']], ignore_index=True)\n",
    "\n",
    "class PADUFESDataset(Dataset):\n",
    "    def __init__(self, img_paths, meta_data, labels, transform=None):\n",
    "        self.img_paths = img_paths.reset_index(drop=True)\n",
    "        self.meta_data = meta_data.reset_index(drop=True)\n",
    "        self.labels = pd.Series(labels).reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        meta = torch.tensor(self.meta_data.iloc[idx].values.astype(np.float32))\n",
    "        label = torch.tensor(self.labels.iloc[idx], dtype=torch.long)\n",
    "        return image, meta, label\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # Random horizontal flip\n",
    "    transforms.RandomRotation(70),          \n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Color jitter\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = PADUFESDataset(X_train_img_paths_final, X_train_meta_final, y_train_final, transform=train_transform)\n",
    "val_dataset = PADUFESDataset(X_val_img_paths, X_val_meta, y_val, transform=val_test_transform)\n",
    "test_dataset = PADUFESDataset(X_test_img_paths, X_test_meta, y_test, transform=val_test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"\\n✅ Loaded Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ee67bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm  \n",
    "import torch.nn.functional as F\n",
    "\n",
    "class EarlyFusionModelMobileViT(nn.Module):\n",
    "    def __init__(self, input_dim_meta, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embed metadata to smaller spatial dimensions first\n",
    "        self.meta_embed = nn.Sequential(\n",
    "            nn.Linear(input_dim_meta, 64 * 64),  # Updated for mobilevit's smaller receptive field\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64 * 64),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        # Load MobileViT model\n",
    "        self.mobilevit = timm.create_model(\"mobilevit_s.cvnets_in1k\", pretrained=True, num_classes=num_classes)\n",
    "        \n",
    "        # Inspect the model to identify the first conv layer\n",
    "        # Modify the first conv layer to accept additional channel\n",
    "        first_conv = self.mobilevit.stem.conv  # `stem.conv` is the correct initial layer\n",
    "        self.mobilevit.stem.conv = nn.Conv2d(4, first_conv.out_channels, \n",
    "                                             kernel_size=first_conv.kernel_size, \n",
    "                                             stride=first_conv.stride, \n",
    "                                             padding=first_conv.padding, \n",
    "                                             bias=first_conv.bias)\n",
    "        \n",
    "        # Initialize new channel weights\n",
    "        with torch.no_grad():\n",
    "            self.mobilevit.stem.conv.weight.data[:, :3] = first_conv.weight.data\n",
    "            # Initialize the new channel with smaller weights to prevent dominating\n",
    "            self.mobilevit.stem.conv.weight.data[:, 3:] = first_conv.weight.data.mean(dim=1, keepdim=True) * 0.1\n",
    "\n",
    "    def forward(self, img, meta):\n",
    "        # Reshape metadata to image-like format\n",
    "        batch_size = img.shape[0]\n",
    "        meta_reshaped = self.meta_embed(meta).view(batch_size, 1, 64, 64)\n",
    "        \n",
    "        # Upsample to match image dimensions\n",
    "        meta_upsampled = F.interpolate(meta_reshaped, \n",
    "                                       size=(224, 224),  # MobileViT expects 256x256\n",
    "                                       mode='bilinear', \n",
    "                                       align_corners=False)\n",
    "        \n",
    "        # Early fusion\n",
    "        combined_input = torch.cat([img, meta_upsampled], dim=1)\n",
    "        \n",
    "        # Process through modified MobileViT\n",
    "        out = self.mobilevit(combined_input)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class EarlyFusionModelPvtV2(nn.Module):\n",
    "    def __init__(self, input_dim_meta, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embed metadata to smaller spatial dimensions first\n",
    "        self.meta_embed = nn.Sequential(\n",
    "            nn.Linear(input_dim_meta, 56 * 56),  # Smaller initial dimension\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(56 * 56),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        # Load PVT v2 model\n",
    "        self.pvt = timm.create_model(\"pvt_v2_b1\", pretrained=True, num_classes=num_classes)\n",
    "        \n",
    "        # Modify the first convolution layer to accept additional channel (4 instead of 3)\n",
    "        first_conv = self.pvt.patch_embed.proj\n",
    "        self.pvt.patch_embed.proj = nn.Conv2d(4, first_conv.out_channels, \n",
    "                                              kernel_size=first_conv.kernel_size,\n",
    "                                              stride=first_conv.stride,\n",
    "                                              padding=first_conv.padding,\n",
    "                                              bias=first_conv.bias is not None)\n",
    "        \n",
    "        # Initialize new channel weights\n",
    "        with torch.no_grad():\n",
    "            self.pvt.patch_embed.proj.weight.data[:, :3] = first_conv.weight.data\n",
    "            self.pvt.patch_embed.proj.weight.data[:, 3:] = first_conv.weight.data.mean(dim=1, keepdim=True) * 0.1\n",
    "\n",
    "    def forward(self, img, meta):\n",
    "        # Reshape metadata to image-like format\n",
    "        batch_size = img.shape[0]\n",
    "        meta_reshaped = self.meta_embed(meta).view(batch_size, 1, 56, 56)\n",
    "        \n",
    "        # Upsample to match image dimensions\n",
    "        meta_upsampled = F.interpolate(meta_reshaped, \n",
    "                                       size=(224, 224), \n",
    "                                       mode='bilinear', \n",
    "                                       align_corners=False)\n",
    "        \n",
    "        # Early fusion\n",
    "        combined_input = torch.cat([img, meta_upsampled], dim=1)\n",
    "        \n",
    "        # Process through modified PVT\n",
    "        out = self.pvt(combined_input)\n",
    "        return out\n",
    "\n",
    "input_dim_meta = X_train_meta.shape[1]\n",
    "num_classes = 6\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class EarlyFusionMobileNetV3(nn.Module):\n",
    "    def __init__(self, input_dim_meta, num_classes, model_size='large'):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embed metadata to smaller spatial dimensions\n",
    "        self.meta_embed = nn.Sequential(\n",
    "            nn.Linear(input_dim_meta, 56 * 56),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(56 * 56),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        # Load MobileNetV3\n",
    "        model_name = f'mobilenetv3_{model_size}'  # 'mobilenetv3_large' or 'mobilenetv3_small'\n",
    "        self.mobilenet = timm.create_model(\"mobilenetv3_large_100\", pretrained=True, num_classes=6)\n",
    "        \n",
    "        # Store the original first conv layer\n",
    "        first_conv = self.mobilenet.conv_stem\n",
    "        \n",
    "        # Modify first conv layer to accept 4 channels (RGB + metadata)\n",
    "        self.mobilenet.conv_stem = nn.Conv2d(\n",
    "            4, \n",
    "            first_conv.out_channels,\n",
    "            kernel_size=first_conv.kernel_size,\n",
    "            stride=first_conv.stride,\n",
    "            padding=first_conv.padding,\n",
    "            bias=False\n",
    "        )\n",
    "        \n",
    "        # Initialize new conv layer\n",
    "        with torch.no_grad():\n",
    "            # Copy weights for RGB channels\n",
    "            self.mobilenet.conv_stem.weight[:, :3] = first_conv.weight\n",
    "            # Initialize metadata channel with mean of RGB weights * small factor\n",
    "            self.mobilenet.conv_stem.weight[:, 3:] = first_conv.weight.mean(dim=1, keepdim=True) * 0.1\n",
    "        \n",
    "        # Modify classifier\n",
    "        in_features = self.mobilenet.classifier.in_features\n",
    "        self.mobilenet.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(in_features, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, img, meta):\n",
    "        batch_size = img.shape[0]\n",
    "        \n",
    "        # Process metadata\n",
    "        meta_features = self.meta_embed(meta)\n",
    "        meta_reshaped = meta_features.view(batch_size, 1, 56, 56)\n",
    "        \n",
    "        meta_upsampled = F.interpolate(\n",
    "            meta_reshaped,\n",
    "            size=(224, 224),\n",
    "            mode='bilinear',\n",
    "            align_corners=False\n",
    "        )\n",
    "        \n",
    "        # Concatenate along channel dimension\n",
    "        combined_input = torch.cat([img, meta_upsampled], dim=1)\n",
    "        \n",
    "        # Forward pass through MobileNetV3\n",
    "        output = self.mobilenet(combined_input)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "class EarlyFusionModelCoatNet(nn.Module):\n",
    "    def __init__(self, input_dim_meta, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embed metadata to smaller spatial dimensions\n",
    "        self.meta_embed = nn.Sequential(\n",
    "            nn.Linear(input_dim_meta, 56 * 56),  # Smaller initial dimension\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(56 * 56),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        # Load the CoAtNet model\n",
    "        self.coatnet = timm.create_model('coatnet_2_rw_224.sw_in12k_ft_in1k', pretrained=True, num_classes=num_classes)\n",
    "        \n",
    "        # Modify the first conv layer to accept 4 input channels\n",
    "        first_conv = self.coatnet.stem.conv1\n",
    "        self.coatnet.stem.conv1 = nn.Conv2d(4, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        \n",
    "        # Initialize new channel weights\n",
    "        with torch.no_grad():\n",
    "            self.coatnet.stem.conv1.weight.data[:, :3] = first_conv.weight.data\n",
    "            # Initialize the new channel with smaller weights\n",
    "            self.coatnet.stem.conv1.weight.data[:, 3:] = first_conv.weight.data.mean(dim=1, keepdim=True) * 0.1\n",
    "\n",
    "    def forward(self, img, meta):\n",
    "        # Reshape metadata to image-like format\n",
    "        batch_size = img.shape[0]\n",
    "        meta_reshaped = self.meta_embed(meta).view(batch_size, 1, 56, 56)\n",
    "        \n",
    "        # Upsample metadata to match image dimensions\n",
    "        meta_upsampled = F.interpolate(meta_reshaped, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "        \n",
    "        # Early fusion: Combine image and metadata as additional channel\n",
    "        combined_input = torch.cat([img, meta_upsampled], dim=1)\n",
    "        \n",
    "        out = self.coatnet(combined_input)\n",
    "        return out\n",
    "       \n",
    "\n",
    "class EarlyFusionModelXception(nn.Module):\n",
    "    def __init__(self, input_dim_meta, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embed metadata to smaller spatial dimensions first\n",
    "        self.meta_embed = nn.Sequential(\n",
    "            nn.Linear(input_dim_meta, 56 * 56),  # Smaller initial dimension\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(56 * 56),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        # Modified Xception\n",
    "        self.xception = timm.create_model('xception', pretrained=True)\n",
    "        # Modify first conv layer to accept additional channel\n",
    "        first_conv = self.xception.conv1\n",
    "        self.xception.conv1 = nn.Conv2d(4, 32, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        \n",
    "        # Modify the final classification layer\n",
    "        in_features = self.xception.fc.in_features  # Usually 2048 for Xception\n",
    "        self.xception.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(in_features, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Initialize new channel weights\n",
    "        with torch.no_grad():\n",
    "            self.xception.conv1.weight.data[:, :3] = first_conv.weight.data\n",
    "            # Initialize the new channel with smaller weights to prevent dominating\n",
    "            self.xception.conv1.weight.data[:, 3:] = first_conv.weight.data.mean(dim=1, keepdim=True) * 0.1\n",
    "        \n",
    "    def forward(self, img, meta):\n",
    "        # Reshape metadata to image-like format\n",
    "        batch_size = img.shape[0]\n",
    "        meta_reshaped = self.meta_embed(meta).view(batch_size, 1, 56, 56)\n",
    "        \n",
    "        # Upsample to match image dimensions\n",
    "        meta_upsampled = F.interpolate(meta_reshaped, \n",
    "                                     size=(224, 224), \n",
    "                                     mode='bilinear', \n",
    "                                     align_corners=False)\n",
    "        \n",
    "        # Early fusion\n",
    "        combined_input = torch.cat([img, meta_upsampled], dim=1)\n",
    "        \n",
    "        # Process through modified Xception\n",
    "        out = self.xception(combined_input)\n",
    "        return out\n",
    "        \n",
    "\n",
    "class EarlyFusionModelVgg16(nn.Module):\n",
    "    def __init__(self, input_dim_meta, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embed metadata to smaller spatial dimensions first\n",
    "        self.meta_embed = nn.Sequential(\n",
    "            nn.Linear(input_dim_meta, 56 * 56),  # Smaller initial dimension\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(56 * 56),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        # Modified VGG16\n",
    "        self.vgg16 = timm.create_model('vgg16', pretrained=True, num_classes = 6)\n",
    "        \n",
    "        # Modify the first convolutional layer to accept 4 input channels\n",
    "        first_conv = self.vgg16.features[0]\n",
    "        self.vgg16.features[0] = nn.Conv2d(\n",
    "            4, 64, kernel_size=3, stride=1, padding=1, bias=False\n",
    "        )\n",
    "        \n",
    "        # Modify the classifier to match the number of classes\n",
    "        in_features = self.vgg16.get_classifier().in_features  # Use `get_classifier()` to access the classifier\n",
    "        self.vgg16.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(in_features, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4096, num_classes)\n",
    "        )\n",
    "\n",
    "        # Initialize new channel weights\n",
    "        with torch.no_grad():\n",
    "            self.vgg16.features[0].weight.data[:, :3] = first_conv.weight.data\n",
    "            # Initialize the new channel with smaller weights\n",
    "            self.vgg16.features[0].weight.data[:, 3:] = first_conv.weight.data.mean(dim=1, keepdim=True) * 0.1\n",
    "        \n",
    "    def forward(self, img, meta):\n",
    "        # Reshape metadata to image-like format\n",
    "        batch_size = img.shape[0]\n",
    "        meta_reshaped = self.meta_embed(meta).view(batch_size, 1, 56, 56)\n",
    "        \n",
    "        # Upsample to match image dimensions\n",
    "        meta_upsampled = F.interpolate(\n",
    "            meta_reshaped, size=(224, 224), mode='bilinear', align_corners=False\n",
    "        )\n",
    "        \n",
    "        # Early fusion\n",
    "        combined_input = torch.cat([img, meta_upsampled], dim=1)\n",
    "        \n",
    "        # Process through modified VGG16\n",
    "        out = self.vgg16(combined_input)\n",
    "        return out\n",
    "\n",
    "\n",
    "from transformers import AutoModelForImageClassification, AutoConfig\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForImageClassification, AutoImageProcessor\n",
    "\n",
    "class EarlyFusionModelSwinTiny(nn.Module):\n",
    "    def __init__(self, input_dim_meta, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embed metadata to smaller spatial dimensions\n",
    "        self.meta_embed = nn.Sequential(\n",
    "            nn.Linear(input_dim_meta, 56 * 56),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(56 * 56),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        # Load Swin Transformer from Hugging Face\n",
    "        self.swin_transformer = AutoModelForImageClassification.from_pretrained(\n",
    "            \"microsoft/swin-tiny-patch4-window7-224\",\n",
    "            ignore_mismatched_sizes=True\n",
    "        )\n",
    "        \n",
    "        # Replace the classifier to match the number of classes\n",
    "        in_features = self.swin_transformer.classifier.in_features\n",
    "        self.swin_transformer.classifier = nn.Linear(in_features, num_classes)\n",
    "        \n",
    "        # Access and modify the patch embedding layer\n",
    "        patch_embed_layer = self.swin_transformer.swin.embeddings.patch_embeddings.projection\n",
    "        self.swin_transformer.swin.embeddings.patch_embeddings.projection = nn.Conv2d(\n",
    "            in_channels=4,  # Update to accept 4 input channels\n",
    "            out_channels=patch_embed_layer.out_channels,\n",
    "            kernel_size=patch_embed_layer.kernel_size,\n",
    "            stride=patch_embed_layer.stride,\n",
    "            padding=patch_embed_layer.padding,\n",
    "            bias=patch_embed_layer.bias is not None,  # Ensure bias is a boolean\n",
    "        )\n",
    "        \n",
    "        # Initialize weights for the new input channel\n",
    "        with torch.no_grad():\n",
    "            self.swin_transformer.swin.embeddings.patch_embeddings.projection.weight[:, :3] = patch_embed_layer.weight\n",
    "            self.swin_transformer.swin.embeddings.patch_embeddings.projection.weight[:, 3:] = patch_embed_layer.weight.mean(dim=1, keepdim=True)\n",
    "\n",
    "    def forward(self, img, meta):\n",
    "        # Embed metadata\n",
    "        batch_size = img.size(0)\n",
    "        meta_reshaped = self.meta_embed(meta).view(batch_size, 1, 56, 56)\n",
    "        \n",
    "        # Upsample metadata to match image dimensions\n",
    "        meta_upsampled = F.interpolate(meta_reshaped, size=(224, 224), mode=\"bilinear\", align_corners=False)\n",
    "        \n",
    "        # Concatenate the image and metadata\n",
    "        combined_input = torch.cat([img, meta_upsampled], dim=1)\n",
    "        \n",
    "        # Pass through Swin Transformer\n",
    "        outputs = self.swin_transformer(pixel_values=combined_input)\n",
    "        return outputs.logits\n",
    "\n",
    "\n",
    "class EarlyFusionModelEfficientViT(nn.Module):\n",
    "    def __init__(self, input_dim_meta, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embed metadata to smaller spatial dimensions first\n",
    "        self.meta_embed = nn.Sequential(\n",
    "            nn.Linear(input_dim_meta, 56 * 56),  # Smaller initial dimension\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(56 * 56),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        self.efficientvit = timm.create_model(\"efficientvit_b0.r224_in1k\", pretrained=True, num_classes=6)\n",
    "        \n",
    "        # Modify the first conv layer to accept additional channel\n",
    "        first_conv = self.efficientvit.stem.in_conv.conv\n",
    "        self.efficientvit.stem.in_conv.conv = nn.Conv2d(4, 8, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        \n",
    "        # Initialize new channel weights\n",
    "        with torch.no_grad():\n",
    "            self.efficientvit.stem.in_conv.conv.weight.data[:, :3] = first_conv.weight.data\n",
    "            # Initialize the new channel with smaller weights to prevent dominating\n",
    "            self.efficientvit.stem.in_conv.conv.weight.data[:, 3:] = first_conv.weight.data.mean(dim=1, keepdim=True) * 0.1\n",
    "\n",
    "    def forward(self, img, meta):\n",
    "        # Reshape metadata to image-like format\n",
    "        batch_size = img.shape[0]\n",
    "        meta_reshaped = self.meta_embed(meta).view(batch_size, 1, 56, 56)\n",
    "        \n",
    "        # Upsample to match image dimensions\n",
    "        meta_upsampled = F.interpolate(meta_reshaped, \n",
    "                                     size=(224, 224), \n",
    "                                     mode='bilinear', \n",
    "                                     align_corners=False)\n",
    "        \n",
    "        # Early fusion\n",
    "        combined_input = torch.cat([img, meta_upsampled], dim=1)\n",
    "        \n",
    "        # Process through modified EfficientViT\n",
    "        out = self.efficientvit(combined_input)\n",
    "        return out\n",
    "        \n",
    "\n",
    "\n",
    "class EarlyFusionModelDenseNet(nn.Module):\n",
    "    def __init__(self, input_dim_meta, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.meta_embed = nn.Sequential(\n",
    "            nn.Linear(input_dim_meta, 56 * 56),  # Smaller initial dimension\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(56 * 56),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        # Modified DenseNet121\n",
    "        self.densenet = timm.create_model('densenet121', pretrained=True)\n",
    "        \n",
    "        first_conv = self.densenet.features.conv0\n",
    "        self.densenet.features.conv0 = nn.Conv2d(\n",
    "            4, 64, kernel_size=7, stride=2, padding=3, bias=False\n",
    "        )\n",
    "        \n",
    "        # Modify the final classification layer\n",
    "        in_features = self.densenet.classifier.in_features\n",
    "        self.densenet.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(in_features, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Initialize new channel weights\n",
    "        with torch.no_grad():\n",
    "            self.densenet.features.conv0.weight.data[:, :3] = first_conv.weight.data\n",
    "            self.densenet.features.conv0.weight.data[:, 3:] = first_conv.weight.data.mean(dim=1, keepdim=True) * 0.1\n",
    "        \n",
    "    def forward(self, img, meta):\n",
    "        # Reshape metadata to image-like format\n",
    "        batch_size = img.shape[0]\n",
    "        meta_reshaped = self.meta_embed(meta).view(batch_size, 1, 56, 56)\n",
    "        \n",
    "        # Upsample to match image dimensions\n",
    "        meta_upsampled = F.interpolate(\n",
    "            meta_reshaped, size=(224, 224), mode='bilinear', align_corners=False\n",
    "        )\n",
    "        \n",
    "        # Early fusion\n",
    "        combined_input = torch.cat([img, meta_upsampled], dim=1)\n",
    "        \n",
    "        # Process through modified DenseNet121\n",
    "        out = self.densenet(combined_input)\n",
    "        return out\n",
    "        \n",
    "\n",
    "class EarlyFusionModelInceptionResnetv2(nn.Module):\n",
    "    def __init__(self, input_dim_meta, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embed metadata to smaller spatial dimensions\n",
    "        self.meta_embed = nn.Sequential(\n",
    "            nn.Linear(input_dim_meta, 56 * 56),  # Smaller initial dimension\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(56 * 56),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        # Load the InceptionResNetV2 model from timm\n",
    "        self.inception_resnet = timm.create_model('inception_resnet_v2', pretrained=True)\n",
    "        \n",
    "        # Modify the first convolutional block (conv2d_1a) to accept 4 channels\n",
    "        first_conv = self.inception_resnet.conv2d_1a.conv\n",
    "        self.inception_resnet.conv2d_1a.conv = nn.Conv2d(\n",
    "            4, 32, kernel_size=(3, 3), stride=(2, 2), bias=False\n",
    "        )\n",
    "        \n",
    "        # Copy weights for the first 3 channels and initialize the 4th channel\n",
    "        with torch.no_grad():\n",
    "            self.inception_resnet.conv2d_1a.conv.weight[:, :3] = first_conv.weight\n",
    "            # Initialize the new channel with the mean of existing weights\n",
    "            self.inception_resnet.conv2d_1a.conv.weight[:, 3:] = first_conv.weight.mean(dim=1, keepdim=True) * 0.1\n",
    "        \n",
    "        # Modify the final classification head to match the number of classes\n",
    "        in_features = self.inception_resnet.classif.in_features\n",
    "        self.inception_resnet.classif = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(in_features, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, img, meta):\n",
    "        # Reshape metadata to image-like format\n",
    "        batch_size = img.shape[0]\n",
    "        meta_reshaped = self.meta_embed(meta).view(batch_size, 1, 56, 56)\n",
    "        \n",
    "        # Upsample metadata to match image dimensions\n",
    "        meta_upsampled = F.interpolate(meta_reshaped, \n",
    "                                       size=(224, 224), \n",
    "                                       mode='bilinear', \n",
    "                                       align_corners=False)\n",
    "        \n",
    "        # Concatenate image and metadata along the channel dimension\n",
    "        combined_input = torch.cat([img, meta_upsampled], dim=1)\n",
    "        \n",
    "        # Process through the modified InceptionResNetV2\n",
    "        out = self.inception_resnet(combined_input)\n",
    "        return out\n",
    "        \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_cluster import knn_graph\n",
    "import timm\n",
    "\n",
    "\n",
    "class EarlyFusionWithDynamicGCN(nn.Module):\n",
    "    def __init__(self, input_dim_meta, num_classes, k=8):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "\n",
    "        # === GCN with residual ===\n",
    "        self.gcn1 = GCNConv(input_dim_meta, 64)\n",
    "        self.gcn2 = GCNConv(64, 32)\n",
    "        self.res_proj = nn.Linear(64, 32)\n",
    "\n",
    "        # === Metadata to image (56×56 = 3136) ===\n",
    "        self.meta_to_image = nn.Sequential(\n",
    "            nn.Linear(32, 56 * 56),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(56 * 56),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "        # === MobileViT Backbone Modification ===\n",
    "        self.mobilevit = timm.create_model(\"mobilevit_s.cvnets_in1k\", pretrained=True, num_classes=0)\n",
    "\n",
    "        stem_conv = self.mobilevit.stem.conv\n",
    "        new_conv = nn.Conv2d(4, stem_conv.out_channels,\n",
    "                             kernel_size=stem_conv.kernel_size,\n",
    "                             stride=stem_conv.stride,\n",
    "                             padding=stem_conv.padding,\n",
    "                             bias=stem_conv.bias is not None)\n",
    "        with torch.no_grad():\n",
    "            new_conv.weight[:, :3] = stem_conv.weight\n",
    "            new_conv.weight[:, 3:] = stem_conv.weight.mean(dim=1, keepdim=True) * 0.1\n",
    "            if stem_conv.bias is not None:\n",
    "                new_conv.bias = stem_conv.bias\n",
    "        self.mobilevit.stem.conv = new_conv\n",
    "\n",
    "        self.mobilevit.stages = nn.Sequential(*list(self.mobilevit.stages.children())[:4])\n",
    "        self.mobilevit.final_conv = nn.Identity()\n",
    "        self.mobilevit.head = nn.Identity()\n",
    "\n",
    "        # === Post Conv ===\n",
    "        self.post_conv = nn.Sequential(\n",
    "            nn.Conv2d(128, 160, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(160),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # === Classifier ===\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(160, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, img, meta, batch_idx):\n",
    "        B = meta.size(0)\n",
    "\n",
    "        # Dynamic KNN graph\n",
    "        edge_index = knn_graph(meta, k=self.k, batch=batch_idx)\n",
    "\n",
    "        # GCN + residual\n",
    "        x1 = F.relu(self.gcn1(meta, edge_index))\n",
    "        x2 = F.relu(self.gcn2(x1, edge_index) + self.res_proj(x1))\n",
    "        x_meta = x2  # [B, 32]\n",
    "\n",
    "        # Metadata → image → reshape\n",
    "        meta_img = self.meta_to_image(x_meta).view(B, 1, 56, 56)\n",
    "        meta_img = F.interpolate(meta_img, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "\n",
    "        # Early fusion\n",
    "        x = torch.cat([img, meta_img], dim=1)  # [B, 4, 224, 224]\n",
    "\n",
    "        # CNN\n",
    "        x_cnn = self.mobilevit.stem(x)\n",
    "        x_cnn = self.mobilevit.stages(x_cnn)\n",
    "        x_cnn = self.post_conv(x_cnn)\n",
    "        x_cnn = self.pool(x_cnn).view(B, -1)  # [B, 160]                                            # [B, 160]\n",
    "\n",
    "        return self.classifier(x_cnn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e1dfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import numpy as np\n",
    "from ptflops import get_model_complexity_info\n",
    "\n",
    "# ============================================================\n",
    "# SETTINGS\n",
    "# ============================================================\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "meta_dim = 59\n",
    "num_classes = 6\n",
    "image_size = (3, 224, 224)\n",
    "\n",
    "PAD_ROOT = r\"D:\\PAD-UFES\"\n",
    "STUDENT_CKPT = r\"C:\\Users\\User\\MDY Research\\With Augmentation\\Concatenation\\Adasyn\\best_student_model_final2.pth\"\n",
    "\n",
    "# ============================================================\n",
    "# UTILS\n",
    "# ============================================================\n",
    "\n",
    "def count_parameters(model, trainable_only=False):\n",
    "    if trainable_only:\n",
    "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    else:\n",
    "        return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "\n",
    "\n",
    "class FusionWrapper(nn.Module):\n",
    "    \"\"\"\n",
    "    Wraps 2-input (img, meta) models so ptflops sees a single image input.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, meta_dim):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.meta_dim = meta_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        dummy_meta = torch.randn(x.size(0), self.meta_dim).to(x.device)\n",
    "        return self.model(x, dummy_meta)\n",
    "\n",
    "\n",
    "class GCN2InputWrapper(nn.Module):\n",
    "    \"\"\"\n",
    "    Wraps EarlyFusionWithDynamicGCN so it behaves like forward(img, meta).\n",
    "    Batch indices are synthesized internally.\n",
    "    \"\"\"\n",
    "    def __init__(self, gcn_model, meta_dim):\n",
    "        super().__init__()\n",
    "        self.gcn_model = gcn_model\n",
    "        self.meta_dim = meta_dim\n",
    "\n",
    "    def forward(self, img, meta):\n",
    "        B = meta.size(0)\n",
    "        batch_idx = torch.arange(B, device=meta.device)\n",
    "        return self.gcn_model(img, meta, batch_idx)\n",
    "\n",
    "\n",
    "def compute_flops(model, meta_dim):\n",
    "    wrapper = FusionWrapper(model, meta_dim).to(device)\n",
    "    with torch.no_grad():\n",
    "        flops, _ = get_model_complexity_info(\n",
    "            wrapper,\n",
    "            image_size,\n",
    "            as_strings=False,\n",
    "            print_per_layer_stat=False,\n",
    "            verbose=False\n",
    "        )\n",
    "    return float(flops / 1e9)  # GFLOPs\n",
    "\n",
    "\n",
    "def measure_gpu_latency(model, meta_dim, runs=200, warmup=30):\n",
    "    if not torch.cuda.is_available():\n",
    "        return None, None, None\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    model.eval().to(device)\n",
    "\n",
    "    dummy_img = torch.randn(1, *image_size, device=device)\n",
    "    dummy_meta = torch.randn(1, meta_dim, device=device)\n",
    "\n",
    "    # warmup\n",
    "    for _ in range(warmup):\n",
    "        _ = model(dummy_img, dummy_meta)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    start_evt = torch.cuda.Event(enable_timing=True)\n",
    "    end_evt = torch.cuda.Event(enable_timing=True)\n",
    "    times = []\n",
    "\n",
    "    for _ in range(runs):\n",
    "        start_evt.record()\n",
    "        _ = model(dummy_img, dummy_meta)\n",
    "        end_evt.record()\n",
    "        torch.cuda.synchronize()\n",
    "        times.append(start_evt.elapsed_time(end_evt))  # ms\n",
    "\n",
    "    times = np.array(times)\n",
    "    mean = float(times.mean())\n",
    "    std = float(times.std())\n",
    "    fps = float(1000.0 / mean)\n",
    "    return mean, std, fps\n",
    "\n",
    "\n",
    "def measure_cpu_latency(model, meta_dim, runs=100, warmup=20):\n",
    "    model_cpu = model.cpu()\n",
    "    model_cpu.eval()\n",
    "\n",
    "    dummy_img = torch.randn(1, *image_size)\n",
    "    dummy_meta = torch.randn(1, meta_dim)\n",
    "\n",
    "    for _ in range(warmup):\n",
    "        _ = model_cpu(dummy_img, dummy_meta)\n",
    "\n",
    "    times = []\n",
    "    for _ in range(runs):\n",
    "        start = time.perf_counter()\n",
    "        _ = model_cpu(dummy_img, dummy_meta)\n",
    "        end = time.perf_counter()\n",
    "        times.append((end - start) * 1000.0)\n",
    "\n",
    "    times = np.array(times)\n",
    "    mean = float(times.mean())\n",
    "    std = float(times.std())\n",
    "    fps = float(1000.0 / mean)\n",
    "    return mean, std, fps\n",
    "\n",
    "\n",
    "def load_model(model_class, ckpt_path, name, wrap_gcn=False):\n",
    "    \"\"\"\n",
    "    Creates model_class(input_dim_meta=59, num_classes=6), loads checkpoint,\n",
    "    optionally wraps GCN so it behaves like (img, meta).\n",
    "    \"\"\"\n",
    "    print(f\"\\n[LOAD] {name} from {ckpt_path}\")\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    model = model_class(meta_dim, num_classes)\n",
    "    state = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    model.load_state_dict(state)\n",
    "    if wrap_gcn:\n",
    "        model = GCN2InputWrapper(model, meta_dim)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    print(\"[OK] Model loaded.\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def benchmark_model(model, name, meta_dim, count_all_params=False):\n",
    "    \"\"\"\n",
    "    Given a ready-to-use 2-input (img, meta) model, compute all stats.\n",
    "    If count_all_params=True, counts ALL parameters (ignoring requires_grad),\n",
    "    which is what we want for the teacher ensemble.\n",
    "    \"\"\"\n",
    "    print(f\"\\n======= Benchmarking: {name} =======\")\n",
    "\n",
    "    # trainable_only=False for teacher, True for others via this flag\n",
    "    trainable_only = not count_all_params\n",
    "    params_m = count_parameters(model, trainable_only=trainable_only) / 1e6\n",
    "    print(f\"Params: {params_m:.3f} M\")\n",
    "\n",
    "    flops_g = compute_flops(model, meta_dim)\n",
    "    print(f\"FLOPs: {flops_g:.3f} G\")\n",
    "\n",
    "    gpu_mean, gpu_std, gpu_fps = measure_gpu_latency(model, meta_dim)\n",
    "    if gpu_mean is not None:\n",
    "        print(f\"GPU Latency: {gpu_mean:.3f} ± {gpu_std:.3f} ms  |  FPS: {gpu_fps:.1f}\")\n",
    "    else:\n",
    "        print(\"GPU Latency: N/A\")\n",
    "\n",
    "    cpu_mean, cpu_std, cpu_fps = measure_cpu_latency(model, meta_dim)\n",
    "    print(f\"CPU Latency: {cpu_mean:.3f} ± {cpu_std:.3f} ms  |  FPS: {cpu_fps:.1f}\")\n",
    "\n",
    "    return {\n",
    "        \"model\": name,\n",
    "        \"params_M\": params_m,\n",
    "        \"flops_G\": flops_g,\n",
    "        \"gpu_latency_mean_ms\": gpu_mean,\n",
    "        \"gpu_latency_std_ms\": gpu_std,\n",
    "        \"gpu_fps\": gpu_fps,\n",
    "        \"cpu_latency_mean_ms\": cpu_mean,\n",
    "        \"cpu_latency_std_ms\": cpu_std,\n",
    "        \"cpu_fps\": cpu_fps,\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# TEACHER ENSEMBLE MODEL\n",
    "# ============================================================\n",
    "\n",
    "class TeacherModel(nn.Module):\n",
    "    def __init__(self, models, ensemble_method=\"mean\"):\n",
    "        super().__init__()\n",
    "        self.models = nn.ModuleList(models)\n",
    "        self.ensemble_method = ensemble_method\n",
    "        for m in self.models:\n",
    "            m.eval()\n",
    "            for p in m.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "    def forward(self, img, meta):\n",
    "        outputs = []\n",
    "        with torch.no_grad():\n",
    "            for m in self.models:\n",
    "                outputs.append(m(img, meta))\n",
    "        outputs = torch.stack(outputs, dim=0)  # [n_models, B, C]\n",
    "        if self.ensemble_method == \"mean\":\n",
    "            return outputs.mean(dim=0)\n",
    "        else:\n",
    "            # simple majority vote\n",
    "            _, preds = torch.max(outputs, dim=2)\n",
    "            return preds.mode(dim=0).values\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# BENCHMARK ALL MODELS\n",
    "# (Assumes all EarlyFusion* classes and EarlyFusionWithDynamicGCN\n",
    "#  are defined exactly as in your message.)\n",
    "# ============================================================\n",
    "\n",
    "results = []\n",
    "\n",
    "# 1) MobileViT + MLP\n",
    "mv_ckpt = rf\"{PAD_ROOT}\\best_early_fusion_mobilevitDA.pth\"\n",
    "mv_model = load_model(EarlyFusionModelMobileViT, mv_ckpt, \"MobileViT + MLP\")\n",
    "results.append(benchmark_model(mv_model, \"MobileViT + MLP\", meta_dim))\n",
    "\n",
    "# 2) PVTv2 + MLP\n",
    "pvt_ckpt = rf\"{PAD_ROOT}\\best_early_fusion_pvtv2smoteDA.pth\"\n",
    "pvt_model = load_model(EarlyFusionModelPvtV2, pvt_ckpt, \"PVTv2 + MLP\")\n",
    "results.append(benchmark_model(pvt_model, \"PVTv2 + MLP\", meta_dim))\n",
    "\n",
    "# 3) MobileNetV3 + MLP\n",
    "mn_ckpt = rf\"{PAD_ROOT}\\best_early_fusion_mobilenetv3smoteDA.pth\"\n",
    "mn_model = load_model(EarlyFusionMobileNetV3, mn_ckpt, \"MobileNetV3 + MLP\")\n",
    "results.append(benchmark_model(mn_model, \"MobileNetV3 + MLP\", meta_dim))\n",
    "\n",
    "# 4) DenseNet121 + MLP\n",
    "dn_ckpt = rf\"{PAD_ROOT}\\best_early_fusion_densenetsmoteDA.pth\"\n",
    "dn_model = load_model(EarlyFusionModelDenseNet, dn_ckpt, \"DenseNet121 + MLP\")\n",
    "results.append(benchmark_model(dn_model, \"DenseNet121 + MLP\", meta_dim))\n",
    "\n",
    "# 5) EfficientViT + MLP\n",
    "ev_ckpt = rf\"{PAD_ROOT}\\best_early_fusion_efficientvitDA.pth\"\n",
    "ev_model = load_model(EarlyFusionModelEfficientViT, ev_ckpt, \"EfficientViT + MLP\")\n",
    "results.append(benchmark_model(ev_model, \"EfficientViT + MLP\", meta_dim))\n",
    "\n",
    "# 6) InceptionResNetV2 + MLP\n",
    "ir_ckpt = rf\"{PAD_ROOT}\\best_early_fusion_inceptionresnetv2smoteDA.pth\"\n",
    "ir_model = load_model(EarlyFusionModelInceptionResnetv2, ir_ckpt, \"InceptionResNetV2 + MLP\")\n",
    "results.append(benchmark_model(ir_model, \"InceptionResNetV2 + MLP\", meta_dim))\n",
    "\n",
    "# 7) VGG16 + MLP\n",
    "vgg_ckpt = rf\"{PAD_ROOT}\\best_early_fusion_vgg16smoteDA.pth\"\n",
    "vgg_model = load_model(EarlyFusionModelVgg16, vgg_ckpt, \"VGG16 + MLP\")\n",
    "results.append(benchmark_model(vgg_model, \"VGG16 + MLP\", meta_dim))\n",
    "\n",
    "# 8) Xception + MLP\n",
    "xc_ckpt = rf\"{PAD_ROOT}\\best_early_fusion_xceptionsmoteDA.pth\"\n",
    "xc_model = load_model(EarlyFusionModelXception, xc_ckpt, \"Xception + MLP\")\n",
    "results.append(benchmark_model(xc_model, \"Xception + MLP\", meta_dim))\n",
    "\n",
    "# 9) Swin-Tiny + MLP\n",
    "swin_ckpt = rf\"{PAD_ROOT}\\best_early_fusion_swintinysmoteDA.pth\"\n",
    "swin_model = load_model(EarlyFusionModelSwinTiny, swin_ckpt, \"Swin-Tiny + MLP\")\n",
    "results.append(benchmark_model(swin_model, \"Swin-Tiny + MLP\", meta_dim))\n",
    "\n",
    "# 10) CoAtNet-2 + MLP\n",
    "coat_ckpt = rf\"{PAD_ROOT}\\best_early_fusion_coatnetsmoteDA.pth\"\n",
    "coat_model = load_model(EarlyFusionModelCoatNet, coat_ckpt, \"CoAtNet-2 + MLP\")\n",
    "results.append(benchmark_model(coat_model, \"CoAtNet-2 + MLP\", meta_dim))\n",
    "\n",
    "# 11) TabFusion (GCN Student)\n",
    "student_raw = EarlyFusionWithDynamicGCN(meta_dim, num_classes)\n",
    "student_state = torch.load(STUDENT_CKPT, map_location=\"cpu\")\n",
    "student_raw.load_state_dict(student_state)\n",
    "student_model = GCN2InputWrapper(student_raw, meta_dim).to(device).eval()\n",
    "results.append(benchmark_model(student_model, \"TabFusion (GCN Student)\", meta_dim))\n",
    "\n",
    "# 12) Teacher Ensemble (MobileViT + PVTv2)\n",
    "teacher_models = [mv_model, pvt_model]  # reuse the ones we already loaded\n",
    "teacher = TeacherModel(teacher_models, ensemble_method=\"mean\").to(device)\n",
    "results.append(\n",
    "    benchmark_model(\n",
    "        teacher,\n",
    "        name=\"Teacher Ensemble (MobileViT + PVTv2)\",\n",
    "        meta_dim=59,\n",
    "        count_all_params=True   # <---- IMPORTANT\n",
    "    )\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# SUMMARY\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n================ FINAL SUMMARY ================\\n\")\n",
    "for r in results:\n",
    "    print(r)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f029026f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c2eabb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc914940",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4428b5d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
