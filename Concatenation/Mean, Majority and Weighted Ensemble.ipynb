{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf1cf7d-e12a-469e-b3c6-c37dee98db7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)  # If you are using CUDA\n",
    "torch.backends.cudnn.deterministic = True  # For deterministic results\n",
    "torch.backends.cudnn.benchmark = False  # For consistency across different environments\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "IMAGE_DIR = 'D:\\\\PAD-UFES\\\\images'  \n",
    "METADATA_PATH = 'D:\\\\PAD-UFES\\\\metadata.csv'\n",
    "\n",
    "metadata = pd.read_csv(METADATA_PATH)\n",
    "\n",
    "def preprocess_metadata(metadata):\n",
    "    metadata = metadata.fillna('UNK')\n",
    "\n",
    "    boolean_cols = [\n",
    "        'smoke',\n",
    "        'drink',\n",
    "        'pesticide',\n",
    "        'skin_cancer_history',\n",
    "        'cancer_history',\n",
    "        'has_piped_water',\n",
    "        'has_sewage_system',\n",
    "        'itch',\n",
    "        'grew',\n",
    "        'hurt',\n",
    "        'changed',\n",
    "        'bleed',\n",
    "        'elevation',\n",
    "        'biopsed',\n",
    "    ]\n",
    "    # Ensure columns are strings and lowercase\n",
    "    for col in boolean_cols:\n",
    "        metadata[col] = metadata[col].astype(str).str.lower()\n",
    "    \n",
    "    # Map boolean columns to 1/0/-1\n",
    "    boolean_mapping = {'true': 1, 'false': 0, 'unk': -1}\n",
    "    for col in boolean_cols:\n",
    "        metadata[col] = metadata[col].map(boolean_mapping)\n",
    "    \n",
    "    # Handle categorical variables\n",
    "    categorical_cols = [\n",
    "        'background_father',\n",
    "        'background_mother',\n",
    "        'gender',\n",
    "        'region',\n",
    "        'diagnostic',\n",
    "    ]\n",
    "    # Convert categorical columns to string\n",
    "    for col in categorical_cols:\n",
    "        metadata[col] = metadata[col].astype(str)\n",
    "    \n",
    "    # One-hot encode categorical variables\n",
    "    metadata_encoded = pd.get_dummies(metadata[categorical_cols])\n",
    "    \n",
    "    # Normalize numerical variables\n",
    "    numerical_cols = ['age', 'fitspatrick', 'diameter_1', 'diameter_2']\n",
    "    # Ensure numerical columns are numeric\n",
    "    for col in numerical_cols:\n",
    "        metadata[col] = pd.to_numeric(metadata[col], errors='coerce')\n",
    "    # Fill NaNs in numerical columns with the mean\n",
    "    metadata[numerical_cols] = metadata[numerical_cols].fillna(metadata[numerical_cols].mean())\n",
    "    # Scale numerical columns\n",
    "    scaler = StandardScaler()\n",
    "    metadata_numeric = metadata[numerical_cols]\n",
    "    metadata_numeric_scaled = pd.DataFrame(\n",
    "        scaler.fit_transform(metadata_numeric), columns=numerical_cols\n",
    "    )\n",
    "    \n",
    "    # Combine all metadata features\n",
    "    metadata_processed = pd.concat(\n",
    "        [metadata_numeric_scaled.reset_index(drop=True),\n",
    "         metadata_encoded.reset_index(drop=True),\n",
    "         metadata[boolean_cols].reset_index(drop=True)], axis=1\n",
    "    )\n",
    "    \n",
    "    return metadata_processed\n",
    "\n",
    "# Preprocess metadata\n",
    "metadata_processed = preprocess_metadata(metadata)\n",
    "\n",
    "def get_image_paths(metadata, image_dir):\n",
    "    image_paths = []\n",
    "    for idx, row in metadata.iterrows():\n",
    "        filename = row['img_id']\n",
    "        # Ensure filename is a string\n",
    "        filename = str(filename)\n",
    "        # Check if filename has an extension\n",
    "        if not filename.endswith(('.jpg', '.jpeg', '.png')):\n",
    "            # Try common extensions\n",
    "            possible_extensions = ['.jpg', '.jpeg', '.png']\n",
    "            found = False\n",
    "            for ext in possible_extensions:\n",
    "                filepath = os.path.join(image_dir, filename + ext)\n",
    "                if os.path.isfile(filepath):\n",
    "                    image_paths.append(filepath)\n",
    "                    found = True\n",
    "                    break\n",
    "            if not found:\n",
    "                print(f\"Image file not found for ID: {filename}\")\n",
    "                image_paths.append(None)\n",
    "        else:\n",
    "            filepath = os.path.join(image_dir, filename)\n",
    "            if os.path.isfile(filepath):\n",
    "                image_paths.append(filepath)\n",
    "            else:\n",
    "                print(f\"Image file not found: {filepath}\")\n",
    "                image_paths.append(None)\n",
    "    metadata['ImagePath'] = image_paths\n",
    "    return metadata\n",
    "\n",
    "metadata = get_image_paths(metadata, IMAGE_DIR)\n",
    "\n",
    "# Remove entries with missing images\n",
    "metadata = metadata[metadata['ImagePath'].notnull()]\n",
    "metadata_processed = metadata_processed.loc[metadata.index].reset_index(drop=True)\n",
    "metadata = metadata.reset_index(drop=True)\n",
    "\n",
    "# Drop diagnostic-related columns from features\n",
    "diagnostic_cols = ['diagnostic_ACK', 'diagnostic_BCC', 'diagnostic_MEL', 'diagnostic_NEV', 'diagnostic_SCC', 'diagnostic_SEK']\n",
    "metadata_processed = metadata_processed.drop(columns=diagnostic_cols)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(metadata['diagnostic'])\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# Split data into features and labels\n",
    "X_meta = metadata_processed.reset_index(drop=True)\n",
    "X_img_paths = metadata['ImagePath'].reset_index(drop=True)\n",
    "y = pd.Series(y_encoded)\n",
    "\n",
    "X_train_meta, X_temp_meta, X_train_img_paths, X_temp_img_paths, y_train, y_temp = train_test_split(\n",
    "    X_meta,\n",
    "    X_img_paths,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "X_val_meta, X_test_meta, X_val_img_paths, X_test_img_paths, y_val, y_test = train_test_split(\n",
    "    X_temp_meta,\n",
    "    X_temp_img_paths,\n",
    "    y_temp,\n",
    "    test_size=0.5,\n",
    "    random_state=42,\n",
    "    stratify=y_temp\n",
    ")\n",
    "\n",
    "# Load augmented metadata + image paths\n",
    "aug_meta_df   = pd.read_csv(\"D:/PAD-UFES/augmented_metadata.csv\")\n",
    "aug_labels_df = pd.read_csv(\"D:/PAD-UFES/augmented_labels.csv\")\n",
    "\n",
    "# Combine augmented samples with training set\n",
    "X_train_meta_final = pd.concat([X_train_meta, aug_meta_df], ignore_index=True)\n",
    "X_train_img_paths_final = pd.concat([X_train_img_paths.reset_index(drop=True),\n",
    "                                     aug_labels_df['ImagePath']], ignore_index=True)\n",
    "y_train_final = pd.concat([y_train.reset_index(drop=True),\n",
    "                           aug_labels_df['Label']], ignore_index=True)\n",
    "\n",
    "class PADUFESDataset(Dataset):\n",
    "    def __init__(self, img_paths, meta_data, labels, transform=None):\n",
    "        self.img_paths = img_paths.reset_index(drop=True)\n",
    "        self.meta_data = meta_data.reset_index(drop=True)\n",
    "        self.labels = pd.Series(labels).reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        meta = torch.tensor(self.meta_data.iloc[idx].values.astype(np.float32))\n",
    "        label = torch.tensor(self.labels.iloc[idx], dtype=torch.long)\n",
    "        return image, meta, label\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # Random horizontal flip\n",
    "    transforms.RandomRotation(70),          \n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Color jitter\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = PADUFESDataset(X_train_img_paths_final, X_train_meta_final, y_train_final, transform=train_transform)\n",
    "val_dataset = PADUFESDataset(X_val_img_paths, X_val_meta, y_val, transform=val_test_transform)\n",
    "test_dataset = PADUFESDataset(X_test_img_paths, X_test_meta, y_test, transform=val_test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"\\n✅ Loaded Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8631b0b-4925-47aa-8fa6-0c9d9d049248",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt'):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_accuracy_max = -np.Inf\n",
    "        \n",
    "    def __call__(self, val_acc, model):\n",
    "        score = val_acc\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_acc, model)\n",
    "        elif score <= self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_acc, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_acc, model):\n",
    "        if self.verbose:\n",
    "            print(f'Validation accuracy increased ({self.val_accuracy_max:.6f} --> {val_acc:.6f}). Saving model...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_accuracy_max = val_acc\n",
    "\n",
    "from PIL import Image\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "def test(model, loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, metas, labels in loader:\n",
    "            imgs, metas, labels = imgs.to(device), metas.to(device), labels.to(device)\n",
    "            outputs = model(imgs, metas)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    return all_labels, all_preds\n",
    "\n",
    "def train(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc='Training')\n",
    "    for images, meta, labels in pbar:\n",
    "        images, meta, labels = images.to(device), meta.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images, meta)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Optional: Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{running_loss/total:.4f}',\n",
    "            'acc': f'{100.*correct/total:.2f}%'\n",
    "        })\n",
    "    \n",
    "    return running_loss/len(train_loader), correct/total\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, meta, labels in val_loader:\n",
    "            images, meta, labels = images.to(device), meta.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images, meta)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    return running_loss/len(val_loader), correct/total\n",
    "\n",
    "def train_model_with_scheduler_and_checkpoint(\n",
    "    model, train_loader, val_loader, optimizer, criterion, device, \n",
    "    epochs=20, patience=5, scheduler_patience=5, checkpoint_dir='checkpoints'):\n",
    "    \n",
    "    # Create checkpoint directory if it doesn't exist\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, 'best_model.pt')\n",
    "    \n",
    "    early_stopping = EarlyStopping(\n",
    "        patience=patience, \n",
    "        verbose=True, \n",
    "        path=checkpoint_path\n",
    "    )\n",
    "    scheduler = ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='max',  # Changed to max since we're monitoring accuracy\n",
    "        patience=scheduler_patience, \n",
    "        verbose=True,\n",
    "        factor=0.1,\n",
    "        min_lr=1e-6\n",
    "    )\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [], 'val_loss': [],\n",
    "        'train_acc': [], 'val_acc': [],\n",
    "        'lr': []\n",
    "    }\n",
    "    \n",
    "    best_model_epoch = None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f'\\nEpoch {epoch+1}/{epochs}')\n",
    "        \n",
    "        # Training phase\n",
    "        train_loss, train_acc = train(model, train_loader, optimizer, criterion, device)\n",
    "        \n",
    "        # Validation phase\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        # Update history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['lr'].append(optimizer.param_groups[0]['lr'])\n",
    "        \n",
    "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
    "        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "        \n",
    "        # Update scheduler based on validation accuracy\n",
    "        scheduler.step(val_acc)\n",
    "        \n",
    "        # Early stopping check\n",
    "        early_stopping(val_acc, model)\n",
    "        if val_acc > early_stopping.val_accuracy_max:\n",
    "            best_model_epoch = epoch + 1\n",
    "            \n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load(checkpoint_path))\n",
    "    \n",
    "    # Plot training curves\n",
    "    plot_training_curves_with_checkpoint(history, best_model_epoch)\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "def plot_training_curves_with_checkpoint(history, best_model_epoch):\n",
    "    epochs_range = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Loss curves\n",
    "    ax1.plot(epochs_range, history['train_loss'], label='Training Loss')\n",
    "    ax1.plot(epochs_range, history['val_loss'], label='Validation Loss')\n",
    "    if best_model_epoch:\n",
    "        ax1.axvline(best_model_epoch, color='r', linestyle='--', label='Best Model')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Training and Validation Loss')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Accuracy curves\n",
    "    ax2.plot(epochs_range, history['train_acc'], label='Training Accuracy')\n",
    "    ax2.plot(epochs_range, history['val_acc'], label='Validation Accuracy')\n",
    "    if best_model_epoch:\n",
    "        ax2.axvline(best_model_epoch, color='r', linestyle='--', label='Best Model')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.set_title('Training and Validation Accuracy')\n",
    "    ax2.legend()\n",
    "    \n",
    "    # Learning rate curve\n",
    "    ax3.plot(epochs_range, history['lr'], label='Learning Rate')\n",
    "    if best_model_epoch:\n",
    "        ax3.axvline(best_model_epoch, color='r', linestyle='--', label='Best Model')\n",
    "    ax3.set_xlabel('Epoch')\n",
    "    ax3.set_ylabel('Learning Rate')\n",
    "    ax3.set_title('Learning Rate Schedule')\n",
    "    ax3.set_yscale('log')\n",
    "    ax3.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef89115c-3eff-4ca6-9f4b-7377108be1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm  \n",
    "import torch.nn.functional as F\n",
    "\n",
    "class EarlyFusionModel(nn.Module):\n",
    "    def __init__(self, input_dim_meta, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embed metadata to smaller spatial dimensions first\n",
    "        self.meta_embed = nn.Sequential(\n",
    "            nn.Linear(input_dim_meta, 56 * 56),  # Smaller initial dimension\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(56 * 56),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        # Modified Xception\n",
    "        self.xception = timm.create_model('xception', pretrained=True)\n",
    "        # Modify first conv layer to accept additional channel\n",
    "        first_conv = self.xception.conv1\n",
    "        self.xception.conv1 = nn.Conv2d(4, 32, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        \n",
    "        # Modify the final classification layer\n",
    "        in_features = self.xception.fc.in_features  # Usually 2048 for Xception\n",
    "        self.xception.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(in_features, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Initialize new channel weights\n",
    "        with torch.no_grad():\n",
    "            self.xception.conv1.weight.data[:, :3] = first_conv.weight.data\n",
    "            # Initialize the new channel with smaller weights to prevent dominating\n",
    "            self.xception.conv1.weight.data[:, 3:] = first_conv.weight.data.mean(dim=1, keepdim=True) * 0.1\n",
    "        \n",
    "    def forward(self, img, meta):\n",
    "        # Reshape metadata to image-like format\n",
    "        batch_size = img.shape[0]\n",
    "        meta_reshaped = self.meta_embed(meta).view(batch_size, 1, 56, 56)\n",
    "        \n",
    "        # Upsample to match image dimensions\n",
    "        meta_upsampled = F.interpolate(meta_reshaped, \n",
    "                                     size=(224, 224), \n",
    "                                     mode='bilinear', \n",
    "                                     align_corners=False)\n",
    "        \n",
    "        # Early fusion\n",
    "        combined_input = torch.cat([img, meta_upsampled], dim=1)\n",
    "        \n",
    "        # Process through modified Xception\n",
    "        out = self.xception(combined_input)\n",
    "        return out\n",
    "        \n",
    "input_dim_meta = X_train_meta.shape[1]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = EarlyFusionModel(input_dim_meta, num_classes).to(device)\n",
    "\n",
    "from torchinfo import summary\n",
    "summary(model=model, \n",
    "        input_size=[(16, 3, 224, 224), (16, input_dim_meta)],  \n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    ")\n",
    "xception_model = EarlyFusionModel(input_dim_meta, num_classes).to(device)\n",
    "xception_model.load_state_dict(torch.load('D:\\\\PAD-UFES\\\\best_early_fusion_xceptionsmoteDA.pth'))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "xception_model.to(device)\n",
    "\n",
    "true_labels, pred_labels = test(xception_model, test_loader, device)\n",
    "\n",
    "class_names = label_encoder.classes_\n",
    "report = classification_report(true_labels, pred_labels, digits=4,target_names=class_names)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "cm = confusion_matrix(true_labels, pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a8b844-ef1a-44cd-8ad1-9b6d630b935a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_dim_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7279bf2a-d008-4abe-b887-366efe15d700",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt'):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_accuracy_max = -np.Inf\n",
    "        \n",
    "    def __call__(self, val_acc, model):\n",
    "        score = val_acc\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_acc, model)\n",
    "        elif score <= self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_acc, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_acc, model):\n",
    "        if self.verbose:\n",
    "            print(f'Validation accuracy increased ({self.val_accuracy_max:.6f} --> {val_acc:.6f}). Saving model...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_accuracy_max = val_acc\n",
    "\n",
    "from PIL import Image\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "def test(model, loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, metas, labels in loader:\n",
    "            imgs, metas, labels = imgs.to(device), metas.to(device), labels.to(device)\n",
    "            outputs = model(imgs, metas)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    return all_labels, all_preds\n",
    "\n",
    "def train(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc='Training')\n",
    "    for images, meta, labels in pbar:\n",
    "        images, meta, labels = images.to(device), meta.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images, meta)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Optional: Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{running_loss/total:.4f}',\n",
    "            'acc': f'{100.*correct/total:.2f}%'\n",
    "        })\n",
    "    \n",
    "    return running_loss/len(train_loader), correct/total\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, meta, labels in val_loader:\n",
    "            images, meta, labels = images.to(device), meta.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images, meta)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    return running_loss/len(val_loader), correct/total\n",
    "\n",
    "def train_model_with_scheduler_and_checkpoint(\n",
    "    model, train_loader, val_loader, optimizer, criterion, device, \n",
    "    epochs=20, patience=5, scheduler_patience=5, checkpoint_dir='checkpoints'):\n",
    "    \n",
    "    # Create checkpoint directory if it doesn't exist\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, 'best_model.pt')\n",
    "    \n",
    "    early_stopping = EarlyStopping(\n",
    "        patience=patience, \n",
    "        verbose=True, \n",
    "        path=checkpoint_path\n",
    "    )\n",
    "    scheduler = ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='max',  # Changed to max since we're monitoring accuracy\n",
    "        patience=scheduler_patience, \n",
    "        verbose=True,\n",
    "        factor=0.1,\n",
    "        min_lr=1e-6\n",
    "    )\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [], 'val_loss': [],\n",
    "        'train_acc': [], 'val_acc': [],\n",
    "        'lr': []\n",
    "    }\n",
    "    \n",
    "    best_model_epoch = None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f'\\nEpoch {epoch+1}/{epochs}')\n",
    "        \n",
    "        # Training phase\n",
    "        train_loss, train_acc = train(model, train_loader, optimizer, criterion, device)\n",
    "        \n",
    "        # Validation phase\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        # Update history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['lr'].append(optimizer.param_groups[0]['lr'])\n",
    "        \n",
    "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
    "        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "        \n",
    "        # Update scheduler based on validation accuracy\n",
    "        scheduler.step(val_acc)\n",
    "        \n",
    "        # Early stopping check\n",
    "        early_stopping(val_acc, model)\n",
    "        if val_acc > early_stopping.val_accuracy_max:\n",
    "            best_model_epoch = epoch + 1\n",
    "            \n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load(checkpoint_path))\n",
    "    \n",
    "    # Plot training curves\n",
    "    plot_training_curves_with_checkpoint(history, best_model_epoch)\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "def plot_training_curves_with_checkpoint(history, best_model_epoch):\n",
    "    epochs_range = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Loss curves\n",
    "    ax1.plot(epochs_range, history['train_loss'], label='Training Loss')\n",
    "    ax1.plot(epochs_range, history['val_loss'], label='Validation Loss')\n",
    "    if best_model_epoch:\n",
    "        ax1.axvline(best_model_epoch, color='r', linestyle='--', label='Best Model')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Training and Validation Loss')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Accuracy curves\n",
    "    ax2.plot(epochs_range, history['train_acc'], label='Training Accuracy')\n",
    "    ax2.plot(epochs_range, history['val_acc'], label='Validation Accuracy')\n",
    "    if best_model_epoch:\n",
    "        ax2.axvline(best_model_epoch, color='r', linestyle='--', label='Best Model')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.set_title('Training and Validation Accuracy')\n",
    "    ax2.legend()\n",
    "    \n",
    "    # Learning rate curve\n",
    "    ax3.plot(epochs_range, history['lr'], label='Learning Rate')\n",
    "    if best_model_epoch:\n",
    "        ax3.axvline(best_model_epoch, color='r', linestyle='--', label='Best Model')\n",
    "    ax3.set_xlabel('Epoch')\n",
    "    ax3.set_ylabel('Learning Rate')\n",
    "    ax3.set_title('Learning Rate Schedule')\n",
    "    ax3.set_yscale('log')\n",
    "    ax3.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd9eb80-ceed-4ace-b395-d2e315118b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class EarlyFusionModel(nn.Module):\n",
    "    def __init__(self, input_dim_meta, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embed metadata to smaller spatial dimensions first\n",
    "        self.meta_embed = nn.Sequential(\n",
    "            nn.Linear(input_dim_meta, 56 * 56),  # Smaller initial dimension\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(56 * 56),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        # Load PVT v2 model\n",
    "        self.pvt = timm.create_model(\"pvt_v2_b1\", pretrained=True, num_classes=num_classes)\n",
    "        \n",
    "        # Modify the first convolution layer to accept additional channel (4 instead of 3)\n",
    "        first_conv = self.pvt.patch_embed.proj\n",
    "        self.pvt.patch_embed.proj = nn.Conv2d(4, first_conv.out_channels, \n",
    "                                              kernel_size=first_conv.kernel_size,\n",
    "                                              stride=first_conv.stride,\n",
    "                                              padding=first_conv.padding,\n",
    "                                              bias=first_conv.bias is not None)\n",
    "        \n",
    "        # Initialize new channel weights\n",
    "        with torch.no_grad():\n",
    "            self.pvt.patch_embed.proj.weight.data[:, :3] = first_conv.weight.data\n",
    "            self.pvt.patch_embed.proj.weight.data[:, 3:] = first_conv.weight.data.mean(dim=1, keepdim=True) * 0.1\n",
    "\n",
    "    def forward(self, img, meta):\n",
    "        # Reshape metadata to image-like format\n",
    "        batch_size = img.shape[0]\n",
    "        meta_reshaped = self.meta_embed(meta).view(batch_size, 1, 56, 56)\n",
    "        \n",
    "        # Upsample to match image dimensions\n",
    "        meta_upsampled = F.interpolate(meta_reshaped, \n",
    "                                       size=(224, 224), \n",
    "                                       mode='bilinear', \n",
    "                                       align_corners=False)\n",
    "        \n",
    "        # Early fusion\n",
    "        combined_input = torch.cat([img, meta_upsampled], dim=1)\n",
    "        \n",
    "        # Process through modified PVT\n",
    "        out = self.pvt(combined_input)\n",
    "        return out\n",
    "\n",
    "input_dim_meta = X_train_meta.shape[1]\n",
    "num_classes = 6\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = EarlyFusionModel(input_dim_meta, num_classes).to(device)\n",
    "\n",
    "from torchinfo import summary\n",
    "summary(model=model, \n",
    "        input_size=[(16, 3, 224, 224), (16, input_dim_meta)],  \n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    ")\n",
    "\n",
    "pv2_model = EarlyFusionModel(input_dim_meta=input_dim_meta, num_classes=num_classes).to(device)\n",
    "pv2_model.load_state_dict(torch.load('D:\\\\PAD-UFES\\\\best_early_fusion_pvtv2smoteDA.pth'))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pv2_model.to(device)\n",
    "\n",
    "true_labels, pred_labels = test(pv2_model, test_loader, device)\n",
    "\n",
    "class_names = label_encoder.classes_\n",
    "report = classification_report(true_labels, pred_labels, digits=4,target_names=class_names)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "cm = confusion_matrix(true_labels, pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfa4a19-7a92-4217-971a-d750b9868c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm  \n",
    "import torch.nn.functional as F\n",
    "\n",
    "class EarlyFusionModel(nn.Module):\n",
    "    def __init__(self, input_dim_meta, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embed metadata to smaller spatial dimensions first\n",
    "        self.meta_embed = nn.Sequential(\n",
    "            nn.Linear(input_dim_meta, 64 * 64),  # Updated for mobilevit's smaller receptive field\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64 * 64),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        # Load MobileViT model\n",
    "        self.mobilevit = timm.create_model(\"mobilevit_s.cvnets_in1k\", pretrained=True, num_classes=num_classes)\n",
    "        \n",
    "        # Inspect the model to identify the first conv layer\n",
    "        # Modify the first conv layer to accept additional channel\n",
    "        first_conv = self.mobilevit.stem.conv  # `stem.conv` is the correct initial layer\n",
    "        self.mobilevit.stem.conv = nn.Conv2d(4, first_conv.out_channels, \n",
    "                                             kernel_size=first_conv.kernel_size, \n",
    "                                             stride=first_conv.stride, \n",
    "                                             padding=first_conv.padding, \n",
    "                                             bias=first_conv.bias)\n",
    "        \n",
    "        # Initialize new channel weights\n",
    "        with torch.no_grad():\n",
    "            self.mobilevit.stem.conv.weight.data[:, :3] = first_conv.weight.data\n",
    "            # Initialize the new channel with smaller weights to prevent dominating\n",
    "            self.mobilevit.stem.conv.weight.data[:, 3:] = first_conv.weight.data.mean(dim=1, keepdim=True) * 0.1\n",
    "\n",
    "    def forward(self, img, meta):\n",
    "        # Reshape metadata to image-like format\n",
    "        batch_size = img.shape[0]\n",
    "        meta_reshaped = self.meta_embed(meta).view(batch_size, 1, 64, 64)\n",
    "        \n",
    "        # Upsample to match image dimensions\n",
    "        meta_upsampled = F.interpolate(meta_reshaped, \n",
    "                                       size=(224, 224),  # MobileViT expects 256x256\n",
    "                                       mode='bilinear', \n",
    "                                       align_corners=False)\n",
    "        \n",
    "        # Early fusion\n",
    "        combined_input = torch.cat([img, meta_upsampled], dim=1)\n",
    "        \n",
    "        # Process through modified MobileViT\n",
    "        out = self.mobilevit(combined_input)\n",
    "        return out\n",
    "\n",
    "# Assuming X_train_meta and other variables are defined\n",
    "input_dim_meta = X_train_meta.shape[1]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = EarlyFusionModel(input_dim_meta, num_classes).to(device)\n",
    "\n",
    "from torchinfo import summary\n",
    "summary(model=model, \n",
    "        input_size=[(16, 3, 224, 224), (16, input_dim_meta)],  # Updated for MobileViT input size\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    ")\n",
    "\n",
    "mobilevit_model = EarlyFusionModel(input_dim_meta=input_dim_meta, num_classes=num_classes).to(device)\n",
    "mobilevit_model.load_state_dict(torch.load('D:\\\\PAD-UFES\\\\best_early_fusion_mobilevitDA.pth'))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "mobilevit_model.to(device)\n",
    "\n",
    "true_labels, pred_labels = test(mobilevit_model, test_loader, device)\n",
    "\n",
    "class_names = label_encoder.classes_\n",
    "report = classification_report(true_labels, pred_labels, digits=4,target_names=class_names)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "cm = confusion_matrix(true_labels, pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ceabbe7-e45e-4e16-b25c-8bc20e27bbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForImageClassification, AutoConfig\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForImageClassification, AutoImageProcessor\n",
    "\n",
    "class EarlyFusionModel(nn.Module):\n",
    "    def __init__(self, input_dim_meta, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embed metadata to smaller spatial dimensions\n",
    "        self.meta_embed = nn.Sequential(\n",
    "            nn.Linear(input_dim_meta, 56 * 56),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(56 * 56),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        # Load Swin Transformer from Hugging Face\n",
    "        self.swin_transformer = AutoModelForImageClassification.from_pretrained(\n",
    "            \"microsoft/swin-tiny-patch4-window7-224\",\n",
    "            ignore_mismatched_sizes=True\n",
    "        )\n",
    "        \n",
    "        # Replace the classifier to match the number of classes\n",
    "        in_features = self.swin_transformer.classifier.in_features\n",
    "        self.swin_transformer.classifier = nn.Linear(in_features, num_classes)\n",
    "        \n",
    "        # Access and modify the patch embedding layer\n",
    "        patch_embed_layer = self.swin_transformer.swin.embeddings.patch_embeddings.projection\n",
    "        self.swin_transformer.swin.embeddings.patch_embeddings.projection = nn.Conv2d(\n",
    "            in_channels=4,  # Update to accept 4 input channels\n",
    "            out_channels=patch_embed_layer.out_channels,\n",
    "            kernel_size=patch_embed_layer.kernel_size,\n",
    "            stride=patch_embed_layer.stride,\n",
    "            padding=patch_embed_layer.padding,\n",
    "            bias=patch_embed_layer.bias is not None,  # Ensure bias is a boolean\n",
    "        )\n",
    "        \n",
    "        # Initialize weights for the new input channel\n",
    "        with torch.no_grad():\n",
    "            self.swin_transformer.swin.embeddings.patch_embeddings.projection.weight[:, :3] = patch_embed_layer.weight\n",
    "            self.swin_transformer.swin.embeddings.patch_embeddings.projection.weight[:, 3:] = patch_embed_layer.weight.mean(dim=1, keepdim=True)\n",
    "\n",
    "    def forward(self, img, meta):\n",
    "        # Embed metadata\n",
    "        batch_size = img.size(0)\n",
    "        meta_reshaped = self.meta_embed(meta).view(batch_size, 1, 56, 56)\n",
    "        \n",
    "        # Upsample metadata to match image dimensions\n",
    "        meta_upsampled = F.interpolate(meta_reshaped, size=(224, 224), mode=\"bilinear\", align_corners=False)\n",
    "        \n",
    "        # Concatenate the image and metadata\n",
    "        combined_input = torch.cat([img, meta_upsampled], dim=1)\n",
    "        \n",
    "        # Pass through Swin Transformer\n",
    "        outputs = self.swin_transformer(pixel_values=combined_input)\n",
    "        return outputs.logits\n",
    "\n",
    "\n",
    "input_dim_meta = X_train_meta.shape[1]  # Number of metadata features\n",
    "num_classes = 6  # Number of output classes\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = EarlyFusionModel(input_dim_meta, num_classes).to(device)\n",
    "\n",
    "# Model Summary\n",
    "from torchinfo import summary\n",
    "summary(\n",
    "    model=model,\n",
    "    input_size=[(16, 3, 224, 224), (16, input_dim_meta)],\n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "    col_width=20,\n",
    "    row_settings=[\"var_names\"]\n",
    ")\n",
    "\n",
    "swin_model = EarlyFusionModel(input_dim_meta=input_dim_meta, num_classes=num_classes).to(device)\n",
    "swin_model.load_state_dict(torch.load('D:\\\\PAD-UFES\\\\best_early_fusion_swintinysmoteDA.pth'))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "swin_model.to(device)\n",
    "\n",
    "true_labels, pred_labels = test(swin_model, test_loader, device)\n",
    "\n",
    "class_names = label_encoder.classes_\n",
    "report = classification_report(true_labels, pred_labels, digits=4,target_names=class_names)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "cm = confusion_matrix(true_labels, pred_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228b6d07-a7b1-493b-a95e-08d3afca5c91",
   "metadata": {},
   "source": [
    "<h1>Simple Averaging</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfeee46d-8551-4663-89f4-dad23ae284e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Function to set random seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # If using CUDA\n",
    "\n",
    "def test_ensemble(models, loader, device):\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, metas, labels in loader:\n",
    "            imgs, metas, labels = imgs.to(device), metas.to(device), labels.to(device)\n",
    "            \n",
    "            # Initialize a list to store individual model predictions\n",
    "            model_outputs = []\n",
    "            \n",
    "            # Get predictions from each model and store\n",
    "            for model in models:\n",
    "                outputs = model(imgs, metas)\n",
    "                model_outputs.append(outputs)\n",
    "            \n",
    "            # Stack model outputs along a new axis (axis 0: models)\n",
    "            model_outputs = torch.stack(model_outputs, dim=0)\n",
    "            \n",
    "            # Average the outputs along the new axis (across models)\n",
    "            avg_outputs = model_outputs.mean(dim=0)  # Averaging logits/probabilities\n",
    "\n",
    "            # Convert the averaged outputs to predicted labels\n",
    "            _, predicted = torch.max(avg_outputs, 1)\n",
    "\n",
    "            # Store the predictions and the true labels\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    return all_labels, all_preds\n",
    "\n",
    "\n",
    "# Function to calculate metrics and return mean ± std deviation\n",
    "def calculate_metrics(true_labels, predictions):\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions, average='macro')\n",
    "    recall = recall_score(true_labels, predictions, average='macro')\n",
    "    f1 = f1_score(true_labels, predictions, average='macro')\n",
    "\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "\n",
    "# Main loop to test the ensemble and calculate metrics with standard deviation\n",
    "def evaluate_ensemble(models, test_loader, device, seeds):\n",
    "    accuracies, precisions, recalls, f1_scores = [], [], [], []\n",
    "\n",
    "    # Loop through predefined seeds\n",
    "    for seed in seeds:\n",
    "        set_seed(seed)  # Set the random seed for reproducibility\n",
    "        \n",
    "        # Run the ensemble evaluation\n",
    "        true_labels, ensemble_preds = test_ensemble(models, test_loader, device)\n",
    "\n",
    "        # Calculate metrics for this run\n",
    "        accuracy, precision, recall, f1 = calculate_metrics(true_labels, ensemble_preds)\n",
    "\n",
    "        # Store metrics\n",
    "        accuracies.append(accuracy)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    # Calculate mean and standard deviation for each metric\n",
    "    mean_accuracy = np.mean(accuracies)\n",
    "    std_accuracy = np.std(accuracies)\n",
    "\n",
    "    mean_precision = np.mean(precisions)\n",
    "    std_precision = np.std(precisions)\n",
    "\n",
    "    mean_recall = np.mean(recalls)\n",
    "    std_recall = np.std(recalls)\n",
    "\n",
    "    mean_f1 = np.mean(f1_scores)\n",
    "    std_f1 = np.std(f1_scores)\n",
    "\n",
    "    # Print out results\n",
    "    print(f\"Accuracy: {mean_accuracy:.4f} ± {std_accuracy:.4f}\")\n",
    "    print(f\"Precision: {mean_precision:.4f} ± {std_precision:.4f}\")\n",
    "    print(f\"Recall: {mean_recall:.4f} ± {std_recall:.4f}\")\n",
    "    print(f\"F1 Score: {mean_f1:.4f} ± {std_f1:.4f}\")\n",
    "\n",
    "\n",
    "# Example usage with 4 models:\n",
    "models = [mobilevit_model, pv2_model, swin_model]  # List of models\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Fi``xed random seeds to use\n",
    "seeds = [42, 123, 569]\n",
    "\n",
    "# Assume test_loader is defined (DataLoader for test set)\n",
    "evaluate_ensemble(models, test_loader, device, seeds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cf72c8-6520-40e5-b7a6-620f940f504a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "class TeacherModel(nn.Module):\n",
    "    def __init__(self, models, ensemble_method=\"mean\"):\n",
    "        \"\"\"\n",
    "        Teacher Model using Ensemble Learning.\n",
    "\n",
    "        Args:\n",
    "            models (list): List of trained models to use for ensembling.\n",
    "            ensemble_method (str): \"mean\" for averaging logits, \"vote\" for majority voting.\n",
    "        \"\"\"\n",
    "        super(TeacherModel, self).__init__()\n",
    "        self.models = models\n",
    "        self.ensemble_method = ensemble_method\n",
    "\n",
    "        # Ensure all models are in eval mode and no gradients are computed\n",
    "        for model in self.models:\n",
    "            model.eval()\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, img, meta):\n",
    "        \"\"\"\n",
    "        Forward pass through the ensemble teacher model.\n",
    "\n",
    "        Args:\n",
    "            img (torch.Tensor): Batch of images.\n",
    "            meta (torch.Tensor): Batch of metadata.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The ensembled output (soft probabilities).\n",
    "        \"\"\"\n",
    "        model_outputs = []\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient computation for teacher\n",
    "            for model in self.models:\n",
    "                outputs = model(img, meta)\n",
    "                model_outputs.append(outputs)\n",
    "\n",
    "        # Convert list to tensor shape [num_models, batch_size, num_classes]\n",
    "        model_outputs = torch.stack(model_outputs, dim=0)\n",
    "\n",
    "        if self.ensemble_method == \"mean\":\n",
    "            # Soft-label generation: Averaging logits\n",
    "            avg_outputs = model_outputs.mean(dim=0)  # Shape [batch_size, num_classes]\n",
    "        elif self.ensemble_method == \"vote\":\n",
    "            # Majority voting: Get the most common prediction\n",
    "            _, predictions = torch.max(model_outputs, dim=2)  # Shape [num_models, batch_size]\n",
    "            avg_outputs = predictions.mode(dim=0).values  # Majority vote\n",
    "\n",
    "        return avg_outputs  # These are the soft labels for KD\n",
    "\n",
    "teacher_model = TeacherModel(models=[mobilevit_model, pv2_model, swin_model, xception_model], ensemble_method=\"mean\")\n",
    "\n",
    "# Move to the correct device (CPU/GPU)\n",
    "teacher_model = teacher_model.to(device)\n",
    "\n",
    "true_labels, pred_labels = test(teacher_model, test_loader, device)\n",
    "\n",
    "class_names = label_encoder.classes_\n",
    "report = classification_report(true_labels, pred_labels, digits=4,target_names=class_names)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "cm = confusion_matrix(true_labels, pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbdcbf9-e87a-4766-b5e7-2e84545475de",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model = TeacherModel(models=[pv2_model, swin_model], ensemble_method=\"mean\")\n",
    "\n",
    "# Move to the correct device (CPU/GPU)\n",
    "teacher_model = teacher_model.to(device)\n",
    "\n",
    "true_labels, pred_labels = test(teacher_model, test_loader, device)\n",
    "\n",
    "class_names = label_encoder.classes_\n",
    "report = classification_report(true_labels, pred_labels, digits=4,target_names=class_names)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "cm = confusion_matrix(true_labels, pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a76476-b4e0-40e5-8df6-9b9f0fa59da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model = TeacherModel(models=[mobilevit_model, pv2_model], ensemble_method=\"mean\")\n",
    "\n",
    "# Move to the correct device (CPU/GPU)\n",
    "teacher_model = teacher_model.to(device)\n",
    "\n",
    "true_labels, pred_labels = test(teacher_model, test_loader, device)\n",
    "\n",
    "class_names = label_encoder.classes_\n",
    "report = classification_report(true_labels, pred_labels, digits=4,target_names=class_names)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "cm = confusion_matrix(true_labels, pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45354c34-8106-4d30-ac26-c6eaffe85931",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model = TeacherModel(models=[mobilevit_model, swin_model], ensemble_method=\"mean\")\n",
    "\n",
    "# Move to the correct device (CPU/GPU)\n",
    "teacher_model = teacher_model.to(device)\n",
    "\n",
    "true_labels, pred_labels = test(teacher_model, test_loader, device)\n",
    "\n",
    "class_names = label_encoder.classes_\n",
    "report = classification_report(true_labels, pred_labels, digits=4,target_names=class_names)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "cm = confusion_matrix(true_labels, pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfba06d-d8f9-4202-a087-c6b784f89a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model = TeacherModel(models=[mobilevit_model, pv2_model, swin_model], ensemble_method=\"mean\")\n",
    "\n",
    "# Move to the correct device (CPU/GPU)\n",
    "teacher_model = teacher_model.to(device)\n",
    "\n",
    "true_labels, pred_labels = test(teacher_model, test_loader, device)\n",
    "\n",
    "class_names = label_encoder.classes_\n",
    "report = classification_report(true_labels, pred_labels, digits=4,target_names=class_names)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "cm = confusion_matrix(true_labels, pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d7278e-2438-4c25-b95f-f6cbf76ba8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model = TeacherModel(models=[mobilevit_model, swin_model, xception_model], ensemble_method=\"mean\")\n",
    "\n",
    "# Move to the correct device (CPU/GPU)\n",
    "teacher_model = teacher_model.to(device)\n",
    "\n",
    "true_labels, pred_labels = test(teacher_model, test_loader, device)\n",
    "\n",
    "class_names = label_encoder.classes_\n",
    "report = classification_report(true_labels, pred_labels, digits=4,target_names=class_names)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "cm = confusion_matrix(true_labels, pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da06d4a0-14e3-44b8-ba65-f42767df88b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model = TeacherModel(models=[mobilevit_model, pv2_model, xception_model], ensemble_method=\"mean\")\n",
    "\n",
    "# Move to the correct device (CPU/GPU)\n",
    "teacher_model = teacher_model.to(device)\n",
    "\n",
    "true_labels, pred_labels = test(teacher_model, test_loader, device)\n",
    "\n",
    "class_names = label_encoder.classes_\n",
    "report = classification_report(true_labels, pred_labels, digits=4,target_names=class_names)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "cm = confusion_matrix(true_labels, pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36d9995-1ad6-4f7c-a0b2-fb08fc0be409",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model = TeacherModel(models=[pv2_model, swin_model, xception_model], ensemble_method=\"mean\")\n",
    "\n",
    "# Move to the correct device (CPU/GPU)\n",
    "teacher_model = teacher_model.to(device)\n",
    "\n",
    "true_labels, pred_labels = test(teacher_model, test_loader, device)\n",
    "\n",
    "class_names = label_encoder.classes_\n",
    "report = classification_report(true_labels, pred_labels, digits=4,target_names=class_names)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "cm = confusion_matrix(true_labels, pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e5258b-79be-4248-81ec-035f61507ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model = TeacherModel(models=[pv2_model, xception_model], ensemble_method=\"mean\")\n",
    "\n",
    "# Move to the correct device (CPU/GPU)\n",
    "teacher_model = teacher_model.to(device)\n",
    "\n",
    "true_labels, pred_labels = test(teacher_model, test_loader, device)\n",
    "\n",
    "class_names = label_encoder.classes_\n",
    "report = classification_report(true_labels, pred_labels, digits=4,target_names=class_names)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "cm = confusion_matrix(true_labels, pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309fa555-839b-4640-befe-0132a4df244d",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model = TeacherModel(models=[swin_model, xception_model], ensemble_method=\"mean\")\n",
    "\n",
    "# Move to the correct device (CPU/GPU)\n",
    "teacher_model = teacher_model.to(device)\n",
    "\n",
    "true_labels, pred_labels = test(teacher_model, test_loader, device)\n",
    "\n",
    "class_names = label_encoder.classes_\n",
    "report = classification_report(true_labels, pred_labels, digits=4,target_names=class_names)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "cm = confusion_matrix(true_labels, pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a120619-06e7-4a7b-9bc7-0d2f2fa8bf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model = TeacherModel(models=[mobilevit_model, xception_model], ensemble_method=\"mean\")\n",
    "\n",
    "# Move to the correct device (CPU/GPU)\n",
    "teacher_model = teacher_model.to(device)\n",
    "\n",
    "true_labels, pred_labels = test(teacher_model, test_loader, device)\n",
    "\n",
    "class_names = label_encoder.classes_\n",
    "report = classification_report(true_labels, pred_labels, digits=4,target_names=class_names)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "cm = confusion_matrix(true_labels, pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087f771c-373a-4888-a978-0742d3780b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model = TeacherModel(models=[mobilevit_model, swin_model], ensemble_method=\"mean\")\n",
    "\n",
    "# Move to the correct device (CPU/GPU)\n",
    "teacher_model = teacher_model.to(device)\n",
    "\n",
    "true_labels, pred_labels = test(teacher_model, test_loader, device)\n",
    "\n",
    "class_names = label_encoder.classes_\n",
    "report = classification_report(true_labels, pred_labels, digits=4,target_names=class_names)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "cm = confusion_matrix(true_labels, pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9999e1a9-d62d-49a3-abc6-390b64ded4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model = TeacherModel(models=[mobilevit_model, pv2_model], ensemble_method=\"mean\")\n",
    "\n",
    "# Move to the correct device (CPU/GPU)\n",
    "teacher_model = teacher_model.to(device)\n",
    "\n",
    "true_labels, pred_labels = test(teacher_model, test_loader, device)\n",
    "\n",
    "class_names = label_encoder.classes_\n",
    "report = classification_report(true_labels, pred_labels, digits=4,target_names=class_names)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "cm = confusion_matrix(true_labels, pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc5bdbe-c270-4bce-94da-7a68dcfb7a30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93be2fbe-4d9d-43d8-9b9e-a5d467a8e7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load the best model\n",
    "best_model = EarlyFusionWithDynamicGCN(input_dim_meta=59, num_classes=6).to(device)\n",
    "best_model.load_state_dict(torch.load(\"best_student_model_test_loss1.pth\"))\n",
    "best_model.eval()\n",
    "\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "all_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, metas, labels in test_loader:\n",
    "        images, metas, labels = images.to(device), metas.to(device), labels.to(device)\n",
    "        batch_indices = torch.arange(metas.size(0)).to(device).long()\n",
    "        outputs = student_model(images, metas, batch_indices)        \n",
    "        probs = F.softmax(outputs, dim=1)\n",
    "        preds = probs.argmax(dim=1)\n",
    "\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "all_labels = np.array(all_labels)\n",
    "all_preds = np.array(all_preds)\n",
    "all_probs = np.array(all_probs)\n",
    "\n",
    "# Compute classification report\n",
    "class_report = classification_report(all_labels, all_preds, target_names=class_names, digits=4)\n",
    "\n",
    "# Compute normalized confusion matrix\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds, normalize=\"true\")\n",
    "\n",
    "# Display classification report\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(class_report)\n",
    "\n",
    "# Display confusion matrix (black and white)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, cmap=\"gray\", fmt=\".2f\", xticklabels=class_names, yticklabels=class_names, cbar=True)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Normalized Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Compute and plot ROC-AUC curve for each class\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for i, class_name in enumerate(class_names):\n",
    "    fpr, tpr, _ = roc_curve((all_labels == i).astype(int), all_probs[:, i])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label=f\"{class_name} (AUC = {roc_auc:.2f})\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], \"k--\")  # Diagonal line for reference\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC-AUC Curve\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Optional: Compute and plot Precision-Recall Curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for i, class_name in enumerate(class_names):\n",
    "    precision, recall, _ = precision_recall_curve((all_labels == i).astype(int), all_probs[:, i])\n",
    "    plt.plot(recall, precision, label=f\"{class_name}\")\n",
    "\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc07928",
   "metadata": {},
   "source": [
    "#### <h1>Majority Voting</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e5c69c-81ce-414a-9146-27af24d01fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Function to set random seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # If using CUDA\n",
    "\n",
    "def test_ensemble(models, loader, device):\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, metas, labels in loader:\n",
    "            imgs, metas, labels = imgs.to(device), metas.to(device), labels.to(device)\n",
    "            \n",
    "            # Initialize a list to store individual model predictions\n",
    "            model_outputs = []\n",
    "            \n",
    "            # Get predictions from each model and store\n",
    "            for model in models:\n",
    "                outputs = model(imgs, metas)\n",
    "                model_outputs.append(outputs)\n",
    "            \n",
    "            # Stack model outputs along a new axis (axis 0: models)\n",
    "            model_outputs = torch.stack(model_outputs, dim=0)\n",
    "            \n",
    "            # Average the outputs along the new axis (across models)\n",
    "            avg_outputs = model_outputs.mean(dim=0)  # Averaging logits/probabilities\n",
    "\n",
    "            # Convert the averaged outputs to predicted labels\n",
    "            _, predicted = torch.max(avg_outputs, 1)\n",
    "\n",
    "            # Store the predictions and the true labels\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    return all_labels, all_preds\n",
    "\n",
    "\n",
    "# Function to calculate metrics and return mean ± std deviation\n",
    "def calculate_metrics(true_labels, predictions):\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions, average='macro')\n",
    "    recall = recall_score(true_labels, predictions, average='macro')\n",
    "    f1 = f1_score(true_labels, predictions, average='macro')\n",
    "\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "\n",
    "# Main loop to test the ensemble and calculate metrics with standard deviation\n",
    "def evaluate_ensemble(models, test_loader, device, seeds):\n",
    "    accuracies, precisions, recalls, f1_scores = [], [], [], []\n",
    "\n",
    "    # Loop through predefined seeds\n",
    "    for seed in seeds:\n",
    "        set_seed(seed)  # Set the random seed for reproducibility\n",
    "        \n",
    "        # Run the ensemble evaluation\n",
    "        true_labels, ensemble_preds = test_ensemble(models, test_loader, device)\n",
    "\n",
    "        # Calculate metrics for this run\n",
    "        accuracy, precision, recall, f1 = calculate_metrics(true_labels, ensemble_preds)\n",
    "\n",
    "        # Store metrics\n",
    "        accuracies.append(accuracy)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    # Calculate mean and standard deviation for each metric\n",
    "    mean_accuracy = np.mean(accuracies)\n",
    "    std_accuracy = np.std(accuracies)\n",
    "\n",
    "    mean_precision = np.mean(precisions)\n",
    "    std_precision = np.std(precisions)\n",
    "\n",
    "    mean_recall = np.mean(recalls)\n",
    "    std_recall = np.std(recalls)\n",
    "\n",
    "    mean_f1 = np.mean(f1_scores)\n",
    "    std_f1 = np.std(f1_scores)\n",
    "\n",
    "    # Print out results\n",
    "    print(f\"Accuracy: {mean_accuracy:.4f} ± {std_accuracy:.4f}\")\n",
    "    print(f\"Precision: {mean_precision:.4f} ± {std_precision:.4f}\")\n",
    "    print(f\"Recall: {mean_recall:.4f} ± {std_recall:.4f}\")\n",
    "    print(f\"F1 Score: {mean_f1:.4f} ± {std_f1:.4f}\")\n",
    "\n",
    "\n",
    "# Example usage with 4 models:\n",
    "models = [mobilevit_model, pv2_model]  # List of models\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Fi``xed random seeds to use\n",
    "seeds = [42, 123, 569]\n",
    "\n",
    "# Assume test_loader is defined (DataLoader for test set)\n",
    "evaluate_ensemble(models, test_loader, device, seeds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d02237-00e6-4159-b8bd-d4956f8a596f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Function to set random seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # If using CUDA\n",
    "\n",
    "def test_ensemble(models, loader, device):\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, metas, labels in loader:\n",
    "            imgs, metas, labels = imgs.to(device), metas.to(device), labels.to(device)\n",
    "            \n",
    "            # Initialize a list to store individual model predictions\n",
    "            model_outputs = []\n",
    "            \n",
    "            # Get predictions from each model and store\n",
    "            for model in models:\n",
    "                outputs = model(imgs, metas)\n",
    "                model_outputs.append(outputs)\n",
    "            \n",
    "            # Stack model outputs along a new axis (axis 0: models)\n",
    "            model_outputs = torch.stack(model_outputs, dim=0)\n",
    "            \n",
    "            # Average the outputs along the new axis (across models)\n",
    "            avg_outputs = model_outputs.mean(dim=0)  # Averaging logits/probabilities\n",
    "\n",
    "            # Convert the averaged outputs to predicted labels\n",
    "            _, predicted = torch.max(avg_outputs, 1)\n",
    "\n",
    "            # Store the predictions and the true labels\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    return all_labels, all_preds\n",
    "\n",
    "\n",
    "# Function to calculate metrics and return mean ± std deviation\n",
    "def calculate_metrics(true_labels, predictions):\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions, average='macro')\n",
    "    recall = recall_score(true_labels, predictions, average='macro')\n",
    "    f1 = f1_score(true_labels, predictions, average='macro')\n",
    "\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "\n",
    "# Main loop to test the ensemble and calculate metrics with standard deviation\n",
    "def evaluate_ensemble(models, test_loader, device, seeds):\n",
    "    accuracies, precisions, recalls, f1_scores = [], [], [], []\n",
    "\n",
    "    # Loop through predefined seeds\n",
    "    for seed in seeds:\n",
    "        set_seed(seed)  # Set the random seed for reproducibility\n",
    "        \n",
    "        # Run the ensemble evaluation\n",
    "        true_labels, ensemble_preds = test_ensemble(models, test_loader, device)\n",
    "\n",
    "        # Calculate metrics for this run\n",
    "        accuracy, precision, recall, f1 = calculate_metrics(true_labels, ensemble_preds)\n",
    "\n",
    "        # Store metrics\n",
    "        accuracies.append(accuracy)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    # Calculate mean and standard deviation for each metric\n",
    "    mean_accuracy = np.mean(accuracies)\n",
    "    std_accuracy = np.std(accuracies)\n",
    "\n",
    "    mean_precision = np.mean(precisions)\n",
    "    std_precision = np.std(precisions)\n",
    "\n",
    "    mean_recall = np.mean(recalls)\n",
    "    std_recall = np.std(recalls)\n",
    "\n",
    "    mean_f1 = np.mean(f1_scores)\n",
    "    std_f1 = np.std(f1_scores)\n",
    "\n",
    "    # Print out results\n",
    "    print(f\"Accuracy: {mean_accuracy:.4f} ± {std_accuracy:.4f}\")\n",
    "    print(f\"Precision: {mean_precision:.4f} ± {std_precision:.4f}\")\n",
    "    print(f\"Recall: {mean_recall:.4f} ± {std_recall:.4f}\")\n",
    "    print(f\"F1 Score: {mean_f1:.4f} ± {std_f1:.4f}\")\n",
    "\n",
    "\n",
    "# Example usage with 4 models:\n",
    "models = [mobilevit_model, swin_model]  # List of models\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Fi``xed random seeds to use\n",
    "seeds = [42, 123, 569]\n",
    "\n",
    "# Assume test_loader is defined (DataLoader for test set)\n",
    "evaluate_ensemble(models, test_loader, device, seeds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b87573-7264-4113-a18c-03988a195a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy import stats  # For majority voting\n",
    "\n",
    "# Function to set random seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # If using CUDA\n",
    "\n",
    "def test_ensemble_majority_voting(models, loader, device):\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, metas, labels in loader:\n",
    "            imgs, metas, labels = imgs.to(device), metas.to(device), labels.to(device)\n",
    "            \n",
    "            # Initialize a list to store individual model predictions\n",
    "            model_preds = []\n",
    "            \n",
    "            # Get predictions from each model and store\n",
    "            for model in models:\n",
    "                outputs = model(imgs, metas)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                model_preds.append(predicted.cpu().numpy())  # Store each model's predictions\n",
    "\n",
    "            # Majority voting: get the majority class from model_preds\n",
    "            # `stats.mode()` returns the most frequent element along axis 0 (per sample)\n",
    "            majority_preds = stats.mode(np.array(model_preds), axis=0)[0]  # Majority vote\n",
    "\n",
    "            # Store the predictions and the true labels\n",
    "            all_preds.extend(majority_preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    return all_labels, all_preds\n",
    "\n",
    "\n",
    "# Function to calculate metrics and return mean ± std deviation\n",
    "def calculate_metrics(true_labels, predictions):\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions, average='macro')\n",
    "    recall = recall_score(true_labels, predictions, average='macro')\n",
    "    f1 = f1_score(true_labels, predictions, average='macro')\n",
    "\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "\n",
    "# Main loop to test the ensemble with majority voting and calculate metrics with standard deviation\n",
    "def evaluate_ensemble_majority_voting(models, test_loader, device, seeds):\n",
    "    accuracies, precisions, recalls, f1_scores = [], [], [], []\n",
    "\n",
    "    # Loop through predefined seeds\n",
    "    for seed in seeds:\n",
    "        set_seed(seed)  # Set the random seed for reproducibility\n",
    "        \n",
    "        # Run the ensemble evaluation with majority voting\n",
    "        true_labels, ensemble_preds = test_ensemble_majority_voting(models, test_loader, device)\n",
    "\n",
    "        # Calculate metrics for this run\n",
    "        accuracy, precision, recall, f1 = calculate_metrics(true_labels, ensemble_preds)\n",
    "\n",
    "        # Store metrics\n",
    "        accuracies.append(accuracy)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    # Calculate mean and standard deviation for each metric\n",
    "    mean_accuracy = np.mean(accuracies)\n",
    "    std_accuracy = np.std(accuracies)\n",
    "\n",
    "    mean_precision = np.mean(precisions)\n",
    "    std_precision = np.std(precisions)\n",
    "\n",
    "    mean_recall = np.mean(recalls)\n",
    "    std_recall = np.std(recalls)\n",
    "\n",
    "    mean_f1 = np.mean(f1_scores)\n",
    "    std_f1 = np.std(f1_scores)\n",
    "\n",
    "    # Print out results\n",
    "    print(f\"Accuracy: {mean_accuracy:.4f} ± {std_accuracy:.4f}\")\n",
    "    print(f\"Precision: {mean_precision:.4f} ± {std_precision:.4f}\")\n",
    "    print(f\"Recall: {mean_recall:.4f} ± {std_recall:.4f}\")\n",
    "    print(f\"F1 Score: {mean_f1:.4f} ± {std_f1:.4f}\")\n",
    "\n",
    "\n",
    "# Example usage with 4 models:\n",
    "models = [mobilevit_model, pv2_model, swin_model]  # List of models\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Fixed random seeds to use\n",
    "seeds = [42, 123, 569]\n",
    "\n",
    "# Assume test_loader is defined (DataLoader for test set)\n",
    "evaluate_ensemble_majority_voting(models, test_loader, device, seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7f12f9-befe-42b2-ac42-dec18bcc6b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy import stats  # For majority voting\n",
    "\n",
    "# Function to set random seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # If using CUDA\n",
    "\n",
    "def test_ensemble_majority_voting(models, loader, device):\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, metas, labels in loader:\n",
    "            imgs, metas, labels = imgs.to(device), metas.to(device), labels.to(device)\n",
    "            \n",
    "            # Initialize a list to store individual model predictions\n",
    "            model_preds = []\n",
    "            \n",
    "            # Get predictions from each model and store\n",
    "            for model in models:\n",
    "                outputs = model(imgs, metas)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                model_preds.append(predicted.cpu().numpy())  # Store each model's predictions\n",
    "\n",
    "            # Majority voting: get the majority class from model_preds\n",
    "            # `stats.mode()` returns the most frequent element along axis 0 (per sample)\n",
    "            majority_preds = stats.mode(np.array(model_preds), axis=0)[0]  # Majority vote\n",
    "\n",
    "            # Store the predictions and the true labels\n",
    "            all_preds.extend(majority_preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    return all_labels, all_preds\n",
    "\n",
    "\n",
    "# Function to calculate metrics and return mean ± std deviation\n",
    "def calculate_metrics(true_labels, predictions):\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions, average='macro')\n",
    "    recall = recall_score(true_labels, predictions, average='macro')\n",
    "    f1 = f1_score(true_labels, predictions, average='macro')\n",
    "\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "\n",
    "# Main loop to test the ensemble with majority voting and calculate metrics with standard deviation\n",
    "def evaluate_ensemble_majority_voting(models, test_loader, device, seeds):\n",
    "    accuracies, precisions, recalls, f1_scores = [], [], [], []\n",
    "\n",
    "    # Loop through predefined seeds\n",
    "    for seed in seeds:\n",
    "        set_seed(seed)  # Set the random seed for reproducibility\n",
    "        \n",
    "        # Run the ensemble evaluation with majority voting\n",
    "        true_labels, ensemble_preds = test_ensemble_majority_voting(models, test_loader, device)\n",
    "\n",
    "        # Calculate metrics for this run\n",
    "        accuracy, precision, recall, f1 = calculate_metrics(true_labels, ensemble_preds)\n",
    "\n",
    "        # Store metrics\n",
    "        accuracies.append(accuracy)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    # Calculate mean and standard deviation for each metric\n",
    "    mean_accuracy = np.mean(accuracies)\n",
    "    std_accuracy = np.std(accuracies)\n",
    "\n",
    "    mean_precision = np.mean(precisions)\n",
    "    std_precision = np.std(precisions)\n",
    "\n",
    "    mean_recall = np.mean(recalls)\n",
    "    std_recall = np.std(recalls)\n",
    "\n",
    "    mean_f1 = np.mean(f1_scores)\n",
    "    std_f1 = np.std(f1_scores)\n",
    "\n",
    "    # Print out results\n",
    "    print(f\"Accuracy: {mean_accuracy:.4f} ± {std_accuracy:.4f}\")\n",
    "    print(f\"Precision: {mean_precision:.4f} ± {std_precision:.4f}\")\n",
    "    print(f\"Recall: {mean_recall:.4f} ± {std_recall:.4f}\")\n",
    "    print(f\"F1 Score: {mean_f1:.4f} ± {std_f1:.4f}\")\n",
    "\n",
    "\n",
    "# Example usage with 4 models:\n",
    "models = [pv2_model, swin_model]  # List of models\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Fixed random seeds to use\n",
    "seeds = [42, 123, 569]\n",
    "\n",
    "# Assume test_loader is defined (DataLoader for test set)\n",
    "evaluate_ensemble_majority_voting(models, test_loader, device, seeds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79efe2db-a29e-4506-be75-374be2442e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy import stats  # For majority voting\n",
    "\n",
    "# Function to set random seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # If using CUDA\n",
    "\n",
    "def test_ensemble_majority_voting(models, loader, device):\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, metas, labels in loader:\n",
    "            imgs, metas, labels = imgs.to(device), metas.to(device), labels.to(device)\n",
    "            \n",
    "            # Initialize a list to store individual model predictions\n",
    "            model_preds = []\n",
    "            \n",
    "            # Get predictions from each model and store\n",
    "            for model in models:\n",
    "                outputs = model(imgs, metas)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                model_preds.append(predicted.cpu().numpy())  # Store each model's predictions\n",
    "\n",
    "            # Majority voting: get the majority class from model_preds\n",
    "            # `stats.mode()` returns the most frequent element along axis 0 (per sample)\n",
    "            majority_preds = stats.mode(np.array(model_preds), axis=0)[0]  # Majority vote\n",
    "\n",
    "            # Store the predictions and the true labels\n",
    "            all_preds.extend(majority_preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    return all_labels, all_preds\n",
    "\n",
    "\n",
    "# Function to calculate metrics and return mean ± std deviation\n",
    "def calculate_metrics(true_labels, predictions):\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions, average='macro')\n",
    "    recall = recall_score(true_labels, predictions, average='macro')\n",
    "    f1 = f1_score(true_labels, predictions, average='macro')\n",
    "\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "\n",
    "# Main loop to test the ensemble with majority voting and calculate metrics with standard deviation\n",
    "def evaluate_ensemble_majority_voting(models, test_loader, device, seeds):\n",
    "    accuracies, precisions, recalls, f1_scores = [], [], [], []\n",
    "\n",
    "    # Loop through predefined seeds\n",
    "    for seed in seeds:\n",
    "        set_seed(seed)  # Set the random seed for reproducibility\n",
    "        \n",
    "        # Run the ensemble evaluation with majority voting\n",
    "        true_labels, ensemble_preds = test_ensemble_majority_voting(models, test_loader, device)\n",
    "\n",
    "        # Calculate metrics for this run\n",
    "        accuracy, precision, recall, f1 = calculate_metrics(true_labels, ensemble_preds)\n",
    "\n",
    "        # Store metrics\n",
    "        accuracies.append(accuracy)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    # Calculate mean and standard deviation for each metric\n",
    "    mean_accuracy = np.mean(accuracies)\n",
    "    std_accuracy = np.std(accuracies)\n",
    "\n",
    "    mean_precision = np.mean(precisions)\n",
    "    std_precision = np.std(precisions)\n",
    "\n",
    "    mean_recall = np.mean(recalls)\n",
    "    std_recall = np.std(recalls)\n",
    "\n",
    "    mean_f1 = np.mean(f1_scores)\n",
    "    std_f1 = np.std(f1_scores)\n",
    "\n",
    "    # Print out results\n",
    "    print(f\"Accuracy: {mean_accuracy:.4f} ± {std_accuracy:.4f}\")\n",
    "    print(f\"Precision: {mean_precision:.4f} ± {std_precision:.4f}\")\n",
    "    print(f\"Recall: {mean_recall:.4f} ± {std_recall:.4f}\")\n",
    "    print(f\"F1 Score: {mean_f1:.4f} ± {std_f1:.4f}\")\n",
    "\n",
    "\n",
    "# Example usage with 4 models:\n",
    "models = [mobilevit_model, pv2_model]  # List of models\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Fixed random seeds to use\n",
    "seeds = [42, 123, 569]\n",
    "\n",
    "# Assume test_loader is defined (DataLoader for test set)\n",
    "evaluate_ensemble_majority_voting(models, test_loader, device, seeds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbeef38-0734-4798-bd36-c2d7694138dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy import stats  # For majority voting\n",
    "\n",
    "# Function to set random seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # If using CUDA\n",
    "\n",
    "def test_ensemble_majority_voting(models, loader, device):\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, metas, labels in loader:\n",
    "            imgs, metas, labels = imgs.to(device), metas.to(device), labels.to(device)\n",
    "            \n",
    "            # Initialize a list to store individual model predictions\n",
    "            model_preds = []\n",
    "            \n",
    "            # Get predictions from each model and store\n",
    "            for model in models:\n",
    "                outputs = model(imgs, metas)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                model_preds.append(predicted.cpu().numpy())  # Store each model's predictions\n",
    "\n",
    "            # Majority voting: get the majority class from model_preds\n",
    "            # `stats.mode()` returns the most frequent element along axis 0 (per sample)\n",
    "            majority_preds = stats.mode(np.array(model_preds), axis=0)[0]  # Majority vote\n",
    "\n",
    "            # Store the predictions and the true labels\n",
    "            all_preds.extend(majority_preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    return all_labels, all_preds\n",
    "\n",
    "\n",
    "# Function to calculate metrics and return mean ± std deviation\n",
    "def calculate_metrics(true_labels, predictions):\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions, average='macro')\n",
    "    recall = recall_score(true_labels, predictions, average='macro')\n",
    "    f1 = f1_score(true_labels, predictions, average='macro')\n",
    "\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "\n",
    "# Main loop to test the ensemble with majority voting and calculate metrics with standard deviation\n",
    "def evaluate_ensemble_majority_voting(models, test_loader, device, seeds):\n",
    "    accuracies, precisions, recalls, f1_scores = [], [], [], []\n",
    "\n",
    "    # Loop through predefined seeds\n",
    "    for seed in seeds:\n",
    "        set_seed(seed)  # Set the random seed for reproducibility\n",
    "        \n",
    "        # Run the ensemble evaluation with majority voting\n",
    "        true_labels, ensemble_preds = test_ensemble_majority_voting(models, test_loader, device)\n",
    "\n",
    "        # Calculate metrics for this run\n",
    "        accuracy, precision, recall, f1 = calculate_metrics(true_labels, ensemble_preds)\n",
    "\n",
    "        # Store metrics\n",
    "        accuracies.append(accuracy)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    # Calculate mean and standard deviation for each metric\n",
    "    mean_accuracy = np.mean(accuracies)\n",
    "    std_accuracy = np.std(accuracies)\n",
    "\n",
    "    mean_precision = np.mean(precisions)\n",
    "    std_precision = np.std(precisions)\n",
    "\n",
    "    mean_recall = np.mean(recalls)\n",
    "    std_recall = np.std(recalls)\n",
    "\n",
    "    mean_f1 = np.mean(f1_scores)\n",
    "    std_f1 = np.std(f1_scores)\n",
    "\n",
    "    # Print out results\n",
    "    print(f\"Accuracy: {mean_accuracy:.4f} ± {std_accuracy:.4f}\")\n",
    "    print(f\"Precision: {mean_precision:.4f} ± {std_precision:.4f}\")\n",
    "    print(f\"Recall: {mean_recall:.4f} ± {std_recall:.4f}\")\n",
    "    print(f\"F1 Score: {mean_f1:.4f} ± {std_f1:.4f}\")\n",
    "\n",
    "\n",
    "# Example usage with 4 models:\n",
    "models = [mobilevit_model, swin_model]  # List of models\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Fixed random seeds to use\n",
    "seeds = [42, 123, 569]\n",
    "\n",
    "# Assume test_loader is defined (DataLoader for test set)\n",
    "evaluate_ensemble_majority_voting(models, test_loader, device, seeds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfdc56f-d37d-4c84-99c8-ab42f909b551",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [mobilevit_model, xception_model]  # List of models\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Fixed random seeds to use\n",
    "seeds = [42, 123, 569]\n",
    "\n",
    "# Assume test_loader is defined (DataLoader for test set)\n",
    "evaluate_ensemble_majority_voting(models, test_loader, device, seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0061ae15-8cab-42fb-aa18-25e01fc91cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [xception_model, swin_model]  # List of models\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Fixed random seeds to use\n",
    "seeds = [42, 123, 569]\n",
    "\n",
    "# Assume test_loader is defined (DataLoader for test set)\n",
    "evaluate_ensemble_majority_voting(models, test_loader, device, seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19295e1-8f48-4341-b2ca-827d72e383d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [xception_model, pv2_model]  # List of models\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Fixed random seeds to use\n",
    "seeds = [42, 123, 569]\n",
    "\n",
    "# Assume test_loader is defined (DataLoader for test set)\n",
    "evaluate_ensemble_majority_voting(models, test_loader, device, seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714e28a7-4426-44e0-b458-7a4295757197",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [mobilevit_model, xception_model, swin_model]  # List of models\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Fixed random seeds to use\n",
    "seeds = [42, 123, 569]\n",
    "\n",
    "# Assume test_loader is defined (DataLoader for test set)\n",
    "evaluate_ensemble_majority_voting(models, test_loader, device, seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2518fd81-0217-4b48-9a60-8911cdc5ae17",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [mobilevit_model, xception_model, pv2_model]  # List of models\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Fixed random seeds to use\n",
    "seeds = [42, 123, 569]\n",
    "\n",
    "# Assume test_loader is defined (DataLoader for test set)\n",
    "evaluate_ensemble_majority_voting(models, test_loader, device, seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301251ee-67f8-4326-82d0-c1f309831bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [mobilevit_model, pv2_model, swin_model]  # List of models\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Fixed random seeds to use\n",
    "seeds = [42, 123, 569]\n",
    "\n",
    "# Assume test_loader is defined (DataLoader for test set)\n",
    "evaluate_ensemble_majority_voting(models, test_loader, device, seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620a3e3a-10c6-40a9-b454-3dd32c534d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [mobilevit_model, xception_model, pv2_model]  # List of models\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Fixed random seeds to use\n",
    "seeds = [42, 123, 569]\n",
    "\n",
    "# Assume test_loader is defined (DataLoader for test set)\n",
    "evaluate_ensemble_majority_voting(models, test_loader, device, seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c63e329-421c-43f0-85a5-cec091ac01e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [xception_model, pv2_model, swin_model]  # List of models\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Fixed random seeds to use\n",
    "seeds = [42, 123, 569]\n",
    "\n",
    "# Assume test_loader is defined (DataLoader for test set)\n",
    "evaluate_ensemble_majority_voting(models, test_loader, device, seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117b54e5-3068-4fee-95a0-ac837f267cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [mobilevit_model, xception_model, pv2_model, swin_model]  # List of models\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Fixed random seeds to use\n",
    "seeds = [42, 123, 569]\n",
    "\n",
    "# Assume test_loader is defined (DataLoader for test set)\n",
    "evaluate_ensemble_majority_voting(models, test_loader, device, seeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b31bbb7-05b1-4ec8-a277-c05852a51d72",
   "metadata": {},
   "source": [
    "<h1>Weighted Averaging</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87251ccf-5ac8-4d54-9096-e251f2f9036d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import optuna\n",
    "import json\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from torch import nn\n",
    "\n",
    "# Function to set random seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # If using CUDA\n",
    "\n",
    "class WeightedAverageEnsemble(nn.Module):\n",
    "    def __init__(self, models, weights):\n",
    "        super(WeightedAverageEnsemble, self).__init__()\n",
    "        self.models = nn.ModuleList(models)\n",
    "        self.weights = weights\n",
    "\n",
    "    def forward(self, x):\n",
    "        weighted_avg = 0\n",
    "        for i, model in enumerate(self.models):\n",
    "            weighted_avg += self.weights[i] * model(x)\n",
    "        return weighted_avg\n",
    "\n",
    "def evaluate_ensemble(models, weights, test_loader, device):\n",
    "    if not isinstance(weights, torch.Tensor):\n",
    "        weights = torch.tensor(weights, device=device)\n",
    "    \n",
    "    # Round to 4 decimal places\n",
    "    weights = torch.round(weights * 10000) / 10000\n",
    "    \n",
    "    # Normalize weights to sum to 1\n",
    "    weights = weights / weights.sum()\n",
    "\n",
    "    # Switch models to evaluation mode\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            # Unpack the batch into inputs, meta, and labels\n",
    "            inputs, meta, labels = batch  # inputs is the image, meta is the metadata\n",
    "\n",
    "            # Move inputs, meta, and labels to device (GPU/CPU)\n",
    "            inputs, meta, labels = inputs.to(device), meta.to(device), labels.to(device)\n",
    "\n",
    "            # Initialize a list to store individual model predictions\n",
    "            model_outputs = []\n",
    "\n",
    "            # Get predictions from each model and store\n",
    "            for model in models:\n",
    "                # Ensure that each model receives both inputs and meta\n",
    "                outputs = model(inputs, meta)\n",
    "                model_outputs.append(outputs)\n",
    "\n",
    "            # Stack model outputs along a new axis (axis 0: models)\n",
    "            model_outputs = torch.stack(model_outputs, dim=0)\n",
    "\n",
    "            # Weighted average of the outputs (according to the weights)\n",
    "            weighted_avg = torch.zeros_like(model_outputs[0])\n",
    "            for i, output in enumerate(model_outputs):\n",
    "                weighted_avg += weights[i] * output\n",
    "\n",
    "            # Convert the averaged outputs to predicted labels\n",
    "            _, predicted = torch.max(weighted_avg, 1)\n",
    "\n",
    "            # Store the predictions and the true labels\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    return accuracy, f1, precision, recall, cm\n",
    "\n",
    "def objective(trial, models, train_loader, val_loader, device):\n",
    "    weights = [trial.suggest_float(f'weight_{i+1}', 0.0, 1.0) for i in range(len(models))]\n",
    "    normalized_weights = [round(w, 4) for w in weights]  # Round to 4 decimal places\n",
    "\n",
    "    # Evaluate the ensemble with the suggested weights\n",
    "    accuracy, _, _, _, _ = evaluate_ensemble(models, normalized_weights, val_loader, device)\n",
    "    return accuracy\n",
    "\n",
    "def evaluate_ensemble_with_optuna(models, train_loader, val_loader, test_loader, device, seeds, save_weights_file):\n",
    "    best_weights_across_seeds = {}\n",
    "\n",
    "    # Lists to store metrics for all seeds\n",
    "    accuracies, precisions, recalls, f1_scores = [], [], [], []\n",
    "\n",
    "    for seed in seeds:\n",
    "        # Set the seed for reproducibility\n",
    "        set_seed(seed)\n",
    "\n",
    "        # Create a study for hyperparameter optimization\n",
    "        study = optuna.create_study(direction='maximize')\n",
    "        study.optimize(lambda trial: objective(trial, models, train_loader, val_loader, device), n_trials=30)\n",
    "\n",
    "        best_weights = study.best_trial.params\n",
    "        best_weights_tensor = torch.tensor([round(best_weights[f'weight_{i+1}'], 4) for i in range(len(models))], device=device)\n",
    "        normalized_best_weights = best_weights_tensor / best_weights_tensor.sum()\n",
    "\n",
    "        # Save the best weights for this seed\n",
    "        best_weights_across_seeds[seed] = normalized_best_weights.cpu().numpy().tolist()\n",
    "\n",
    "        # Print out best weights for this seed\n",
    "        print(f'Best Weights for Seed {seed}: {normalized_best_weights.cpu().numpy()}')\n",
    "        print(f'Best Accuracy for Seed {seed}: {study.best_value:.4f}')\n",
    "\n",
    "        # Evaluate the ensemble with the best weights on the test set\n",
    "        accuracy, f1, precision, recall, cm = evaluate_ensemble(models, normalized_best_weights, test_loader, device)\n",
    "\n",
    "        # Collect metrics for later calculation of mean and std deviation\n",
    "        accuracies.append(accuracy)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "        # Save the confusion matrix as well for further analysis\n",
    "        print(f'Confusion Matrix for Seed {seed}:\\n{cm}')\n",
    "\n",
    "    # Save best weights for all seeds\n",
    "    with open(save_weights_file, 'w') as f:\n",
    "        json.dump(best_weights_across_seeds, f)\n",
    "\n",
    "    # Calculate mean and standard deviation for each metric\n",
    "    mean_accuracy = np.mean(accuracies)\n",
    "    std_accuracy = np.std(accuracies)\n",
    "\n",
    "    mean_precision = np.mean(precisions)\n",
    "    std_precision = np.std(precisions)\n",
    "\n",
    "    mean_recall = np.mean(recalls)\n",
    "    std_recall = np.std(recalls)\n",
    "\n",
    "    mean_f1 = np.mean(f1_scores)\n",
    "    std_f1 = np.std(f1_scores)\n",
    "\n",
    "    # Print out results with mean and standard deviation\n",
    "    print(f\"Accuracy: {mean_accuracy:.4f} ± {std_accuracy:.4f}\")\n",
    "    print(f\"Precision: {mean_precision:.4f} ± {std_precision:.4f}\")\n",
    "    print(f\"Recall: {mean_recall:.4f} ± {std_recall:.4f}\")\n",
    "    print(f\"F1 Score: {mean_f1:.4f} ± {std_f1:.4f}\")\n",
    "\n",
    "\n",
    "# Example usage with models:\n",
    "models = [mobilevit_model, pv2_model, swin_model]  # List of models\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Fixed random seeds to use\n",
    "seeds = [42, 123, 569]\n",
    "\n",
    "# File path to save the best weights\n",
    "save_weights_file = 'best_weights.json'\n",
    "\n",
    "# Assume test_loader, train_loader, and val_loader are defined (DataLoader for test, train, and validation sets)\n",
    "evaluate_ensemble_with_optuna(models, train_loader, val_loader, test_loader, device, seeds, save_weights_file)\n",
    "\n",
    "# Example of loading weights and using them for prediction\n",
    "with open(save_weights_file, 'r') as f:\n",
    "    best_weights = json.load(f)\n",
    "\n",
    "# Example: print the best weights for seed 42\n",
    "print(f'Best Weights for Seed 42: {best_weights[\"42\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6ae3a0-bc32-4718-8d8f-c8c97b4643a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()  # Clear unused GPU memory cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7075268d-9ad6-4c43-9f19-7960650b6c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import optuna\n",
    "import json\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from torch import nn\n",
    "\n",
    "# Function to set random seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # If using CUDA\n",
    "\n",
    "class WeightedAverageEnsemble(nn.Module):\n",
    "    def __init__(self, models, weights):\n",
    "        super(WeightedAverageEnsemble, self).__init__()\n",
    "        self.models = nn.ModuleList(models)\n",
    "        self.weights = weights\n",
    "\n",
    "    def forward(self, x):\n",
    "        weighted_avg = 0\n",
    "        for i, model in enumerate(self.models):\n",
    "            weighted_avg += self.weights[i] * model(x)\n",
    "        return weighted_avg\n",
    "\n",
    "def evaluate_ensemble(models, weights, test_loader, device):\n",
    "    if not isinstance(weights, torch.Tensor):\n",
    "        weights = torch.tensor(weights, device=device)\n",
    "    \n",
    "    # Round to 4 decimal places\n",
    "    weights = torch.round(weights * 10000) / 10000\n",
    "    \n",
    "    # Normalize weights to sum to 1\n",
    "    weights = weights / weights.sum()\n",
    "\n",
    "    # Switch models to evaluation mode\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            # Unpack the batch into inputs, meta, and labels\n",
    "            inputs, meta, labels = batch  # inputs is the image, meta is the metadata\n",
    "\n",
    "            # Move inputs, meta, and labels to device (GPU/CPU)\n",
    "            inputs, meta, labels = inputs.to(device), meta.to(device), labels.to(device)\n",
    "\n",
    "            # Initialize a list to store individual model predictions\n",
    "            model_outputs = []\n",
    "\n",
    "            # Get predictions from each model and store\n",
    "            for model in models:\n",
    "                # Ensure that each model receives both inputs and meta\n",
    "                outputs = model(inputs, meta)\n",
    "                model_outputs.append(outputs)\n",
    "\n",
    "            # Stack model outputs along a new axis (axis 0: models)\n",
    "            model_outputs = torch.stack(model_outputs, dim=0)\n",
    "\n",
    "            # Weighted average of the outputs (according to the weights)\n",
    "            weighted_avg = torch.zeros_like(model_outputs[0])\n",
    "            for i, output in enumerate(model_outputs):\n",
    "                weighted_avg += weights[i] * output\n",
    "\n",
    "            # Convert the averaged outputs to predicted labels\n",
    "            _, predicted = torch.max(weighted_avg, 1)\n",
    "\n",
    "            # Store the predictions and the true labels\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    return accuracy, f1, precision, recall, cm\n",
    "\n",
    "def objective(trial, models, train_loader, val_loader, device):\n",
    "    weights = [trial.suggest_float(f'weight_{i+1}', 0.0, 1.0) for i in range(len(models))]\n",
    "    normalized_weights = [round(w, 4) for w in weights]  # Round to 4 decimal places\n",
    "\n",
    "    # Evaluate the ensemble with the suggested weights\n",
    "    accuracy, _, _, _, _ = evaluate_ensemble(models, normalized_weights, val_loader, device)\n",
    "    return accuracy\n",
    "\n",
    "def evaluate_ensemble_with_optuna(models, train_loader, val_loader, test_loader, device, seeds, save_weights_file):\n",
    "    best_weights_across_seeds = {}\n",
    "\n",
    "    # Lists to store metrics for all seeds\n",
    "    accuracies, precisions, recalls, f1_scores = [], [], [], []\n",
    "\n",
    "    for seed in seeds:\n",
    "        # Set the seed for reproducibility\n",
    "        set_seed(seed)\n",
    "\n",
    "        # Create a study for hyperparameter optimization\n",
    "        study = optuna.create_study(direction='maximize')\n",
    "        study.optimize(lambda trial: objective(trial, models, train_loader, val_loader, device), n_trials=30)\n",
    "\n",
    "        best_weights = study.best_trial.params\n",
    "        best_weights_tensor = torch.tensor([round(best_weights[f'weight_{i+1}'], 4) for i in range(len(models))], device=device)\n",
    "        normalized_best_weights = best_weights_tensor / best_weights_tensor.sum()\n",
    "\n",
    "        # Save the best weights for this seed\n",
    "        best_weights_across_seeds[seed] = normalized_best_weights.cpu().numpy().tolist()\n",
    "\n",
    "        # Print out best weights for this seed\n",
    "        print(f'Best Weights for Seed {seed}: {normalized_best_weights.cpu().numpy()}')\n",
    "        print(f'Best Accuracy for Seed {seed}: {study.best_value:.4f}')\n",
    "\n",
    "        # Evaluate the ensemble with the best weights on the test set\n",
    "        accuracy, f1, precision, recall, cm = evaluate_ensemble(models, normalized_best_weights, test_loader, device)\n",
    "\n",
    "        # Collect metrics for later calculation of mean and std deviation\n",
    "        accuracies.append(accuracy)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "        # Save the confusion matrix as well for further analysis\n",
    "        print(f'Confusion Matrix for Seed {seed}:\\n{cm}')\n",
    "\n",
    "    # Save best weights for all seeds\n",
    "    with open(save_weights_file, 'w') as f:\n",
    "        json.dump(best_weights_across_seeds, f)\n",
    "\n",
    "    # Calculate mean and standard deviation for each metric\n",
    "    mean_accuracy = np.mean(accuracies)\n",
    "    std_accuracy = np.std(accuracies)\n",
    "\n",
    "    mean_precision = np.mean(precisions)\n",
    "    std_precision = np.std(precisions)\n",
    "\n",
    "    mean_recall = np.mean(recalls)\n",
    "    std_recall = np.std(recalls)\n",
    "\n",
    "    mean_f1 = np.mean(f1_scores)\n",
    "    std_f1 = np.std(f1_scores)\n",
    "\n",
    "    # Print out results with mean and standard deviation\n",
    "    print(f\"Accuracy: {mean_accuracy:.4f} ± {std_accuracy:.4f}\")\n",
    "    print(f\"Precision: {mean_precision:.4f} ± {std_precision:.4f}\")\n",
    "    print(f\"Recall: {mean_recall:.4f} ± {std_recall:.4f}\")\n",
    "    print(f\"F1 Score: {mean_f1:.4f} ± {std_f1:.4f}\")\n",
    "\n",
    "\n",
    "# Example usage with models:\n",
    "models = [pv2_model, swin_model]  # List of models\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Fixed random seeds to use\n",
    "seeds = [42, 123, 569]\n",
    "\n",
    "# File path to save the best weights\n",
    "save_weights_file = 'best_weights1.json'\n",
    "\n",
    "# Assume test_loader, train_loader, and val_loader are defined (DataLoader for test, train, and validation sets)\n",
    "evaluate_ensemble_with_optuna(models, train_loader, val_loader, test_loader, device, seeds, save_weights_file)\n",
    "\n",
    "# Example of loading weights and using them for prediction\n",
    "with open(save_weights_file, 'r') as f:\n",
    "    best_weights = json.load(f)\n",
    "\n",
    "# Example: print the best weights for seed 42\n",
    "print(f'Best Weights for Seed 42: {best_weights[\"42\"]}')\n",
    "\n",
    "torch.cuda.empty_cache()  # Clear unused GPU memory cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d199adab-d6e8-4033-b7f2-54c873ab3b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()  # Clear unused GPU memory cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91d15cd-997d-4bcd-bcee-5387e259e0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import optuna\n",
    "import json\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from torch import nn\n",
    "\n",
    "# Function to set random seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # If using CUDA\n",
    "\n",
    "class WeightedAverageEnsemble(nn.Module):\n",
    "    def __init__(self, models, weights):\n",
    "        super(WeightedAverageEnsemble, self).__init__()\n",
    "        self.models = nn.ModuleList(models)\n",
    "        self.weights = weights\n",
    "\n",
    "    def forward(self, x):\n",
    "        weighted_avg = 0\n",
    "        for i, model in enumerate(self.models):\n",
    "            weighted_avg += self.weights[i] * model(x)\n",
    "        return weighted_avg\n",
    "\n",
    "def evaluate_ensemble(models, weights, test_loader, device):\n",
    "    if not isinstance(weights, torch.Tensor):\n",
    "        weights = torch.tensor(weights, device=device)\n",
    "    \n",
    "    # Round to 4 decimal places\n",
    "    weights = torch.round(weights * 10000) / 10000\n",
    "    \n",
    "    # Normalize weights to sum to 1\n",
    "    weights = weights / weights.sum()\n",
    "\n",
    "    # Switch models to evaluation mode\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            # Unpack the batch into inputs, meta, and labels\n",
    "            inputs, meta, labels = batch  # inputs is the image, meta is the metadata\n",
    "\n",
    "            # Move inputs, meta, and labels to device (GPU/CPU)\n",
    "            inputs, meta, labels = inputs.to(device), meta.to(device), labels.to(device)\n",
    "\n",
    "            # Initialize a list to store individual model predictions\n",
    "            model_outputs = []\n",
    "\n",
    "            # Get predictions from each model and store\n",
    "            for model in models:\n",
    "                # Ensure that each model receives both inputs and meta\n",
    "                outputs = model(inputs, meta)\n",
    "                model_outputs.append(outputs)\n",
    "\n",
    "            # Stack model outputs along a new axis (axis 0: models)\n",
    "            model_outputs = torch.stack(model_outputs, dim=0)\n",
    "\n",
    "            # Weighted average of the outputs (according to the weights)\n",
    "            weighted_avg = torch.zeros_like(model_outputs[0])\n",
    "            for i, output in enumerate(model_outputs):\n",
    "                weighted_avg += weights[i] * output\n",
    "\n",
    "            # Convert the averaged outputs to predicted labels\n",
    "            _, predicted = torch.max(weighted_avg, 1)\n",
    "\n",
    "            # Store the predictions and the true labels\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    return accuracy, f1, precision, recall, cm\n",
    "\n",
    "def objective(trial, models, train_loader, val_loader, device):\n",
    "    weights = [trial.suggest_float(f'weight_{i+1}', 0.0, 1.0) for i in range(len(models))]\n",
    "    normalized_weights = [round(w, 4) for w in weights]  # Round to 4 decimal places\n",
    "\n",
    "    # Evaluate the ensemble with the suggested weights\n",
    "    accuracy, _, _, _, _ = evaluate_ensemble(models, normalized_weights, val_loader, device)\n",
    "    return accuracy\n",
    "\n",
    "def evaluate_ensemble_with_optuna(models, train_loader, val_loader, test_loader, device, seeds, save_weights_file):\n",
    "    best_weights_across_seeds = {}\n",
    "\n",
    "    # Lists to store metrics for all seeds\n",
    "    accuracies, precisions, recalls, f1_scores = [], [], [], []\n",
    "\n",
    "    for seed in seeds:\n",
    "        # Set the seed for reproducibility\n",
    "        set_seed(seed)\n",
    "\n",
    "        # Create a study for hyperparameter optimization\n",
    "        study = optuna.create_study(direction='maximize')\n",
    "        study.optimize(lambda trial: objective(trial, models, train_loader, val_loader, device), n_trials=30)\n",
    "\n",
    "        best_weights = study.best_trial.params\n",
    "        best_weights_tensor = torch.tensor([round(best_weights[f'weight_{i+1}'], 4) for i in range(len(models))], device=device)\n",
    "        normalized_best_weights = best_weights_tensor / best_weights_tensor.sum()\n",
    "\n",
    "        # Save the best weights for this seed\n",
    "        best_weights_across_seeds[seed] = normalized_best_weights.cpu().numpy().tolist()\n",
    "\n",
    "        # Print out best weights for this seed\n",
    "        print(f'Best Weights for Seed {seed}: {normalized_best_weights.cpu().numpy()}')\n",
    "        print(f'Best Accuracy for Seed {seed}: {study.best_value:.4f}')\n",
    "\n",
    "        # Evaluate the ensemble with the best weights on the test set\n",
    "        accuracy, f1, precision, recall, cm = evaluate_ensemble(models, normalized_best_weights, test_loader, device)\n",
    "\n",
    "        # Collect metrics for later calculation of mean and std deviation\n",
    "        accuracies.append(accuracy)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "        # Save the confusion matrix as well for further analysis\n",
    "        print(f'Confusion Matrix for Seed {seed}:\\n{cm}')\n",
    "\n",
    "    # Save best weights for all seeds\n",
    "    with open(save_weights_file, 'w') as f:\n",
    "        json.dump(best_weights_across_seeds, f)\n",
    "\n",
    "    # Calculate mean and standard deviation for each metric\n",
    "    mean_accuracy = np.mean(accuracies)\n",
    "    std_accuracy = np.std(accuracies)\n",
    "\n",
    "    mean_precision = np.mean(precisions)\n",
    "    std_precision = np.std(precisions)\n",
    "\n",
    "    mean_recall = np.mean(recalls)\n",
    "    std_recall = np.std(recalls)\n",
    "\n",
    "    mean_f1 = np.mean(f1_scores)\n",
    "    std_f1 = np.std(f1_scores)\n",
    "\n",
    "    # Print out results with mean and standard deviation\n",
    "    print(f\"Accuracy: {mean_accuracy:.4f} ± {std_accuracy:.4f}\")\n",
    "    print(f\"Precision: {mean_precision:.4f} ± {std_precision:.4f}\")\n",
    "    print(f\"Recall: {mean_recall:.4f} ± {std_recall:.4f}\")\n",
    "    print(f\"F1 Score: {mean_f1:.4f} ± {std_f1:.4f}\")\n",
    "\n",
    "\n",
    "# Example usage with models:\n",
    "models = [mobilevit_model, pv2_model]  # List of models\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Fixed random seeds to use\n",
    "seeds = [42, 123, 569]\n",
    "\n",
    "# File path to save the best weights\n",
    "save_weights_file = 'best_weights2.json'\n",
    "\n",
    "# Assume test_loader, train_loader, and val_loader are defined (DataLoader for test, train, and validation sets)\n",
    "evaluate_ensemble_with_optuna(models, train_loader, val_loader, test_loader, device, seeds, save_weights_file)\n",
    "\n",
    "# Example of loading weights and using them for prediction\n",
    "with open(save_weights_file, 'r') as f:\n",
    "    best_weights = json.load(f)\n",
    "\n",
    "# Example: print the best weights for seed 42\n",
    "print(f'Best Weights for Seed 42: {best_weights[\"42\"]}')\n",
    "torch.cuda.empty_cache()  # Clear unused GPU memory cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830755cc-87b3-4a11-81ad-758735bce4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import optuna\n",
    "import json\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from torch import nn\n",
    "\n",
    "# Function to set random seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # If using CUDA\n",
    "\n",
    "class WeightedAverageEnsemble(nn.Module):\n",
    "    def __init__(self, models, weights):\n",
    "        super(WeightedAverageEnsemble, self).__init__()\n",
    "        self.models = nn.ModuleList(models)\n",
    "        self.weights = weights\n",
    "\n",
    "    def forward(self, x):\n",
    "        weighted_avg = 0\n",
    "        for i, model in enumerate(self.models):\n",
    "            weighted_avg += self.weights[i] * model(x)\n",
    "        return weighted_avg\n",
    "\n",
    "def evaluate_ensemble(models, weights, test_loader, device):\n",
    "    if not isinstance(weights, torch.Tensor):\n",
    "        weights = torch.tensor(weights, device=device)\n",
    "    \n",
    "    # Round to 4 decimal places\n",
    "    weights = torch.round(weights * 10000) / 10000\n",
    "    \n",
    "    # Normalize weights to sum to 1\n",
    "    weights = weights / weights.sum()\n",
    "\n",
    "    # Switch models to evaluation mode\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            # Unpack the batch into inputs, meta, and labels\n",
    "            inputs, meta, labels = batch  # inputs is the image, meta is the metadata\n",
    "\n",
    "            # Move inputs, meta, and labels to device (GPU/CPU)\n",
    "            inputs, meta, labels = inputs.to(device), meta.to(device), labels.to(device)\n",
    "\n",
    "            # Initialize a list to store individual model predictions\n",
    "            model_outputs = []\n",
    "\n",
    "            # Get predictions from each model and store\n",
    "            for model in models:\n",
    "                # Ensure that each model receives both inputs and meta\n",
    "                outputs = model(inputs, meta)\n",
    "                model_outputs.append(outputs)\n",
    "\n",
    "            # Stack model outputs along a new axis (axis 0: models)\n",
    "            model_outputs = torch.stack(model_outputs, dim=0)\n",
    "\n",
    "            # Weighted average of the outputs (according to the weights)\n",
    "            weighted_avg = torch.zeros_like(model_outputs[0])\n",
    "            for i, output in enumerate(model_outputs):\n",
    "                weighted_avg += weights[i] * output\n",
    "\n",
    "            # Convert the averaged outputs to predicted labels\n",
    "            _, predicted = torch.max(weighted_avg, 1)\n",
    "\n",
    "            # Store the predictions and the true labels\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    return accuracy, f1, precision, recall, cm\n",
    "\n",
    "def objective(trial, models, train_loader, val_loader, device):\n",
    "    weights = [trial.suggest_float(f'weight_{i+1}', 0.0, 1.0) for i in range(len(models))]\n",
    "    normalized_weights = [round(w, 4) for w in weights]  # Round to 4 decimal places\n",
    "\n",
    "    # Evaluate the ensemble with the suggested weights\n",
    "    accuracy, _, _, _, _ = evaluate_ensemble(models, normalized_weights, val_loader, device)\n",
    "    return accuracy\n",
    "\n",
    "def evaluate_ensemble_with_optuna(models, train_loader, val_loader, test_loader, device, seeds, save_weights_file):\n",
    "    best_weights_across_seeds = {}\n",
    "\n",
    "    # Lists to store metrics for all seeds\n",
    "    accuracies, precisions, recalls, f1_scores = [], [], [], []\n",
    "\n",
    "    for seed in seeds:\n",
    "        # Set the seed for reproducibility\n",
    "        set_seed(seed)\n",
    "\n",
    "        # Create a study for hyperparameter optimization\n",
    "        study = optuna.create_study(direction='maximize')\n",
    "        study.optimize(lambda trial: objective(trial, models, train_loader, val_loader, device), n_trials=30)\n",
    "\n",
    "        best_weights = study.best_trial.params\n",
    "        best_weights_tensor = torch.tensor([round(best_weights[f'weight_{i+1}'], 4) for i in range(len(models))], device=device)\n",
    "        normalized_best_weights = best_weights_tensor / best_weights_tensor.sum()\n",
    "\n",
    "        # Save the best weights for this seed\n",
    "        best_weights_across_seeds[seed] = normalized_best_weights.cpu().numpy().tolist()\n",
    "\n",
    "        # Print out best weights for this seed\n",
    "        print(f'Best Weights for Seed {seed}: {normalized_best_weights.cpu().numpy()}')\n",
    "        print(f'Best Accuracy for Seed {seed}: {study.best_value:.4f}')\n",
    "\n",
    "        # Evaluate the ensemble with the best weights on the test set\n",
    "        accuracy, f1, precision, recall, cm = evaluate_ensemble(models, normalized_best_weights, test_loader, device)\n",
    "\n",
    "        # Collect metrics for later calculation of mean and std deviation\n",
    "        accuracies.append(accuracy)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "        # Save the confusion matrix as well for further analysis\n",
    "        print(f'Confusion Matrix for Seed {seed}:\\n{cm}')\n",
    "\n",
    "    # Save best weights for all seeds\n",
    "    with open(save_weights_file, 'w') as f:\n",
    "        json.dump(best_weights_across_seeds, f)\n",
    "\n",
    "    # Calculate mean and standard deviation for each metric\n",
    "    mean_accuracy = np.mean(accuracies)\n",
    "    std_accuracy = np.std(accuracies)\n",
    "\n",
    "    mean_precision = np.mean(precisions)\n",
    "    std_precision = np.std(precisions)\n",
    "\n",
    "    mean_recall = np.mean(recalls)\n",
    "    std_recall = np.std(recalls)\n",
    "\n",
    "    mean_f1 = np.mean(f1_scores)\n",
    "    std_f1 = np.std(f1_scores)\n",
    "\n",
    "    # Print out results with mean and standard deviation\n",
    "    print(f\"Accuracy: {mean_accuracy:.4f} ± {std_accuracy:.4f}\")\n",
    "    print(f\"Precision: {mean_precision:.4f} ± {std_precision:.4f}\")\n",
    "    print(f\"Recall: {mean_recall:.4f} ± {std_recall:.4f}\")\n",
    "    print(f\"F1 Score: {mean_f1:.4f} ± {std_f1:.4f}\")\n",
    "\n",
    "\n",
    "# Example usage with models:\n",
    "models = [mobilevit_model, swin_model]  # List of models\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Fixed random seeds to use\n",
    "seeds = [42, 123, 569]\n",
    "\n",
    "# File path to save the best weights\n",
    "save_weights_file = 'best_weights3.json'\n",
    "\n",
    "# Assume test_loader, train_loader, and val_loader are defined (DataLoader for test, train, and validation sets)\n",
    "evaluate_ensemble_with_optuna(models, train_loader, val_loader, test_loader, device, seeds, save_weights_file)\n",
    "\n",
    "# Example of loading weights and using them for prediction\n",
    "with open(save_weights_file, 'r') as f:\n",
    "    best_weights = json.load(f)\n",
    "\n",
    "# Example: print the best weights for seed 42\n",
    "print(f'Best Weights for Seed 42: {best_weights[\"42\"]}')\n",
    "torch.cuda.empty_cache()  # Clear unused GPU memory cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9528ce5f-bc7f-46d7-ae5a-38e18eabefb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [mobilevit_model, xception_model]  # List of models\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Fixed random seeds to use\n",
    "seeds = [42, 123, 569]\n",
    "\n",
    "# File path to save the best weights\n",
    "save_weights_file = 'best_weights3.json'\n",
    "\n",
    "# Assume test_loader, train_loader, and val_loader are defined (DataLoader for test, train, and validation sets)\n",
    "evaluate_ensemble_with_optuna(models, train_loader, val_loader, test_loader, device, seeds, save_weights_file)\n",
    "\n",
    "# Example of loading weights and using them for prediction\n",
    "with open(save_weights_file, 'r') as f:\n",
    "    best_weights = json.load(f)\n",
    "\n",
    "# Example: print the best weights for seed 42\n",
    "print(f'Best Weights for Seed 42: {best_weights[\"42\"]}')\n",
    "torch.cuda.empty_cache()  # Clear unused GPU memory cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c5b161-d118-4f85-b292-1a11922379df",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [xception_model, swin_model]  # List of models\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Fixed random seeds to use\n",
    "seeds = [42, 123, 569]\n",
    "\n",
    "# File path to save the best weights\n",
    "save_weights_file = 'best_weights3.json'\n",
    "\n",
    "# Assume test_loader, train_loader, and val_loader are defined (DataLoader for test, train, and validation sets)\n",
    "evaluate_ensemble_with_optuna(models, train_loader, val_loader, test_loader, device, seeds, save_weights_file)\n",
    "\n",
    "# Example of loading weights and using them for prediction\n",
    "with open(save_weights_file, 'r') as f:\n",
    "    best_weights = json.load(f)\n",
    "\n",
    "# Example: print the best weights for seed 42\n",
    "print(f'Best Weights for Seed 42: {best_weights[\"42\"]}')\n",
    "torch.cuda.empty_cache()  # Clear unused GPU memory cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0fa246-70d2-4892-b5b7-d7d9bf1f5a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [xception_model, pv2_model]  # List of models\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Fixed random seeds to use\n",
    "seeds = [42, 123, 569]\n",
    "\n",
    "# File path to save the best weights\n",
    "save_weights_file = 'best_weights3.json'\n",
    "\n",
    "# Assume test_loader, train_loader, and val_loader are defined (DataLoader for test, train, and validation sets)\n",
    "evaluate_ensemble_with_optuna(models, train_loader, val_loader, test_loader, device, seeds, save_weights_file)\n",
    "\n",
    "# Example of loading weights and using them for prediction\n",
    "with open(save_weights_file, 'r') as f:\n",
    "    best_weights = json.load(f)\n",
    "\n",
    "# Example: print the best weights for seed 42\n",
    "print(f'Best Weights for Seed 42: {best_weights[\"42\"]}')\n",
    "torch.cuda.empty_cache()  # Clear unused GPU memory cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047726de-b85f-4b98-8599-c2e583045f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [mobilevit_model, xception_model, swin_model]  # List of models\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Fixed random seeds to use\n",
    "seeds = [42, 123, 569]\n",
    "\n",
    "# File path to save the best weights\n",
    "save_weights_file = 'best_weights3.json'\n",
    "\n",
    "# Assume test_loader, train_loader, and val_loader are defined (DataLoader for test, train, and validation sets)\n",
    "evaluate_ensemble_with_optuna(models, train_loader, val_loader, test_loader, device, seeds, save_weights_file)\n",
    "\n",
    "# Example of loading weights and using them for prediction\n",
    "with open(save_weights_file, 'r') as f:\n",
    "    best_weights = json.load(f)\n",
    "\n",
    "# Example: print the best weights for seed 42\n",
    "print(f'Best Weights for Seed 42: {best_weights[\"42\"]}')\n",
    "torch.cuda.empty_cache()  # Clear unused GPU memory cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb130c00-c806-421c-a0ce-4764f4ce76bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [mobilevit_model, xception_model, pv2_model]  # List of models\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Fixed random seeds to use\n",
    "seeds = [42, 123, 569]\n",
    "\n",
    "# File path to save the best weights\n",
    "save_weights_file = 'best_weights3.json'\n",
    "\n",
    "# Assume test_loader, train_loader, and val_loader are defined (DataLoader for test, train, and validation sets)\n",
    "evaluate_ensemble_with_optuna(models, train_loader, val_loader, test_loader, device, seeds, save_weights_file)\n",
    "\n",
    "# Example of loading weights and using them for prediction\n",
    "with open(save_weights_file, 'r') as f:\n",
    "    best_weights = json.load(f)\n",
    "\n",
    "# Example: print the best weights for seed 42\n",
    "print(f'Best Weights for Seed 42: {best_weights[\"42\"]}')\n",
    "torch.cuda.empty_cache()  # Clear unused GPU memory cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450f99fa-7293-42d2-bd1e-7e9c1c2a0667",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [pv2_model, xception_model, swin_model]  # List of models\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Fixed random seeds to use\n",
    "seeds = [42, 123, 569]\n",
    "\n",
    "# File path to save the best weights\n",
    "save_weights_file = 'best_weights3.json'\n",
    "\n",
    "# Assume test_loader, train_loader, and val_loader are defined (DataLoader for test, train, and validation sets)\n",
    "evaluate_ensemble_with_optuna(models, train_loader, val_loader, test_loader, device, seeds, save_weights_file)\n",
    "\n",
    "# Example of loading weights and using them for prediction\n",
    "with open(save_weights_file, 'r') as f:\n",
    "    best_weights = json.load(f)\n",
    "\n",
    "# Example: print the best weights for seed 42\n",
    "print(f'Best Weights for Seed 42: {best_weights[\"42\"]}')\n",
    "torch.cuda.empty_cache()  # Clear unused GPU memory cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f380b0-73d4-4340-8547-709033802b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [mobilevit_model, xception_model, pv2_model, swin_model]  # List of models\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Fixed random seeds to use\n",
    "seeds = [42, 123, 569]\n",
    "\n",
    "# File path to save the best weights\n",
    "save_weights_file = 'best_weights3.json'\n",
    "\n",
    "# Assume test_loader, train_loader, and val_loader are defined (DataLoader for test, train, and validation sets)\n",
    "evaluate_ensemble_with_optuna(models, train_loader, val_loader, test_loader, device, seeds, save_weights_file)\n",
    "\n",
    "# Example of loading weights and using them for prediction\n",
    "with open(save_weights_file, 'r') as f:\n",
    "    best_weights = json.load(f)\n",
    "\n",
    "# Example: print the best weights for seed 42\n",
    "print(f'Best Weights for Seed 42: {best_weights[\"42\"]}')\n",
    "torch.cuda.empty_cache()  # Clear unused GPU memory cache"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
