{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf1cf7d-e12a-469e-b3c6-c37dee98db7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T05:23:16.930871Z",
     "iopub.status.busy": "2025-11-17T05:23:16.930060Z",
     "iopub.status.idle": "2025-11-17T05:23:18.758376Z",
     "shell.execute_reply": "2025-11-17T05:23:18.757392Z",
     "shell.execute_reply.started": "2025-11-17T05:23:16.930842Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# DERM7PT DATA LOADER FOR TRAINING NOTEBOOKS\n",
    "# =========================================================\n",
    "# Run this code in your training notebooks to load the preprocessed Derm7pt dataset\n",
    "# Make sure the preprocessing pipeline above has been executed first!\n",
    "\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "# =========================================================\n",
    "# PATHS TO PREPROCESSED DATA\n",
    "# =========================================================\n",
    "PREPROCESSED_DIR = r\"augmented\"\n",
    "\n",
    "TRAIN_CSV = os.path.join(PREPROCESSED_DIR, \"train_metadata_final.csv\")\n",
    "VAL_CSV   = os.path.join(PREPROCESSED_DIR, \"val_metadata_final.csv\")\n",
    "TEST_CSV  = os.path.join(PREPROCESSED_DIR, \"test_metadata_final.csv\")\n",
    "\n",
    "INFO_PATH = os.path.join(PREPROCESSED_DIR, \"preprocessing_info.json\")\n",
    "\n",
    "# =========================================================\n",
    "# LOAD PREPROCESSED DATA\n",
    "# =========================================================\n",
    "print(\"Loading preprocessed Derm7pt data...\")\n",
    "\n",
    "# Load CSVs\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "val_df   = pd.read_csv(VAL_CSV)\n",
    "test_df  = pd.read_csv(TEST_CSV)\n",
    "\n",
    "# Load preprocessing info\n",
    "with open(INFO_PATH, \"r\") as f:\n",
    "    preprocessing_info = json.load(f)\n",
    "\n",
    "categorical_cols = preprocessing_info[\"categorical_cols\"]\n",
    "label_mapping = preprocessing_info[\"label_mapping\"]\n",
    "\n",
    "# Extract class names from label_mapping (sorted by label index)\n",
    "class_names = [k for k, v in sorted(label_mapping.items(), key=lambda x: x[1])]\n",
    "\n",
    "print(f\"\\n‚úÖ Training samples:   {len(train_df)}\")\n",
    "print(f\"‚úÖ Validation samples: {len(val_df)}\")\n",
    "print(f\"‚úÖ Test samples:       {len(test_df)}\")\n",
    "print(f\"\\nLabel mapping: {label_mapping}\")\n",
    "print(f\"Class names: {class_names}\")\n",
    "\n",
    "# =========================================================\n",
    "# EXTRACT FEATURES AND LABELS\n",
    "# =========================================================\n",
    "def extract_features(df):\n",
    "    \"\"\"Extract image paths, metadata features, and labels from dataframe\"\"\"\n",
    "    img_paths = df[\"ImagePath\"].values\n",
    "    labels = df[\"label\"].values\n",
    "    \n",
    "    # Metadata features (all columns except ImagePath and label)\n",
    "    metadata_cols = [col for col in df.columns if col not in [\"ImagePath\", \"label\"]]\n",
    "    metadata = df[metadata_cols].values\n",
    "    \n",
    "    return img_paths, metadata, labels\n",
    "\n",
    "X_train_img, X_train_meta, y_train = extract_features(train_df)\n",
    "X_val_img, X_val_meta, y_val       = extract_features(val_df)\n",
    "X_test_img, X_test_meta, y_test    = extract_features(test_df)\n",
    "\n",
    "num_classes = len(label_mapping)\n",
    "print(f\"\\nNumber of classes: {num_classes}\")\n",
    "\n",
    "# =========================================================\n",
    "# PYTORCH DATASET CLASS\n",
    "# =========================================================\n",
    "class Derm7ptDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for Derm7pt with images + metadata\n",
    "    \"\"\"\n",
    "    def __init__(self, img_paths, metadata, labels, transform=None):\n",
    "        self.img_paths = img_paths\n",
    "        self.metadata = metadata\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img_path = self.img_paths[idx]\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            # Fallback to black image if loading fails\n",
    "            print(f\"Warning: Failed to load {img_path}, using placeholder\")\n",
    "            image = Image.new(\"RGB\", (224, 224), color=\"black\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Get metadata and label\n",
    "        metadata = self.metadata[idx].astype(np.float32)\n",
    "        label = int(self.labels[idx])\n",
    "        \n",
    "        return image, metadata, label\n",
    "\n",
    "# =========================================================\n",
    "# DATA TRANSFORMS\n",
    "# =========================================================\n",
    "# Training transforms (with augmentation)\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Validation/Test transforms (no augmentation)\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# =========================================================\n",
    "# CREATE DATASETS\n",
    "# =========================================================\n",
    "train_dataset = Derm7ptDataset(X_train_img, X_train_meta, y_train, transform=train_transform)\n",
    "val_dataset   = Derm7ptDataset(X_val_img, X_val_meta, y_val, transform=val_test_transform)\n",
    "test_dataset  = Derm7ptDataset(X_test_img, X_test_meta, y_test, transform=val_test_transform)\n",
    "\n",
    "print(f\"\\n‚úÖ Created PyTorch Datasets\")\n",
    "print(f\"   - Train: {len(train_dataset)} samples\")\n",
    "print(f\"   - Val:   {len(val_dataset)} samples\")\n",
    "print(f\"   - Test:  {len(test_dataset)} samples\")\n",
    "\n",
    "# =========================================================\n",
    "# CREATE DATALOADERS (EXAMPLE - ADJUST BATCH SIZE AS NEEDED)\n",
    "# =========================================================\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0,  # Set to 0 for Windows, increase for Linux/Mac\n",
    "    pin_memory=True)\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Created DataLoaders (batch_size={BATCH_SIZE})\")\n",
    "print(f\"   - Train batches: {len(train_loader)}\")\n",
    "print(f\"   - Val batches:   {len(val_loader)}\")\n",
    "print(f\"   - Test batches:  {len(test_loader)}\")\n",
    "\n",
    "# =========================================================\n",
    "# EXAMPLE: TEST LOADING A BATCH\n",
    "# =========================================================\n",
    "print(\"\\nüîç Testing batch loading...\")\n",
    "for images, metadata, labels in train_loader:\n",
    "    print(f\"   - Image batch shape:    {images.shape}\")\n",
    "    print(f\"   - Metadata batch shape: {metadata.shape}\")\n",
    "    print(f\"   - Labels batch shape:   {labels.shape}\")\n",
    "    break\n",
    "\n",
    "print(\"\\n‚úÖ Derm7pt data loading complete! Ready for training.\")\n",
    "print(\"\\nüí° Usage in your model:\")\n",
    "print(\"   for images, metadata, labels in train_loader:\")\n",
    "print(\"       # images: torch.Tensor of shape (batch_size, 3, 224, 224)\")\n",
    "print(\"       # metadata: torch.Tensor of shape (batch_size, num_metadata_features)\")\n",
    "print(\"       # labels: torch.Tensor of shape (batch_size,)\")\n",
    "print(\"       # Your training code here...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c47527",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim_meta = X_train_meta.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2e2a7c-fba7-41aa-904f-6f2e62038435",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T05:23:18.759922Z",
     "iopub.status.busy": "2025-11-17T05:23:18.759735Z",
     "iopub.status.idle": "2025-11-17T05:23:18.782894Z",
     "shell.execute_reply": "2025-11-17T05:23:18.782116Z",
     "shell.execute_reply.started": "2025-11-17T05:23:18.759908Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt'):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_accuracy_max = -np.Inf\n",
    "        \n",
    "    def __call__(self, val_acc, model):\n",
    "        score = val_acc\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_acc, model)\n",
    "        elif score <= self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_acc, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_acc, model):\n",
    "        if self.verbose:\n",
    "            print(f'Validation accuracy increased ({self.val_accuracy_max:.6f} --> {val_acc:.6f}). Saving model...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_accuracy_max = val_acc\n",
    "\n",
    "from PIL import Image\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "def test(model, loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, metas, labels in loader:\n",
    "            imgs, metas, labels = imgs.to(device), metas.to(device), labels.to(device)\n",
    "            outputs = model(imgs, metas)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    return all_labels, all_preds\n",
    "\n",
    "def train(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc='Training')\n",
    "    for images, meta, labels in pbar:\n",
    "        images, meta, labels = images.to(device), meta.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images, meta)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Optional: Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{running_loss/total:.4f}',\n",
    "            'acc': f'{100.*correct/total:.2f}%'\n",
    "        })\n",
    "    \n",
    "    return running_loss/len(train_loader), correct/total\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, meta, labels in val_loader:\n",
    "            images, meta, labels = images.to(device), meta.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images, meta)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    return running_loss/len(val_loader), correct/total\n",
    "\n",
    "def train_model_with_scheduler_and_checkpoint(\n",
    "    model, train_loader, val_loader, optimizer, criterion, device, \n",
    "    epochs=20, patience=5, scheduler_patience=5, checkpoint_dir='checkpoints'):\n",
    "    \n",
    "    # Create checkpoint directory if it doesn't exist\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, 'best_model.pt')\n",
    "    \n",
    "    early_stopping = EarlyStopping(\n",
    "        patience=patience, \n",
    "        verbose=True, \n",
    "        path=checkpoint_path\n",
    "    )\n",
    "    scheduler = ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='max',  # Changed to max since we're monitoring accuracy\n",
    "        patience=scheduler_patience, \n",
    "        verbose=True,\n",
    "        factor=0.1,\n",
    "        min_lr=1e-6\n",
    "    )\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [], 'val_loss': [],\n",
    "        'train_acc': [], 'val_acc': [],\n",
    "        'lr': []\n",
    "    }\n",
    "    \n",
    "    best_model_epoch = None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f'\\nEpoch {epoch+1}/{epochs}')\n",
    "        \n",
    "        # Training phase\n",
    "        train_loss, train_acc = train(model, train_loader, optimizer, criterion, device)\n",
    "        \n",
    "        # Validation phase\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        # Update history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['lr'].append(optimizer.param_groups[0]['lr'])\n",
    "        \n",
    "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
    "        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "        \n",
    "        # Update scheduler based on validation accuracy\n",
    "        scheduler.step(val_acc)\n",
    "        \n",
    "        # Early stopping check\n",
    "        early_stopping(val_acc, model)\n",
    "        if val_acc > early_stopping.val_accuracy_max:\n",
    "            best_model_epoch = epoch + 1\n",
    "            \n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load(checkpoint_path))\n",
    "    \n",
    "    # Plot training curves\n",
    "    plot_training_curves_with_checkpoint(history, best_model_epoch)\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "def plot_training_curves_with_checkpoint(history, best_model_epoch):\n",
    "    epochs_range = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Loss curves\n",
    "    ax1.plot(epochs_range, history['train_loss'], label='Training Loss')\n",
    "    ax1.plot(epochs_range, history['val_loss'], label='Validation Loss')\n",
    "    if best_model_epoch:\n",
    "        ax1.axvline(best_model_epoch, color='r', linestyle='--', label='Best Model')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Training and Validation Loss')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Accuracy curves\n",
    "    ax2.plot(epochs_range, history['train_acc'], label='Training Accuracy')\n",
    "    ax2.plot(epochs_range, history['val_acc'], label='Validation Accuracy')\n",
    "    if best_model_epoch:\n",
    "        ax2.axvline(best_model_epoch, color='r', linestyle='--', label='Best Model')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.set_title('Training and Validation Accuracy')\n",
    "    ax2.legend()\n",
    "    \n",
    "    # Learning rate curve\n",
    "    ax3.plot(epochs_range, history['lr'], label='Learning Rate')\n",
    "    if best_model_epoch:\n",
    "        ax3.axvline(best_model_epoch, color='r', linestyle='--', label='Best Model')\n",
    "    ax3.set_xlabel('Epoch')\n",
    "    ax3.set_ylabel('Learning Rate')\n",
    "    ax3.set_title('Learning Rate Schedule')\n",
    "    ax3.set_yscale('log')\n",
    "    ax3.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5ddf23-3575-4c08-bfaf-73158d8caa83",
   "metadata": {},
   "source": [
    "<h1>MobileViT</h>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be60655-13a0-4cf0-91ef-73a62c6062b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T05:23:18.783943Z",
     "iopub.status.busy": "2025-11-17T05:23:18.783685Z",
     "iopub.status.idle": "2025-11-17T05:23:31.241499Z",
     "shell.execute_reply": "2025-11-17T05:23:31.240671Z",
     "shell.execute_reply.started": "2025-11-17T05:23:18.783925Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm  \n",
    "import torch.nn.functional as F\n",
    "\n",
    "class EarlyFusionModel(nn.Module):\n",
    "    def __init__(self, input_dim_meta, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embed metadata to smaller spatial dimensions first\n",
    "        self.meta_embed = nn.Sequential(\n",
    "            nn.Linear(input_dim_meta, 64 * 64),  # Updated for mobilevit's smaller receptive field\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64 * 64),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        # Load MobileViT model\n",
    "        self.mobilevit = timm.create_model(\"mobilevit_s.cvnets_in1k\", pretrained=True, num_classes=num_classes)\n",
    "        \n",
    "        # Inspect the model to identify the first conv layer\n",
    "        # Modify the first conv layer to accept additional channel\n",
    "        first_conv = self.mobilevit.stem.conv  # `stem.conv` is the correct initial layer\n",
    "        self.mobilevit.stem.conv = nn.Conv2d(4, first_conv.out_channels, \n",
    "                                             kernel_size=first_conv.kernel_size, \n",
    "                                             stride=first_conv.stride, \n",
    "                                             padding=first_conv.padding, \n",
    "                                             bias=first_conv.bias)\n",
    "        \n",
    "        # Initialize new channel weights\n",
    "        with torch.no_grad():\n",
    "            self.mobilevit.stem.conv.weight.data[:, :3] = first_conv.weight.data\n",
    "            # Initialize the new channel with smaller weights to prevent dominating\n",
    "            self.mobilevit.stem.conv.weight.data[:, 3:] = first_conv.weight.data.mean(dim=1, keepdim=True) * 0.1\n",
    "\n",
    "    def forward(self, img, meta):\n",
    "        # Reshape metadata to image-like format\n",
    "        batch_size = img.shape[0]\n",
    "        meta_reshaped = self.meta_embed(meta).view(batch_size, 1, 64, 64)\n",
    "        \n",
    "        # Upsample to match image dimensions\n",
    "        meta_upsampled = F.interpolate(meta_reshaped, \n",
    "                                       size=(224, 224),  # MobileViT expects 256x256\n",
    "                                       mode='bilinear', \n",
    "                                       align_corners=False)\n",
    "        \n",
    "        # Early fusion\n",
    "        combined_input = torch.cat([img, meta_upsampled], dim=1)\n",
    "        \n",
    "        # Process through modified MobileViT\n",
    "        out = self.mobilevit(combined_input)\n",
    "        return out\n",
    "\n",
    "# Assuming X_train_meta and other variables are defined\n",
    "input_dim_meta = X_train_meta.shape[1]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = EarlyFusionModel(input_dim_meta, num_classes).to(device)\n",
    "\n",
    "from torchinfo import summary\n",
    "summary(model=model, \n",
    "        input_size=[(16, 3, 224, 224), (16, input_dim_meta)],  # Updated for MobileViT input size\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    ")\n",
    "\n",
    "mobilevit_model = EarlyFusionModel(input_dim_meta=input_dim_meta, num_classes=num_classes).to(device)\n",
    "mobilevit_model.load_state_dict(torch.load('D:\\\\Dermp7\\\\best_early_fusion_mobilevitsmoteDA.pth'))\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "mobilevit_model.to(device)\n",
    "\n",
    "true_labels, pred_labels = test(mobilevit_model, test_loader, device)\n",
    "\n",
    "report = classification_report(true_labels, pred_labels, digits=4,target_names=class_names)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "cm = confusion_matrix(true_labels, pred_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c223cb9a-4391-44cf-911d-07084c698624",
   "metadata": {},
   "source": [
    "<h1>PvtV2</h>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43becc93-75ae-470e-ab2c-3eca7a877442",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T05:23:31.243620Z",
     "iopub.status.busy": "2025-11-17T05:23:31.243290Z",
     "iopub.status.idle": "2025-11-17T05:23:41.590636Z",
     "shell.execute_reply": "2025-11-17T05:23:41.589842Z",
     "shell.execute_reply.started": "2025-11-17T05:23:31.243599Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class EarlyFusionModel(nn.Module):\n",
    "    def __init__(self, input_dim_meta, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embed metadata to smaller spatial dimensions first\n",
    "        self.meta_embed = nn.Sequential(\n",
    "            nn.Linear(input_dim_meta, 56 * 56),  # Smaller initial dimension\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(56 * 56),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        # Load PVT v2 model\n",
    "        self.pvt = timm.create_model(\"pvt_v2_b1\", pretrained=True, num_classes=num_classes)\n",
    "        \n",
    "        # Modify the first convolution layer to accept additional channel (4 instead of 3)\n",
    "        first_conv = self.pvt.patch_embed.proj\n",
    "        self.pvt.patch_embed.proj = nn.Conv2d(4, first_conv.out_channels, \n",
    "                                              kernel_size=first_conv.kernel_size,\n",
    "                                              stride=first_conv.stride,\n",
    "                                              padding=first_conv.padding,\n",
    "                                              bias=first_conv.bias is not None)\n",
    "        \n",
    "        # Initialize new channel weights\n",
    "        with torch.no_grad():\n",
    "            self.pvt.patch_embed.proj.weight.data[:, :3] = first_conv.weight.data\n",
    "            self.pvt.patch_embed.proj.weight.data[:, 3:] = first_conv.weight.data.mean(dim=1, keepdim=True) * 0.1\n",
    "\n",
    "    def forward(self, img, meta):\n",
    "        # Reshape metadata to image-like format\n",
    "        batch_size = img.shape[0]\n",
    "        meta_reshaped = self.meta_embed(meta).view(batch_size, 1, 56, 56)\n",
    "        \n",
    "        # Upsample to match image dimensions\n",
    "        meta_upsampled = F.interpolate(meta_reshaped, \n",
    "                                       size=(224, 224), \n",
    "                                       mode='bilinear', \n",
    "                                       align_corners=False)\n",
    "        \n",
    "        # Early fusion\n",
    "        combined_input = torch.cat([img, meta_upsampled], dim=1)\n",
    "        \n",
    "        # Process through modified PVT\n",
    "        out = self.pvt(combined_input)\n",
    "        return out\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = EarlyFusionModel(input_dim_meta, num_classes).to(device)\n",
    "\n",
    "from torchinfo import summary\n",
    "summary(model=model, \n",
    "        input_size=[(16, 3, 224, 224), (16, input_dim_meta)],  \n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    ")\n",
    "\n",
    "pv2_model = EarlyFusionModel(input_dim_meta=input_dim_meta, num_classes=num_classes).to(device)\n",
    "pv2_model.load_state_dict(torch.load('D:\\\\Dermp7\\\\best_early_fusion_pvtv2smoteDA.pth'))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pv2_model.to(device)\n",
    "\n",
    "true_labels, pred_labels = test(pv2_model, test_loader, device)\n",
    "\n",
    "report = classification_report(true_labels, pred_labels, digits=4,target_names=class_names)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "cm = confusion_matrix(true_labels, pred_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49c7618-8f12-4d5c-9368-2bcd95b49a89",
   "metadata": {},
   "source": [
    "<h1>Teacher Model (Mean Averaging)</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a95b15c-e8a5-4669-859e-81bd46f30aca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T05:23:41.591689Z",
     "iopub.status.busy": "2025-11-17T05:23:41.591423Z",
     "iopub.status.idle": "2025-11-17T05:23:51.644728Z",
     "shell.execute_reply": "2025-11-17T05:23:51.643840Z",
     "shell.execute_reply.started": "2025-11-17T05:23:41.591658Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "class TeacherModel(nn.Module):\n",
    "    def __init__(self, models, ensemble_method=\"mean\"):\n",
    "        \"\"\"\n",
    "        Teacher Model using Ensemble Learning.\n",
    "\n",
    "        Args:\n",
    "            models (list): List of trained models to use for ensembling.\n",
    "            ensemble_method (str): \"mean\" for averaging logits, \"vote\" for majority voting.\n",
    "        \"\"\"\n",
    "        super(TeacherModel, self).__init__()\n",
    "        self.models = models\n",
    "        self.ensemble_method = ensemble_method\n",
    "\n",
    "        # Ensure all models are in eval mode and no gradients are computed\n",
    "        for model in self.models:\n",
    "            model.eval()\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, img, meta):\n",
    "        \"\"\"\n",
    "        Forward pass through the ensemble teacher model.\n",
    "\n",
    "        Args:\n",
    "            img (torch.Tensor): Batch of images.\n",
    "            meta (torch.Tensor): Batch of metadata.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The ensembled output (soft probabilities).\n",
    "        \"\"\"\n",
    "        model_outputs = []\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient computation for teacher\n",
    "            for model in self.models:\n",
    "                outputs = model(img, meta)\n",
    "                model_outputs.append(outputs)\n",
    "\n",
    "        # Convert list to tensor shape [num_models, batch_size, num_classes]\n",
    "        model_outputs = torch.stack(model_outputs, dim=0)\n",
    "\n",
    "        if self.ensemble_method == \"mean\":\n",
    "            # Soft-label generation: Averaging logits\n",
    "            avg_outputs = model_outputs.mean(dim=0)  \n",
    "        elif self.ensemble_method == \"vote\":\n",
    "            # Majority voting: Get the most common prediction\n",
    "            _, predictions = torch.max(model_outputs, dim=2) \n",
    "            avg_outputs = predictions.mode(dim=0).values  \n",
    "\n",
    "        return avg_outputs \n",
    "\n",
    "teacher_model = TeacherModel(models=[mobilevit_model, pv2_model], ensemble_method=\"mean\")\n",
    "\n",
    "# Move to the correct device (CPU/GPU)\n",
    "teacher_model = teacher_model.to(device)\n",
    "\n",
    "true_labels, pred_labels = test(teacher_model, test_loader, device)\n",
    "\n",
    "report = classification_report(true_labels, pred_labels, digits=4,target_names=class_names)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "cm = confusion_matrix(true_labels, pred_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252176ef-94fb-4d22-b869-1e39d2f4cbf2",
   "metadata": {},
   "source": [
    "<h1>Knowledge Distillation on Student Model</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e5bf25-69d0-4003-ba73-e17a09b30154",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T05:23:55.148615Z",
     "iopub.status.busy": "2025-11-17T05:23:55.147908Z",
     "iopub.status.idle": "2025-11-17T05:23:56.952590Z",
     "shell.execute_reply": "2025-11-17T05:23:56.951559Z",
     "shell.execute_reply.started": "2025-11-17T05:23:55.148585Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_cluster import knn_graph\n",
    "from torchinfo import summary\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Early Fusion with Dynamic GCN\n",
    "# -----------------------------\n",
    "class EarlyFusionWithDynamicGCN(nn.Module):\n",
    "    def __init__(self, input_dim_meta, num_classes, backbone=\"xxs\", k=8):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "\n",
    "        # --------- GCN branch ----------\n",
    "        self.gcn1 = GCNConv(input_dim_meta, 64)\n",
    "        self.gcn2 = GCNConv(64, 32)\n",
    "        self.res_proj = nn.Linear(64, 32)\n",
    "\n",
    "        # Metadata ‚Üí pseudo-image\n",
    "        self.meta_to_image = nn.Sequential(\n",
    "            nn.Linear(32, 56*56),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(56*56),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "        # --------- MobileViT backbone ----------\n",
    "        if backbone == \"xxs\":\n",
    "            model_name = \"mobilevit_xxs.cvnets_in1k\"\n",
    "            self.out_channels = 320\n",
    "        elif backbone == \"s\":\n",
    "            model_name = \"mobilevit_s.cvnets_in1k\"\n",
    "            self.out_channels = 640\n",
    "        else:\n",
    "            raise ValueError(\"backbone must be 'xxs' or 's'\")\n",
    "\n",
    "        self.mobilevit = timm.create_model(\n",
    "            model_name,\n",
    "            pretrained=True,\n",
    "            num_classes=0,\n",
    "            global_pool=''   # keep features, no built-in pooling\n",
    "        )\n",
    "\n",
    "        # Modify stem for 4-channel input\n",
    "        stem_conv = self.mobilevit.stem.conv\n",
    "        new_conv = nn.Conv2d(\n",
    "            4, stem_conv.out_channels,\n",
    "            kernel_size=stem_conv.kernel_size,\n",
    "            stride=stem_conv.stride,\n",
    "            padding=stem_conv.padding,\n",
    "            bias=stem_conv.bias is not None\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # copy RGB weights\n",
    "            new_conv.weight[:, :3] = stem_conv.weight\n",
    "            # tiny weight for metadata channel\n",
    "            new_conv.weight[:, 3:] = stem_conv.weight.mean(dim=1, keepdim=True) * 0.1\n",
    "            # copy bias if exists\n",
    "            if stem_conv.bias is not None:\n",
    "                new_conv.bias = stem_conv.bias.clone()\n",
    "\n",
    "        self.mobilevit.stem.conv = new_conv\n",
    "\n",
    "        # --------- Classifier ----------\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.out_channels, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, img, meta, batch_idx):\n",
    "        B = meta.size(0)\n",
    "\n",
    "        # --------- Graph construction (NOW IDENTICAL TO MODEL B) ----------\n",
    "        # dynamic kNN graph WITHOUT self-loops, batch-aware\n",
    "        edge_index = knn_graph(meta, k=self.k, batch=batch_idx)\n",
    "        # (torch_cluster.knn_graph by default excludes self-loops)\n",
    "\n",
    "        # ----- GCN with residual -----\n",
    "        x1 = F.relu(self.gcn1(meta, edge_index))\n",
    "        x2 = F.relu(self.gcn2(x1, edge_index) + self.res_proj(x1))\n",
    "        x_meta = x2  # [B, 32]\n",
    "\n",
    "        # ----- metadata ‚Üí pseudo-image -----\n",
    "        meta_img = self.meta_to_image(x_meta).view(B, 1, 56, 56)\n",
    "        meta_img = F.interpolate(meta_img, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "\n",
    "        # ----- early fusion -----\n",
    "        x = torch.cat([img, meta_img], dim=1)  # [B, 4, 224, 224]\n",
    "\n",
    "        # ----- MobileViT forward -----\n",
    "        features = self.mobilevit(x)          # [B, C, H, W] (no global_pool)\n",
    "        features = self.pool(features).view(B, -1)\n",
    "\n",
    "        return self.classifier(features)\n",
    "\n",
    "\n",
    "input_dim_meta = X_train_meta.shape[1]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size=16\n",
    "model = EarlyFusionWithDynamicGCN(input_dim_meta, num_classes, backbone=\"xxs\").to(device)\n",
    "\n",
    "dummy_img = torch.randn(batch_size, 3, 224, 224).to(device)\n",
    "dummy_meta = torch.randn(batch_size, input_dim_meta).to(device)\n",
    "dummy_batch_idx = torch.arange(batch_size, device=device)  # one node per graph\n",
    "\n",
    "summary(\n",
    "        model,\n",
    "        input_data=[dummy_img, dummy_meta, dummy_batch_idx],\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        depth=3\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79007f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xxs = EarlyFusionMobileViTXXS(input_dim_meta, num_classes).to(device)\n",
    "model_s = EarlyFusionWithDynamicGCN(input_dim_meta, num_classes, backbone=\"s\").to(device)\n",
    "\n",
    "# model_proposed = ...   # Only include if defined\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "def count_mobilevit_blocks_and_layers(backbone):\n",
    "    print(\"\\n===== Counting MobileViT Blocks and Transformer Layers (cvnets MobileViT) =====\")\n",
    "\n",
    "    # 1) Find all MobileVitBlock modules\n",
    "    blocks = []\n",
    "    for name, module in backbone.named_modules():\n",
    "        if \"mobilevitblock\" in module.__class__.__name__.lower():\n",
    "            blocks.append((name, module))\n",
    "\n",
    "    print(f\"\\nFound {len(blocks)} MobileViT Blocks:\")\n",
    "    for i, (name, module) in enumerate(blocks):\n",
    "        print(f\"  Block #{i}: {name} --> {module.__class__.__name__}\")\n",
    "\n",
    "    # 2) Inspect transformer depth inside MobileVitBlocks\n",
    "    total_layers = 0\n",
    "    print(\"\\nPer-block transformer depths:\")\n",
    "    for name, block in blocks:\n",
    "        depth = 0\n",
    "\n",
    "        if hasattr(block, \"transformer\"):\n",
    "            tr = block.transformer\n",
    "\n",
    "            # CVNets MobileViT uses `tr.layers` (ModuleList)\n",
    "            if hasattr(tr, \"layers\") and isinstance(tr.layers, nn.ModuleList):\n",
    "                depth = len(tr.layers)\n",
    "\n",
    "            # Another version uses `tr.blocks`\n",
    "            elif hasattr(tr, \"blocks\") and isinstance(tr.blocks, nn.ModuleList):\n",
    "                depth = len(tr.blocks)\n",
    "\n",
    "            # Fallback: count submodules inside transformer manually\n",
    "            else:\n",
    "                depth = sum(1 for _ in tr.children())\n",
    "\n",
    "        total_layers += depth\n",
    "        print(f\"  - {name}: {depth} transformer layers\")\n",
    "\n",
    "    print(f\"\\nTotal transformer layers across all MobileViT blocks: {total_layers}\")\n",
    "    return blocks, total_layers\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# RUN FOR XXS\n",
    "# -------------------------\n",
    "blocks_xxs, total_layers_xxs = count_mobilevit_blocks_and_layers(model_xxs.backbone)\n",
    "print(\"\\nXXS Total MobileViT Blocks:\", len(blocks_xxs))\n",
    "print(\"XXS Total Transformer Layers:\", total_layers_xxs)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# RUN FOR S\n",
    "# -------------------------\n",
    "blocks_s, total_layers_s = count_mobilevit_blocks_and_layers(model_s.mobilevit)\n",
    "print(\"\\nS Total MobileViT Blocks:\", len(blocks_s))\n",
    "print(\"S Total Transformer Layers:\", total_layers_s)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# RUN FOR PROPOSED (optional)\n",
    "# -------------------------\n",
    "blocks_p, total_layers_p = count_mobilevit_blocks_and_layers(model_proposed.mobilevit)\n",
    "print(\"\\nProposed Total MobileViT Blocks:\", len(blocks_p))\n",
    "print(\"Proposed Total Transformer Layers:\", total_layers_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17147dbc-c29c-491e-a452-955d9e5f1c6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T05:23:56.954172Z",
     "iopub.status.busy": "2025-11-17T05:23:56.953646Z",
     "iopub.status.idle": "2025-11-17T07:46:16.480152Z",
     "shell.execute_reply": "2025-11-17T07:46:16.479262Z",
     "shell.execute_reply.started": "2025-11-17T05:23:56.954150Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Teacher Model (Assumed Already Defined and Loaded)\n",
    "# =========================================================\n",
    "teacher_model = TeacherModel(models=[mobilevit_model, pv2_model], ensemble_method=\"mean\").to(device)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Utility Functions\n",
    "# =========================================================\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "def evaluate_student(model, test_loader, device):\n",
    "    model.eval()\n",
    "    all_labels, all_preds = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, metas, labels in test_loader:\n",
    "            images, metas, labels = images.to(device), metas.to(device), labels.to(device)\n",
    "            batch_indices = torch.arange(metas.size(0), device=device, dtype=torch.long)\n",
    "\n",
    "            outputs = model(images, metas, batch_indices)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_preds = np.array(all_preds)\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "\n",
    "    print(f\"Test Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"Test F1 Score:  {f1:.4f}\")\n",
    "    print(f\"Test Precision: {precision:.4f}\")\n",
    "    print(f\"Test Recall:    {recall:.4f}\")\n",
    "\n",
    "    return accuracy, f1, precision, recall\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Averaged Training Curve Plotting (handles early stopping)\n",
    "# =========================================================\n",
    "def plot_average_training_curves(all_histories, save_path=\"averaged_training_curves.png\"):\n",
    "    # Convert list-of-lists (one per seed) ‚Üí stacked arrays with same length\n",
    "    train_loss_runs = [np.array(run) for run in all_histories[\"train_loss\"]]\n",
    "    val_loss_runs   = [np.array(run) for run in all_histories[\"val_loss\"]]\n",
    "    train_acc_runs  = [np.array(run) for run in all_histories[\"train_acc\"]]\n",
    "    val_acc_runs    = [np.array(run) for run in all_histories[\"val_acc\"]]\n",
    "\n",
    "    # Different seeds may stop earlier due to early stopping ‚Üí align to min length\n",
    "    min_len = min(len(x) for x in train_loss_runs)\n",
    "\n",
    "    train_loss = np.stack([x[:min_len] for x in train_loss_runs])\n",
    "    val_loss   = np.stack([x[:min_len] for x in val_loss_runs])\n",
    "    train_acc  = np.stack([x[:min_len] for x in train_acc_runs])\n",
    "    val_acc    = np.stack([x[:min_len] for x in val_acc_runs])\n",
    "\n",
    "    epochs = np.arange(1, min_len + 1)\n",
    "\n",
    "    # ---- Set global bold font ----\n",
    "    plt.rcParams['font.weight'] = 'bold'\n",
    "    plt.rcParams['axes.labelweight'] = 'bold'\n",
    "    plt.rcParams['axes.titleweight'] = 'bold'\n",
    "    plt.rcParams['xtick.labelsize'] = 12\n",
    "    plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # -------- Loss subplot --------\n",
    "    plt.subplot(1, 2, 1)\n",
    "    tl_mean, tl_std = train_loss.mean(axis=0), train_loss.std(axis=0)\n",
    "    vl_mean, vl_std = val_loss.mean(axis=0),   val_loss.std(axis=0)\n",
    "\n",
    "    plt.plot(epochs, tl_mean, label=\"Train Loss\")\n",
    "    plt.fill_between(epochs, tl_mean - tl_std, tl_mean + tl_std, alpha=0.25)\n",
    "\n",
    "    plt.plot(epochs, vl_mean, label=\"Validation Loss\")\n",
    "    plt.fill_between(epochs, vl_mean - vl_std, vl_mean + vl_std, alpha=0.25)\n",
    "\n",
    "    plt.xlabel(\"Epochs\", fontweight=\"bold\")\n",
    "    plt.ylabel(\"Loss\", fontweight=\"bold\")\n",
    "    plt.title(\"Training and Validation Loss (Averaged Across Runs)\", fontweight=\"bold\")\n",
    "    plt.legend()\n",
    "\n",
    "    # -------- Accuracy subplot --------\n",
    "    plt.subplot(1, 2, 2)\n",
    "    ta_mean, ta_std = train_acc.mean(axis=0), train_acc.std(axis=0)\n",
    "    va_mean, va_std = val_acc.mean(axis=0),   val_acc.std(axis=0)\n",
    "\n",
    "    plt.plot(epochs, ta_mean, label=\"Train Accuracy\")\n",
    "    plt.fill_between(epochs, ta_mean - ta_std, ta_mean + ta_std, alpha=0.25)\n",
    "\n",
    "    plt.plot(epochs, va_mean, label=\"Validation Accuracy\")\n",
    "    plt.fill_between(epochs, va_mean - va_std, va_mean + va_std, alpha=0.25)\n",
    "\n",
    "    plt.xlabel(\"Epochs\", fontweight=\"bold\")\n",
    "    plt.ylabel(\"Accuracy\", fontweight=\"bold\")\n",
    "    plt.title(\"Training and Validation Accuracy (Averaged Across Runs)\", fontweight=\"bold\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=650, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Training with Knowledge Distillation\n",
    "# =========================================================\n",
    "def train_student_model_kd(student_model, teacher_model,\n",
    "                           train_loader, val_loader, test_loader,\n",
    "                           device, alpha=0.5, temperature=3.0,\n",
    "                           epochs=100, patience=10):\n",
    "\n",
    "    student_model.to(device)\n",
    "    teacher_model.eval()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    kl_div_loss = nn.KLDivLoss(reduction='batchmean')\n",
    "    optimizer = torch.optim.Adam(student_model.parameters(), lr=0.001)\n",
    "\n",
    "    # You early-stop on val_accuracy ‚Üí use mode='max' & step on val_accuracy\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='max', patience=5, verbose=True)\n",
    "\n",
    "    best_val_accuracy = 0.0\n",
    "    best_val_model_state = None\n",
    "    patience_counter = 0\n",
    "\n",
    "    history = {\"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": []}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        student_model.train()\n",
    "        train_loss_sum, correct, total = 0.0, 0, 0\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "        for images, metas, labels in pbar:\n",
    "            images, metas, labels = images.to(device), metas.to(device), labels.to(device)\n",
    "            batch_indices = torch.arange(metas.size(0), device=device, dtype=torch.long)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            student_outputs = student_model(images, metas, batch_indices)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = teacher_model(images, metas)\n",
    "\n",
    "            # --- Hard loss ---\n",
    "            loss_hard = criterion(student_outputs, labels)\n",
    "\n",
    "            # --- Soft loss (KD) ---\n",
    "            loss_soft = kl_div_loss(\n",
    "                F.log_softmax(student_outputs / temperature, dim=1),\n",
    "                F.softmax(teacher_outputs / temperature, dim=1)\n",
    "            )\n",
    "\n",
    "            loss = (1 - alpha) * loss_hard + alpha * (temperature ** 2) * loss_soft\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss_sum += loss.item()\n",
    "            _, predicted = student_outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        train_loss = train_loss_sum / len(train_loader)\n",
    "        train_accuracy = correct / total\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_acc\"].append(train_accuracy)\n",
    "\n",
    "        # ---- Validation ----\n",
    "        student_model.eval()\n",
    "        val_loss_sum, correct, total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, metas, labels in val_loader:\n",
    "                images, metas, labels = images.to(device), metas.to(device), labels.to(device)\n",
    "                batch_indices = torch.arange(metas.size(0), device=device, dtype=torch.long)\n",
    "\n",
    "                outputs = student_model(images, metas, batch_indices)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss_sum += loss.item()\n",
    "\n",
    "                _, predicted = outputs.max(1)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "        val_loss = val_loss_sum / len(val_loader)\n",
    "        val_accuracy = correct / total\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_acc\"].append(val_accuracy)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: \"\n",
    "              f\"Train Loss={train_loss:.4f}, Train Acc={train_accuracy:.4f} | \"\n",
    "              f\"Val Loss={val_loss:.4f}, Val Acc={val_accuracy:.4f}\")\n",
    "\n",
    "        # ---- Early Stopping on Val Accuracy ----\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            best_val_model_state = student_model.state_dict()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "        # LR schedule on validation accuracy (consistent with early stopping)\n",
    "        scheduler.step(val_accuracy)\n",
    "\n",
    "    return best_val_model_state, history, best_val_accuracy\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Main Experiment Loop (MULTI-SEED RUNS)\n",
    "# =========================================================\n",
    "seeds = [42, 123, 569]\n",
    "best_overall_model = None\n",
    "best_overall_accuracy = 0.0\n",
    "\n",
    "results = {\"accuracy\": [], \"f1\": [], \"precision\": [], \"recall\": []}\n",
    "all_histories = {\"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": []}\n",
    "\n",
    "for seed in seeds:\n",
    "    print(f\"\\n--- Training with Seed {seed} ---\")\n",
    "    set_seed(seed)\n",
    "\n",
    "    student_model = EarlyFusionWithDynamicGCN(input_dim_meta, num_classes, backbone=\"xxs\").to(device)\n",
    "\n",
    "    best_model_state, history, val_acc = train_student_model_kd(\n",
    "        student_model=student_model,\n",
    "        teacher_model=teacher_model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader,\n",
    "        device=device,\n",
    "        alpha=0.2,\n",
    "        temperature=9.0,\n",
    "        epochs=100,\n",
    "        patience=10\n",
    "    )\n",
    "\n",
    "    # Store history for averaged curves\n",
    "    all_histories[\"train_loss\"].append(history[\"train_loss\"])\n",
    "    all_histories[\"val_loss\"].append(history[\"val_loss\"])\n",
    "    all_histories[\"train_acc\"].append(history[\"train_acc\"])\n",
    "    all_histories[\"val_acc\"].append(history[\"val_acc\"])\n",
    "\n",
    "    # ---- Final Test ----\n",
    "    student_model.load_state_dict(best_model_state)\n",
    "    acc, f1, prec, recall = evaluate_student(student_model, test_loader, device)\n",
    "\n",
    "    results[\"accuracy\"].append(acc)\n",
    "    results[\"f1\"].append(f1)\n",
    "    results[\"precision\"].append(prec)\n",
    "    results[\"recall\"].append(recall)\n",
    "\n",
    "    if val_acc > best_overall_accuracy:\n",
    "        best_overall_accuracy = val_acc\n",
    "        best_overall_model = student_model\n",
    "\n",
    "# ---- Save Best Model ----\n",
    "torch.save(best_overall_model.state_dict(), \"Lightstudent.pth\")\n",
    "print(f\"\\nBest Val Accuracy Model Saved (Acc={best_overall_accuracy:.4f})\")\n",
    "\n",
    "# ---- Summary ----\n",
    "print(\"\\n--- Final Evaluation Across Seeds ---\")\n",
    "for metric in results:\n",
    "    print(f\"{metric.capitalize()}: {np.mean(results[metric]):.4f} ¬± {np.std(results[metric]):.4f}\")\n",
    "\n",
    "# ---- PLOT AVERAGED TRAINING CURVES ----\n",
    "plot_average_training_curves(all_histories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f57a542-252f-4450-88be-fca1c705e229",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T07:46:16.481365Z",
     "iopub.status.busy": "2025-11-17T07:46:16.481108Z",
     "iopub.status.idle": "2025-11-17T07:46:16.903381Z",
     "shell.execute_reply": "2025-11-17T07:46:16.902518Z",
     "shell.execute_reply.started": "2025-11-17T07:46:16.481333Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_average_training_curves(all_histories, save_path=\"averaged_training_curves.png\"):\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Convert lists of lists ‚Üí numpy arrays\n",
    "    train_loss = np.array([np.array(run) for run in all_histories[\"train_loss\"]], dtype=object)\n",
    "    val_loss   = np.array([np.array(run) for run in all_histories[\"val_loss\"]], dtype=object)\n",
    "    train_acc  = np.array([np.array(run) for run in all_histories[\"train_acc\"]], dtype=object)\n",
    "    val_acc    = np.array([np.array(run) for run in all_histories[\"val_acc\"]], dtype=object)\n",
    "\n",
    "    # ---- Ensure equal length by truncating to minimum epoch count ----\n",
    "    min_len = min([len(x) for x in train_loss])\n",
    "\n",
    "    train_loss = np.stack([run[:min_len] for run in train_loss])\n",
    "    val_loss   = np.stack([run[:min_len] for run in val_loss])\n",
    "    train_acc  = np.stack([run[:min_len] for run in train_acc])\n",
    "    val_acc    = np.stack([run[:min_len] for run in val_acc])\n",
    "\n",
    "    epochs = np.arange(1, min_len + 1)\n",
    "\n",
    "    # ---- Styling ----\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.rcParams.update({\n",
    "        \"font.weight\": \"bold\",\n",
    "        \"axes.labelweight\": \"bold\",\n",
    "        \"axes.titleweight\": \"bold\",\n",
    "        \"xtick.labelsize\": 12,\n",
    "        \"ytick.labelsize\": 12\n",
    "    })\n",
    "\n",
    "    # ---------------- LOSS PLOT ----------------\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_loss.mean(0), label=\"Train Loss\", color=\"blue\")\n",
    "    plt.fill_between(\n",
    "        epochs,\n",
    "        train_loss.mean(0) - train_loss.std(0),\n",
    "        train_loss.mean(0) + train_loss.std(0),\n",
    "        alpha=0.25,\n",
    "        color=\"blue\"\n",
    "    )\n",
    "\n",
    "    plt.plot(epochs, val_loss.mean(0), label=\"Val Loss\", color=\"red\")\n",
    "    plt.fill_between(\n",
    "        epochs,\n",
    "        val_loss.mean(0) - val_loss.std(0),\n",
    "        val_loss.mean(0) + val_loss.std(0),\n",
    "        alpha=0.25,\n",
    "        color=\"red\"\n",
    "    )\n",
    "\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training & Validation Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    # ---------------- ACCURACY PLOT ----------------\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_acc.mean(0), label=\"Train Accuracy\", color=\"green\")\n",
    "    plt.fill_between(\n",
    "        epochs,\n",
    "        train_acc.mean(0) - train_acc.std(0),\n",
    "        train_acc.mean(0) + train_acc.std(0),\n",
    "        alpha=0.25,\n",
    "        color=\"green\"\n",
    "    )\n",
    "\n",
    "    plt.plot(epochs, val_acc.mean(0), label=\"Val Accuracy\", color=\"orange\")\n",
    "    plt.fill_between(\n",
    "        epochs,\n",
    "        val_acc.mean(0) - val_acc.std(0),\n",
    "        val_acc.mean(0) + val_acc.std(0),\n",
    "        alpha=0.25,\n",
    "        color=\"orange\"\n",
    "    )\n",
    "\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Training & Validation Accuracy\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_average_training_curves(all_histories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a1e23d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T07:47:01.859911Z",
     "iopub.status.busy": "2025-11-17T07:47:01.859136Z",
     "iopub.status.idle": "2025-11-17T07:47:12.018591Z",
     "shell.execute_reply": "2025-11-17T07:47:12.017664Z",
     "shell.execute_reply.started": "2025-11-17T07:47:01.859884Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load the best model\n",
    "best_model = EarlyFusionWithDynamicGCN(input_dim_meta, num_classes).to(device)\n",
    "best_model.load_state_dict(torch.load(\"Lightstudent.pth\"))\n",
    "best_model.eval()\n",
    "\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "all_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, metas, labels in test_loader:\n",
    "        images, metas, labels = images.to(device), metas.to(device), labels.to(device)\n",
    "        batch_indices = torch.arange(metas.size(0)).to(device).long()\n",
    "        outputs = student_model(images, metas, batch_indices)        \n",
    "        probs = F.softmax(outputs, dim=1)\n",
    "        preds = probs.argmax(dim=1)\n",
    "\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "all_labels = np.array(all_labels)\n",
    "all_preds = np.array(all_preds)\n",
    "all_probs = np.array(all_probs)\n",
    "\n",
    "# Compute classification report\n",
    "class_report = classification_report(all_labels, all_preds, target_names=class_names, digits=4)\n",
    "\n",
    "# Compute normalized confusion matrix\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds, normalize=\"true\")\n",
    "\n",
    "# Display classification report\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(class_report)\n",
    "\n",
    "# Display confusion matrix (black and white)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, cmap=\"gray\", fmt=\".2f\", xticklabels=class_names, yticklabels=class_names, cbar=True)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Normalized Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Compute and plot ROC-AUC curve for each class\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for i, class_name in enumerate(class_names):\n",
    "    fpr, tpr, _ = roc_curve((all_labels == i).astype(int), all_probs[:, i])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label=f\"{class_name} (AUC = {roc_auc:.2f})\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], \"k--\")  # Diagonal line for reference\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC-AUC Curve\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Optional: Compute and plot Precision-Recall Curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for i, class_name in enumerate(class_names):\n",
    "    precision, recall, _ = precision_recall_curve((all_labels == i).astype(int), all_probs[:, i])\n",
    "    plt.plot(recall, precision, label=f\"{class_name}\")\n",
    "\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb48d320-169d-4af6-bb5b-2503eb65aa28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T07:58:56.971851Z",
     "iopub.status.busy": "2025-11-17T07:58:56.971529Z",
     "iopub.status.idle": "2025-11-17T07:59:04.944278Z",
     "shell.execute_reply": "2025-11-17T07:59:04.943510Z",
     "shell.execute_reply.started": "2025-11-17T07:58:56.971828Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import numpy as np\n",
    "from ptflops import get_model_complexity_info\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "image_size = (3, 224, 224)\n",
    "\n",
    "results = []\n",
    "\n",
    "def count_parameters(model, trainable_only=False):\n",
    "    if trainable_only:\n",
    "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    else:\n",
    "        return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "\n",
    "\n",
    "class FusionWrapper(nn.Module):\n",
    "    \"\"\"\n",
    "    Wraps 2-input (img, meta) models so ptflops sees a single image input.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, meta_dim):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.meta_dim = meta_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        dummy_meta = torch.randn(x.size(0), self.meta_dim).to(x.device)\n",
    "        return self.model(x, dummy_meta)\n",
    "\n",
    "\n",
    "class GCN2InputWrapper(nn.Module):\n",
    "    \"\"\"\n",
    "    Wraps EarlyFusionWithDynamicGCN so it behaves like forward(img, meta).\n",
    "    Batch indices are synthesized internally.\n",
    "    \"\"\"\n",
    "    def __init__(self, gcn_model, meta_dim):\n",
    "        super().__init__()\n",
    "        self.gcn_model = gcn_model\n",
    "        self.meta_dim = meta_dim\n",
    "\n",
    "    def forward(self, img, meta):\n",
    "        B = meta.size(0)\n",
    "        batch_idx = torch.arange(B, device=meta.device)\n",
    "        return self.gcn_model(img, meta, batch_idx)\n",
    "\n",
    "\n",
    "def compute_flops(model, meta_dim):\n",
    "    wrapper = FusionWrapper(model, meta_dim).to(device)\n",
    "    with torch.no_grad():\n",
    "        flops, _ = get_model_complexity_info(\n",
    "            wrapper,\n",
    "            image_size,\n",
    "            as_strings=False,\n",
    "            print_per_layer_stat=False,\n",
    "            verbose=False\n",
    "        )\n",
    "    return float(flops / 1e9)  # GFLOPs\n",
    "\n",
    "\n",
    "def measure_gpu_latency(model, meta_dim, runs=200, warmup=30):\n",
    "    if not torch.cuda.is_available():\n",
    "        return None, None, None\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    model.eval().to(device)\n",
    "\n",
    "    dummy_img = torch.randn(1, *image_size, device=device)\n",
    "    dummy_meta = torch.randn(1, meta_dim, device=device)\n",
    "\n",
    "    # warmup\n",
    "    for _ in range(warmup):\n",
    "        _ = model(dummy_img, dummy_meta)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    start_evt = torch.cuda.Event(enable_timing=True)\n",
    "    end_evt = torch.cuda.Event(enable_timing=True)\n",
    "    times = []\n",
    "\n",
    "    for _ in range(runs):\n",
    "        start_evt.record()\n",
    "        _ = model(dummy_img, dummy_meta)\n",
    "        end_evt.record()\n",
    "        torch.cuda.synchronize()\n",
    "        times.append(start_evt.elapsed_time(end_evt))  # ms\n",
    "\n",
    "    times = np.array(times)\n",
    "    mean = float(times.mean())\n",
    "    std = float(times.std())\n",
    "    fps = float(1000.0 / mean)\n",
    "    return mean, std, fps\n",
    "\n",
    "\n",
    "def measure_cpu_latency(model, meta_dim, runs=100, warmup=20):\n",
    "    model_cpu = model.cpu()\n",
    "    model_cpu.eval()\n",
    "\n",
    "    dummy_img = torch.randn(1, *image_size)\n",
    "    dummy_meta = torch.randn(1, meta_dim)\n",
    "\n",
    "    for _ in range(warmup):\n",
    "        _ = model_cpu(dummy_img, dummy_meta)\n",
    "\n",
    "    times = []\n",
    "    for _ in range(runs):\n",
    "        start = time.perf_counter()\n",
    "        _ = model_cpu(dummy_img, dummy_meta)\n",
    "        end = time.perf_counter()\n",
    "        times.append((end - start) * 1000.0)\n",
    "\n",
    "    times = np.array(times)\n",
    "    mean = float(times.mean())\n",
    "    std = float(times.std())\n",
    "    fps = float(1000.0 / mean)\n",
    "    return mean, std, fps\n",
    "\n",
    "\n",
    "def load_model(model_class, ckpt_path, name, wrap_gcn=False):\n",
    "    \"\"\"\n",
    "    Creates model_class(input_dim_meta=59, num_classes=6), loads checkpoint,\n",
    "    optionally wraps GCN so it behaves like (img, meta).\n",
    "    \"\"\"\n",
    "    print(f\"\\n[LOAD] {name} from {ckpt_path}\")\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    model = model_class(meta_dim, num_classes)\n",
    "    state = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    model.load_state_dict(state)\n",
    "    if wrap_gcn:\n",
    "        model = GCN2InputWrapper(model, meta_dim)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    print(\"[OK] Model loaded.\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def benchmark_model(model, name, meta_dim, count_all_params=False):\n",
    "    \"\"\"\n",
    "    Given a ready-to-use 2-input (img, meta) model, compute all stats.\n",
    "    If count_all_params=True, counts ALL parameters (ignoring requires_grad),\n",
    "    which is what we want for the teacher ensemble.\n",
    "    \"\"\"\n",
    "    print(f\"\\n======= Benchmarking: {name} =======\")\n",
    "\n",
    "    # trainable_only=False for teacher, True for others via this flag\n",
    "    trainable_only = not count_all_params\n",
    "    params_m = count_parameters(model, trainable_only=trainable_only) / 1e6\n",
    "    print(f\"Params: {params_m:.3f} M\")\n",
    "\n",
    "    flops_g = compute_flops(model, meta_dim)\n",
    "    print(f\"FLOPs: {flops_g:.3f} G\")\n",
    "\n",
    "    gpu_mean, gpu_std, gpu_fps = measure_gpu_latency(model, meta_dim)\n",
    "    if gpu_mean is not None:\n",
    "        print(f\"GPU Latency: {gpu_mean:.3f} ¬± {gpu_std:.3f} ms  |  FPS: {gpu_fps:.1f}\")\n",
    "    else:\n",
    "        print(\"GPU Latency: N/A\")\n",
    "\n",
    "    cpu_mean, cpu_std, cpu_fps = measure_cpu_latency(model, meta_dim)\n",
    "    print(f\"CPU Latency: {cpu_mean:.3f} ¬± {cpu_std:.3f} ms  |  FPS: {cpu_fps:.1f}\")\n",
    "\n",
    "    return {\n",
    "        \"model\": name,\n",
    "        \"params_M\": params_m,\n",
    "        \"flops_G\": flops_g,\n",
    "        \"gpu_latency_mean_ms\": gpu_mean,\n",
    "        \"gpu_latency_std_ms\": gpu_std,\n",
    "        \"gpu_fps\": gpu_fps,\n",
    "        \"cpu_latency_mean_ms\": cpu_mean,\n",
    "        \"cpu_latency_std_ms\": cpu_std,\n",
    "        \"cpu_fps\": cpu_fps,\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "student_raw = EarlyFusionWithDynamicGCN(input_dim_meta, num_classes=num_classes)\n",
    "student_state = torch.load(\"Lightstudent.pth\", map_location=\"cpu\")\n",
    "student_raw.load_state_dict(student_state)\n",
    "student_model = GCN2InputWrapper(student_raw, input_dim_meta).to(device).eval()\n",
    "results.append(benchmark_model(student_model, \"TabFusion (GCN Student)\", input_dim_meta))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6daaa52",
   "metadata": {},
   "source": [
    "<h1>Knowledge Distillation on Student Model with \"s\" Backbone</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55589735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# Main Experiment Loop (MULTI-SEED RUNS) - \"s\" BACKBONE\n",
    "# =========================================================\n",
    "seeds = [42, 123, 569]\n",
    "best_overall_model_s = None\n",
    "best_overall_accuracy_s = 0.0\n",
    "\n",
    "results_s = {\"accuracy\": [], \"f1\": [], \"precision\": [], \"recall\": []}\n",
    "all_histories_s = {\"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": []}\n",
    "\n",
    "for seed in seeds:\n",
    "    print(f\"\\n--- Training with Seed {seed} (s backbone) ---\")\n",
    "    set_seed(seed)\n",
    "\n",
    "    student_model_s = EarlyFusionWithDynamicGCN(input_dim_meta, num_classes, backbone=\"s\").to(device)\n",
    "\n",
    "    best_model_state, history, val_acc = train_student_model_kd(\n",
    "        student_model=student_model_s,\n",
    "        teacher_model=teacher_model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader,\n",
    "        device=device,\n",
    "        alpha=0.2,\n",
    "        temperature=9.0,\n",
    "        epochs=100,\n",
    "        patience=10\n",
    "    )\n",
    "\n",
    "    # Store history for averaged curves\n",
    "    all_histories_s[\"train_loss\"].append(history[\"train_loss\"])\n",
    "    all_histories_s[\"val_loss\"].append(history[\"val_loss\"])\n",
    "    all_histories_s[\"train_acc\"].append(history[\"train_acc\"])\n",
    "    all_histories_s[\"val_acc\"].append(history[\"val_acc\"])\n",
    "\n",
    "    # ---- Final Test ----\n",
    "    student_model_s.load_state_dict(best_model_state)\n",
    "    acc, f1, prec, recall = evaluate_student(student_model_s, test_loader, device)\n",
    "\n",
    "    results_s[\"accuracy\"].append(acc)\n",
    "    results_s[\"f1\"].append(f1)\n",
    "    results_s[\"precision\"].append(prec)\n",
    "    results_s[\"recall\"].append(recall)\n",
    "\n",
    "    if val_acc > best_overall_accuracy_s:\n",
    "        best_overall_accuracy_s = val_acc\n",
    "        best_overall_model_s = student_model_s\n",
    "\n",
    "# ---- Save Best Model ----\n",
    "torch.save(best_overall_model_s.state_dict(), \"Lightstudent_s.pth\")\n",
    "print(f\"\\nBest Val Accuracy Model Saved (Acc={best_overall_accuracy_s:.4f})\")\n",
    "\n",
    "# ---- Summary ----\n",
    "print(\"\\n--- Final Evaluation Across Seeds (s backbone) ---\")\n",
    "for metric in results_s:\n",
    "    print(f\"{metric.capitalize()}: {np.mean(results_s[metric]):.4f} ¬± {np.std(results_s[metric]):.4f}\")\n",
    "\n",
    "# ---- PLOT AVERAGED TRAINING CURVES ----\n",
    "plot_average_training_curves(all_histories_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b02673a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot averaged training curves for s backbone\n",
    "def plot_average_training_curves_s(all_histories, save_path=\"averaged_training_curves_s.png\"):\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Convert lists of lists ‚Üí numpy arrays\n",
    "    train_loss = np.array([np.array(run) for run in all_histories[\"train_loss\"]], dtype=object)\n",
    "    val_loss   = np.array([np.array(run) for run in all_histories[\"val_loss\"]], dtype=object)\n",
    "    train_acc  = np.array([np.array(run) for run in all_histories[\"train_acc\"]], dtype=object)\n",
    "    val_acc    = np.array([np.array(run) for run in all_histories[\"val_acc\"]], dtype=object)\n",
    "\n",
    "    # ---- Ensure equal length by truncating to minimum epoch count ----\n",
    "    min_len = min([len(x) for x in train_loss])\n",
    "\n",
    "    train_loss = np.stack([run[:min_len] for run in train_loss])\n",
    "    val_loss   = np.stack([run[:min_len] for run in val_loss])\n",
    "    train_acc  = np.stack([run[:min_len] for run in train_acc])\n",
    "    val_acc    = np.stack([run[:min_len] for run in val_acc])\n",
    "\n",
    "    epochs = np.arange(1, min_len + 1)\n",
    "\n",
    "    # ---- Styling ----\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.rcParams.update({\n",
    "        \"font.weight\": \"bold\",\n",
    "        \"axes.labelweight\": \"bold\",\n",
    "        \"axes.titleweight\": \"bold\",\n",
    "        \"xtick.labelsize\": 12,\n",
    "        \"ytick.labelsize\": 12\n",
    "    })\n",
    "\n",
    "    # ---------------- LOSS PLOT ----------------\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_loss.mean(0), label=\"Train Loss\", color=\"blue\")\n",
    "    plt.fill_between(\n",
    "        epochs,\n",
    "        train_loss.mean(0) - train_loss.std(0),\n",
    "        train_loss.mean(0) + train_loss.std(0),\n",
    "        alpha=0.25,\n",
    "        color=\"blue\"\n",
    "    )\n",
    "\n",
    "    plt.plot(epochs, val_loss.mean(0), label=\"Val Loss\", color=\"red\")\n",
    "    plt.fill_between(\n",
    "        epochs,\n",
    "        val_loss.mean(0) - val_loss.std(0),\n",
    "        val_loss.mean(0) + val_loss.std(0),\n",
    "        alpha=0.25,\n",
    "        color=\"red\"\n",
    "    )\n",
    "\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training & Validation Loss (s backbone)\")\n",
    "    plt.legend()\n",
    "\n",
    "    # ---------------- ACCURACY PLOT ----------------\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_acc.mean(0), label=\"Train Accuracy\", color=\"green\")\n",
    "    plt.fill_between(\n",
    "        epochs,\n",
    "        train_acc.mean(0) - train_acc.std(0),\n",
    "        train_acc.mean(0) + train_acc.std(0),\n",
    "        alpha=0.25,\n",
    "        color=\"green\"\n",
    "    )\n",
    "\n",
    "    plt.plot(epochs, val_acc.mean(0), label=\"Val Accuracy\", color=\"orange\")\n",
    "    plt.fill_between(\n",
    "        epochs,\n",
    "        val_acc.mean(0) - val_acc.std(0),\n",
    "        val_acc.mean(0) + val_acc.std(0),\n",
    "        alpha=0.25,\n",
    "        color=\"orange\"\n",
    "    )\n",
    "\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Training & Validation Accuracy (s backbone)\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=650, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "plot_average_training_curves_s(all_histories_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a771ac09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load the best model (s backbone)\n",
    "best_model_s = EarlyFusionWithDynamicGCN(input_dim_meta, num_classes, backbone=\"s\").to(device)\n",
    "best_model_s.load_state_dict(torch.load(\"Lightstudent_s.pth\"))\n",
    "best_model_s.eval()\n",
    "\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "all_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, metas, labels in test_loader:\n",
    "        images, metas, labels = images.to(device), metas.to(device), labels.to(device)\n",
    "        batch_indices = torch.arange(metas.size(0)).to(device).long()\n",
    "        outputs = best_model_s(images, metas, batch_indices)        \n",
    "        probs = F.softmax(outputs, dim=1)\n",
    "        preds = probs.argmax(dim=1)\n",
    "\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "all_labels = np.array(all_labels)\n",
    "all_preds = np.array(all_preds)\n",
    "all_probs = np.array(all_probs)\n",
    "\n",
    "# Compute classification report\n",
    "class_report = classification_report(all_labels, all_preds, target_names=class_names, digits=4)\n",
    "\n",
    "# Compute normalized confusion matrix\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds, normalize=\"true\")\n",
    "\n",
    "# Display classification report\n",
    "print(\"\\nClassification Report (s backbone):\\n\")\n",
    "print(class_report)\n",
    "\n",
    "# Display confusion matrix (black and white)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, cmap=\"gray\", fmt=\".2f\", xticklabels=class_names, yticklabels=class_names, cbar=True)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Normalized Confusion Matrix (s backbone)\")\n",
    "plt.show()\n",
    "\n",
    "# Compute and plot ROC-AUC curve for each class\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for i, class_name in enumerate(class_names):\n",
    "    fpr, tpr, _ = roc_curve((all_labels == i).astype(int), all_probs[:, i])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label=f\"{class_name} (AUC = {roc_auc:.2f})\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], \"k--\")  # Diagonal line for reference\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC-AUC Curve (s backbone)\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Optional: Compute and plot Precision-Recall Curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for i, class_name in enumerate(class_names):\n",
    "    precision, recall, _ = precision_recall_curve((all_labels == i).astype(int), all_probs[:, i])\n",
    "    plt.plot(recall, precision, label=f\"{class_name}\")\n",
    "\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve (s backbone)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab9e1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_cluster import knn_graph\n",
    "from torchinfo import summary\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Early Fusion with Dynamic GCN\n",
    "# -----------------------------\n",
    "class EarlyFusionWithDynamicGCN(nn.Module):\n",
    "    def __init__(self, input_dim_meta, num_classes, backbone=\"xxs\", k=8):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "\n",
    "        # --------- GCN branch ----------\n",
    "        self.gcn1 = GCNConv(input_dim_meta, 64)\n",
    "        self.gcn2 = GCNConv(64, 32)\n",
    "        self.res_proj = nn.Linear(64, 32)\n",
    "\n",
    "        # Metadata ‚Üí pseudo-image\n",
    "        self.meta_to_image = nn.Sequential(\n",
    "            nn.Linear(32, 56*56),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(56*56),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "        # --------- MobileViT backbone ----------\n",
    "        if backbone == \"xxs\":\n",
    "            model_name = \"mobilevit_xxs.cvnets_in1k\"\n",
    "            self.out_channels = 320\n",
    "        elif backbone == \"s\":\n",
    "            model_name = \"mobilevit_s.cvnets_in1k\"\n",
    "            self.out_channels = 640\n",
    "        else:\n",
    "            raise ValueError(\"backbone must be 'xxs' or 's'\")\n",
    "\n",
    "        self.mobilevit = timm.create_model(\n",
    "            model_name,\n",
    "            pretrained=True,\n",
    "            num_classes=0,\n",
    "            global_pool=''   # keep features, no built-in pooling\n",
    "        )\n",
    "\n",
    "        # Modify stem for 4-channel input\n",
    "        stem_conv = self.mobilevit.stem.conv\n",
    "        new_conv = nn.Conv2d(\n",
    "            4, stem_conv.out_channels,\n",
    "            kernel_size=stem_conv.kernel_size,\n",
    "            stride=stem_conv.stride,\n",
    "            padding=stem_conv.padding,\n",
    "            bias=stem_conv.bias is not None\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # copy RGB weights\n",
    "            new_conv.weight[:, :3] = stem_conv.weight\n",
    "            # tiny weight for metadata channel\n",
    "            new_conv.weight[:, 3:] = stem_conv.weight.mean(dim=1, keepdim=True) * 0.1\n",
    "            # copy bias if exists\n",
    "            if stem_conv.bias is not None:\n",
    "                new_conv.bias = stem_conv.bias.clone()\n",
    "\n",
    "        self.mobilevit.stem.conv = new_conv\n",
    "\n",
    "        # --------- Classifier ----------\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.out_channels, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, img, meta, batch_idx):\n",
    "        B = meta.size(0)\n",
    "\n",
    "        # --------- Graph construction (NOW IDENTICAL TO MODEL B) ----------\n",
    "        # dynamic kNN graph WITHOUT self-loops, batch-aware\n",
    "        edge_index = knn_graph(meta, k=self.k, batch=batch_idx)\n",
    "        # (torch_cluster.knn_graph by default excludes self-loops)\n",
    "\n",
    "        # ----- GCN with residual -----\n",
    "        x1 = F.relu(self.gcn1(meta, edge_index))\n",
    "        x2 = F.relu(self.gcn2(x1, edge_index) + self.res_proj(x1))\n",
    "        x_meta = x2  # [B, 32]\n",
    "\n",
    "        # ----- metadata ‚Üí pseudo-image -----\n",
    "        meta_img = self.meta_to_image(x_meta).view(B, 1, 56, 56)\n",
    "        meta_img = F.interpolate(meta_img, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "\n",
    "        # ----- early fusion -----\n",
    "        x = torch.cat([img, meta_img], dim=1)  # [B, 4, 224, 224]\n",
    "\n",
    "        # ----- MobileViT forward -----\n",
    "        features = self.mobilevit(x)          # [B, C, H, W] (no global_pool)\n",
    "        features = self.pool(features).view(B, -1)\n",
    "\n",
    "        return self.classifier(features)\n",
    "\n",
    "\n",
    "input_dim_meta = 59\n",
    "num_classes = 6\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size=16\n",
    "model_xxs = EarlyFusionWithDynamicGCN(input_dim_meta, num_classes, backbone=\"xxs\").to(device)\n",
    "model_s = EarlyFusionWithDynamicGCN(input_dim_meta, num_classes, backbone=\"s\").to(device)\n",
    "dummy_img = torch.randn(batch_size, 3, 224, 224).to(device)\n",
    "dummy_meta = torch.randn(batch_size, input_dim_meta).to(device)\n",
    "dummy_batch_idx = torch.arange(batch_size, device=device)  # one node per graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67540824",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import numpy as np\n",
    "from ptflops import get_model_complexity_info\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "meta_dim = input_dim_meta\n",
    "image_size = (3, 224, 224)\n",
    "\n",
    "results_s_benchmark = []\n",
    "\n",
    "def count_parameters(model, trainable_only=False):\n",
    "    if trainable_only:\n",
    "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    else:\n",
    "        return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "class FusionWrapper(nn.Module):\n",
    "    \"\"\"\n",
    "    Wraps 2-input (img, meta) models so ptflops sees a single image input.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, meta_dim):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.meta_dim = meta_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        dummy_meta = torch.randn(x.size(0), self.meta_dim).to(x.device)\n",
    "        return self.model(x, dummy_meta)\n",
    "\n",
    "class GCN2InputWrapper(nn.Module):\n",
    "    \"\"\"\n",
    "    Wraps EarlyFusionWithDynamicGCN so it behaves like forward(img, meta).\n",
    "    Batch indices are synthesized internally.\n",
    "    \"\"\"\n",
    "    def __init__(self, gcn_model, meta_dim):\n",
    "        super().__init__()\n",
    "        self.gcn_model = gcn_model\n",
    "        self.meta_dim = meta_dim\n",
    "\n",
    "    def forward(self, img, meta):\n",
    "        B = meta.size(0)\n",
    "        batch_idx = torch.arange(B, device=meta.device)\n",
    "        return self.gcn_model(img, meta, batch_idx)\n",
    "\n",
    "def compute_flops(model, meta_dim):\n",
    "    wrapper = FusionWrapper(model, meta_dim).to(device)\n",
    "    with torch.no_grad():\n",
    "        flops, _ = get_model_complexity_info(\n",
    "            wrapper,\n",
    "            image_size,\n",
    "            as_strings=False,\n",
    "            print_per_layer_stat=False,\n",
    "            verbose=False\n",
    "        )\n",
    "    return float(flops / 1e9)  # GFLOPs\n",
    "\n",
    "def measure_gpu_latency(model, meta_dim, runs=200, warmup=30):\n",
    "    if not torch.cuda.is_available():\n",
    "        return None, None, None\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    model.eval().to(device)\n",
    "\n",
    "    dummy_img = torch.randn(1, *image_size, device=device)\n",
    "    dummy_meta = torch.randn(1, meta_dim, device=device)\n",
    "\n",
    "    # warmup\n",
    "    for _ in range(warmup):\n",
    "        _ = model(dummy_img, dummy_meta)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    start_evt = torch.cuda.Event(enable_timing=True)\n",
    "    end_evt = torch.cuda.Event(enable_timing=True)\n",
    "    times = []\n",
    "\n",
    "    for _ in range(runs):\n",
    "        start_evt.record()\n",
    "        _ = model(dummy_img, dummy_meta)\n",
    "        end_evt.record()\n",
    "        torch.cuda.synchronize()\n",
    "        times.append(start_evt.elapsed_time(end_evt))  # ms\n",
    "\n",
    "    times = np.array(times)\n",
    "    mean = float(times.mean())\n",
    "    std = float(times.std())\n",
    "    fps = float(1000.0 / mean)\n",
    "    return mean, std, fps\n",
    "\n",
    "def measure_cpu_latency(model, meta_dim, runs=100, warmup=20):\n",
    "    model_cpu = model.cpu()\n",
    "    model_cpu.eval()\n",
    "\n",
    "    dummy_img = torch.randn(1, *image_size)\n",
    "    dummy_meta = torch.randn(1, meta_dim)\n",
    "\n",
    "    for _ in range(warmup):\n",
    "        _ = model_cpu(dummy_img, dummy_meta)\n",
    "\n",
    "    times = []\n",
    "    for _ in range(runs):\n",
    "        start = time.perf_counter()\n",
    "        _ = model_cpu(dummy_img, dummy_meta)\n",
    "        end = time.perf_counter()\n",
    "        times.append((end - start) * 1000.0)\n",
    "\n",
    "    times = np.array(times)\n",
    "    mean = float(times.mean())\n",
    "    std = float(times.std())\n",
    "    fps = float(1000.0 / mean)\n",
    "    return mean, std, fps\n",
    "\n",
    "def benchmark_model(model, name, meta_dim, count_all_params=False):\n",
    "    \"\"\"\n",
    "    Given a ready-to-use 2-input (img, meta) model, compute all stats.\n",
    "    \"\"\"\n",
    "    print(f\"\\n======= Benchmarking: {name} =======\")\n",
    "\n",
    "    trainable_only = not count_all_params\n",
    "    params_m = count_parameters(model, trainable_only=trainable_only) / 1e6\n",
    "    print(f\"Params: {params_m:.3f} M\")\n",
    "\n",
    "    flops_g = compute_flops(model, meta_dim)\n",
    "    print(f\"FLOPs: {flops_g:.3f} G\")\n",
    "\n",
    "    gpu_mean, gpu_std, gpu_fps = measure_gpu_latency(model, meta_dim)\n",
    "    if gpu_mean is not None:\n",
    "        print(f\"GPU Latency: {gpu_mean:.3f} ¬± {gpu_std:.3f} ms  |  FPS: {gpu_fps:.1f}\")\n",
    "    else:\n",
    "        print(\"GPU Latency: N/A\")\n",
    "\n",
    "    cpu_mean, cpu_std, cpu_fps = measure_cpu_latency(model, meta_dim)\n",
    "    print(f\"CPU Latency: {cpu_mean:.3f} ¬± {cpu_std:.3f} ms  |  FPS: {cpu_fps:.1f}\")\n",
    "\n",
    "    return {\n",
    "        \"model\": name,\n",
    "        \"params_M\": params_m,\n",
    "        \"flops_G\": flops_g,\n",
    "        \"gpu_latency_mean_ms\": gpu_mean,\n",
    "        \"gpu_latency_std_ms\": gpu_std,\n",
    "        \"gpu_fps\": gpu_fps,\n",
    "        \"cpu_latency_mean_ms\": cpu_mean,\n",
    "        \"cpu_latency_std_ms\": cpu_std,\n",
    "        \"cpu_fps\": cpu_fps,\n",
    "    }\n",
    "\n",
    "# Benchmark s backbone student\n",
    "student_raw_s = EarlyFusionWithDynamicGCN(input_dim_meta, num_classes, backbone=\"s\")\n",
    "student_state_s = torch.load(\"Lightstudent_s.pth\", map_location=\"cpu\")\n",
    "student_raw_s.load_state_dict(student_state_s)\n",
    "student_model_s_wrapped = GCN2InputWrapper(student_raw_s, input_dim_meta).to(device).eval()\n",
    "results_s_benchmark.append(benchmark_model(student_model_s_wrapped, \"TabFusion (GCN Student - s backbone)\", input_dim_meta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa68dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_cluster import knn_graph\n",
    "import timm\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# IMPROVED MODEL (A FIXED TO MATCH B'S BEHAVIOR)\n",
    "# =========================================================\n",
    "class EarlyFusionWithGCN(nn.Module):\n",
    "    def __init__(self, input_dim_meta, num_classes, k=8):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "\n",
    "        # --- GCN Layers ---\n",
    "        self.gcn1 = GCNConv(input_dim_meta, 64)\n",
    "        self.gcn2 = GCNConv(64, 32)\n",
    "        self.res_proj = nn.Linear(64, 32)\n",
    "\n",
    "        # --- metadata ‚Üí pseudo image ---\n",
    "        self.meta_to_image = nn.Sequential(\n",
    "            nn.Linear(32, 56 * 56),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(56 * 56),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "        # --- MobileViT backbone ---\n",
    "        self.mobilevit = timm.create_model(\n",
    "            \"mobilevit_s.cvnets_in1k\",\n",
    "            pretrained=True,\n",
    "            num_classes=0\n",
    "        )\n",
    "\n",
    "        # --- Modify first conv to accept 4 channels ---\n",
    "        stem_conv = self.mobilevit.stem.conv\n",
    "        new_conv = nn.Conv2d(\n",
    "            4, stem_conv.out_channels,\n",
    "            kernel_size=stem_conv.kernel_size,\n",
    "            stride=stem_conv.stride,\n",
    "            padding=stem_conv.padding,\n",
    "            bias=stem_conv.bias is not None\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # copy RGB weights\n",
    "            new_conv.weight[:, :3] = stem_conv.weight\n",
    "            # tiny weight for metadata channel\n",
    "            new_conv.weight[:, 3:] = stem_conv.weight.mean(dim=1, keepdim=True) * 0.1\n",
    "            # copy bias if exists\n",
    "            if stem_conv.bias is not None:\n",
    "                new_conv.bias = stem_conv.bias.clone()\n",
    "\n",
    "        self.mobilevit.stem.conv = new_conv\n",
    "\n",
    "        # keep only first 4 stages\n",
    "        self.mobilevit.stages = nn.Sequential(\n",
    "            *list(self.mobilevit.stages.children())[:4]\n",
    "        )\n",
    "        self.mobilevit.final_conv = nn.Identity()\n",
    "        self.mobilevit.head = nn.Identity()\n",
    "\n",
    "        # --- Post Conv ---\n",
    "        self.post_conv = nn.Sequential(\n",
    "            nn.Conv2d(128, 160, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(160),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # --- Classifier ---\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(160, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, img, meta, batch_idx):\n",
    "        B = meta.size(0)\n",
    "\n",
    "        # CORRECT: dynamic kNN graph WITHOUT self-loops\n",
    "        edge_index = knn_graph(meta, k=self.k, batch=batch_idx)\n",
    "\n",
    "        # GCN + residual\n",
    "        x1 = F.relu(self.gcn1(meta, edge_index))\n",
    "        x2 = F.relu(self.gcn2(x1, edge_index) + self.res_proj(x1))\n",
    "\n",
    "        # Metadata ‚Üí pseudo-image\n",
    "        meta_img = self.meta_to_image(x2).view(B, 1, 56, 56)\n",
    "        meta_img = F.interpolate(meta_img, size=(224, 224), mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "        # Early fusion (4 channels)\n",
    "        x = torch.cat([img, meta_img], dim=1)\n",
    "\n",
    "        # CNN forward\n",
    "        feats = self.mobilevit.stem(x)\n",
    "        feats = self.mobilevit.stages(feats)\n",
    "        feats = self.post_conv(feats)\n",
    "        feats = self.pool(feats).view(B, -1)\n",
    "\n",
    "        return self.classifier(feats)\n",
    "    \n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = EarlyFusionWithGCN(input_dim_meta, num_classes).to(device)\n",
    "\n",
    "dummy_img = torch.randn(batch_size, 3, 224, 224).to(device)\n",
    "dummy_meta = torch.randn(batch_size, input_dim_meta).to(device)\n",
    "dummy_batch_idx = torch.arange(batch_size).to(device)\n",
    "\n",
    "summary(\n",
    "    model,\n",
    "    input_data=[dummy_img, dummy_meta, dummy_batch_idx],\n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "    col_width=20,\n",
    "    depth=3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c0e061",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import numpy as np\n",
    "from ptflops import get_model_complexity_info\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "meta_dim = input_dim_meta\n",
    "image_size = (3, 224, 224)\n",
    "\n",
    "results_s_benchmark = []\n",
    "\n",
    "def count_parameters(model, trainable_only=False):\n",
    "    if trainable_only:\n",
    "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    else:\n",
    "        return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "class FusionWrapper(nn.Module):\n",
    "    \"\"\"\n",
    "    Wraps 2-input (img, meta) models so ptflops sees a single image input.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, meta_dim):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.meta_dim = meta_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        dummy_meta = torch.randn(x.size(0), self.meta_dim).to(x.device)\n",
    "        return self.model(x, dummy_meta)\n",
    "\n",
    "class GCN2InputWrapper(nn.Module):\n",
    "    \"\"\"\n",
    "    Wraps EarlyFusionWithDynamicGCN so it behaves like forward(img, meta).\n",
    "    Batch indices are synthesized internally.\n",
    "    \"\"\"\n",
    "    def __init__(self, gcn_model, meta_dim):\n",
    "        super().__init__()\n",
    "        self.gcn_model = gcn_model\n",
    "        self.meta_dim = meta_dim\n",
    "\n",
    "    def forward(self, img, meta):\n",
    "        B = meta.size(0)\n",
    "        batch_idx = torch.arange(B, device=meta.device)\n",
    "        return self.gcn_model(img, meta, batch_idx)\n",
    "\n",
    "def compute_flops(model, meta_dim):\n",
    "    wrapper = FusionWrapper(model, meta_dim).to(device)\n",
    "    with torch.no_grad():\n",
    "        flops, _ = get_model_complexity_info(\n",
    "            wrapper,\n",
    "            image_size,\n",
    "            as_strings=False,\n",
    "            print_per_layer_stat=False,\n",
    "            verbose=False\n",
    "        )\n",
    "    return float(flops / 1e9)  # GFLOPs\n",
    "\n",
    "def measure_gpu_latency(model, meta_dim, runs=200, warmup=30):\n",
    "    if not torch.cuda.is_available():\n",
    "        return None, None, None\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    model.eval().to(device)\n",
    "\n",
    "    dummy_img = torch.randn(1, *image_size, device=device)\n",
    "    dummy_meta = torch.randn(1, meta_dim, device=device)\n",
    "\n",
    "    # warmup\n",
    "    for _ in range(warmup):\n",
    "        _ = model(dummy_img, dummy_meta)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    start_evt = torch.cuda.Event(enable_timing=True)\n",
    "    end_evt = torch.cuda.Event(enable_timing=True)\n",
    "    times = []\n",
    "\n",
    "    for _ in range(runs):\n",
    "        start_evt.record()\n",
    "        _ = model(dummy_img, dummy_meta)\n",
    "        end_evt.record()\n",
    "        torch.cuda.synchronize()\n",
    "        times.append(start_evt.elapsed_time(end_evt))  # ms\n",
    "\n",
    "    times = np.array(times)\n",
    "    mean = float(times.mean())\n",
    "    std = float(times.std())\n",
    "    fps = float(1000.0 / mean)\n",
    "    return mean, std, fps\n",
    "\n",
    "def measure_cpu_latency(model, meta_dim, runs=100, warmup=20):\n",
    "    model_cpu = model.cpu()\n",
    "    model_cpu.eval()\n",
    "\n",
    "    dummy_img = torch.randn(1, *image_size)\n",
    "    dummy_meta = torch.randn(1, meta_dim)\n",
    "\n",
    "    for _ in range(warmup):\n",
    "        _ = model_cpu(dummy_img, dummy_meta)\n",
    "\n",
    "    times = []\n",
    "    for _ in range(runs):\n",
    "        start = time.perf_counter()\n",
    "        _ = model_cpu(dummy_img, dummy_meta)\n",
    "        end = time.perf_counter()\n",
    "        times.append((end - start) * 1000.0)\n",
    "\n",
    "    times = np.array(times)\n",
    "    mean = float(times.mean())\n",
    "    std = float(times.std())\n",
    "    fps = float(1000.0 / mean)\n",
    "    return mean, std, fps\n",
    "\n",
    "def benchmark_model(model, name, meta_dim, count_all_params=False):\n",
    "    \"\"\"\n",
    "    Given a ready-to-use 2-input (img, meta) model, compute all stats.\n",
    "    \"\"\"\n",
    "    print(f\"\\n======= Benchmarking: {name} =======\")\n",
    "\n",
    "    trainable_only = not count_all_params\n",
    "    params_m = count_parameters(model, trainable_only=trainable_only) / 1e6\n",
    "    print(f\"Params: {params_m:.3f} M\")\n",
    "\n",
    "    flops_g = compute_flops(model, meta_dim)\n",
    "    print(f\"FLOPs: {flops_g:.3f} G\")\n",
    "\n",
    "    gpu_mean, gpu_std, gpu_fps = measure_gpu_latency(model, meta_dim)\n",
    "    if gpu_mean is not None:\n",
    "        print(f\"GPU Latency: {gpu_mean:.3f} ¬± {gpu_std:.3f} ms  |  FPS: {gpu_fps:.1f}\")\n",
    "    else:\n",
    "        print(\"GPU Latency: N/A\")\n",
    "\n",
    "    cpu_mean, cpu_std, cpu_fps = measure_cpu_latency(model, meta_dim)\n",
    "    print(f\"CPU Latency: {cpu_mean:.3f} ¬± {cpu_std:.3f} ms  |  FPS: {cpu_fps:.1f}\")\n",
    "\n",
    "    return {\n",
    "        \"model\": name,\n",
    "        \"params_M\": params_m,\n",
    "        \"flops_G\": flops_g,\n",
    "        \"gpu_latency_mean_ms\": gpu_mean,\n",
    "        \"gpu_latency_std_ms\": gpu_std,\n",
    "        \"gpu_fps\": gpu_fps,\n",
    "        \"cpu_latency_mean_ms\": cpu_mean,\n",
    "        \"cpu_latency_std_ms\": cpu_std,\n",
    "        \"cpu_fps\": cpu_fps,\n",
    "    }\n",
    "\n",
    "# Benchmark s backbone student\n",
    "student_raw_s = EarlyFusionWithDynamicGCN(input_dim_meta, num_classes, backbone=\"s\")\n",
    "student_model_s_wrapped = GCN2InputWrapper(student_raw_s, input_dim_meta).to(device).eval()\n",
    "results_s_benchmark.append(benchmark_model(student_model_s_wrapped, \"TabFusion (GCN Student - s backbone)\", input_dim_meta))\n",
    "\n",
    "\n",
    "student_raw_xxs = EarlyFusionWithDynamicGCN(input_dim_meta, num_classes, backbone=\"xxs\")\n",
    "student_model_xxs_wrapped = GCN2InputWrapper(student_raw_xxs, input_dim_meta).to(device).eval()\n",
    "results_s_benchmark.append(benchmark_model(student_model_xxs_wrapped, \"TabFusion (GCN Student - xxs backbone)\", input_dim_meta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07663449",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "student_raw_s = EarlyFusionWithGCN(input_dim_meta, 5)\n",
    "student_state_s = torch.load(\"dermpGCN.pth\", map_location=\"cpu\")\n",
    "student_raw_s.load_state_dict(student_state_s)\n",
    "student_model_s_wrapped = GCN2InputWrapper(student_raw_s, input_dim_meta).to(device).eval()\n",
    "results_s_benchmark.append(benchmark_model(student_model_s_wrapped, \"TabFusion\", input_dim_meta))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8754330,
     "sourceId": 13759182,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 504263,
     "modelInstanceId": 488842,
     "sourceId": 648092,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 504265,
     "modelInstanceId": 488844,
     "sourceId": 648094,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
