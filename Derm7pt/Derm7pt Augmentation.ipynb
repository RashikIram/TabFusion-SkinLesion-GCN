{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf1cf7d-e12a-469e-b3c6-c37dee98db7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics.pairwise import euclidean_distances, cosine_similarity\n",
    "\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from torchvision import transforms\n",
    "import imagehash\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =========================================================\n",
    "# CONFIGURATION ‚Äì CHANGE THESE PATHS\n",
    "# =========================================================\n",
    "META_CSV      = r\"meta\\meta.csv\"\n",
    "TRAIN_IDX_CSV = r\"meta\\train_indexes.csv\"\n",
    "VALID_IDX_CSV = r\"meta\\valid_indexes.csv\"\n",
    "TEST_IDX_CSV  = r\"meta\\test_indexes.csv\"\n",
    "\n",
    "# Root folder where 'derm' image paths are stored\n",
    "# e.g., if derm column is \"NEL/Nel026.jpg\", and they live in \"images/NEL/Nel026.jpg\"\n",
    "IMAGE_ROOT_DIR = r\"images\"\n",
    "\n",
    "SAVE_DIR = r\"augmented\"\n",
    "AUG_DIR  = os.path.join(SAVE_DIR, \"augmented_images\")\n",
    "\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "os.makedirs(AUG_DIR, exist_ok=True)\n",
    "\n",
    "# Random seed\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# =========================================================\n",
    "# LOAD META + SPLITS\n",
    "# =========================================================\n",
    "meta = pd.read_csv(META_CSV)\n",
    "\n",
    "train_idx = pd.read_csv(TRAIN_IDX_CSV)[\"indexes\"].values\n",
    "val_idx   = pd.read_csv(VALID_IDX_CSV)[\"indexes\"].values\n",
    "test_idx  = pd.read_csv(TEST_IDX_CSV)[\"indexes\"].values\n",
    "\n",
    "print(f\"Total cases in meta: {len(meta)}\")\n",
    "print(f\"Train indices: {len(train_idx)}\")\n",
    "print(f\"Valid indices: {len(val_idx)}\")\n",
    "print(f\"Test indices:  {len(test_idx)}\")\n",
    "\n",
    "# =========================================================\n",
    "# IMAGE PATHS & LABELS\n",
    "# =========================================================\n",
    "# Use dermoscopy images\n",
    "meta[\"ImagePath\"] = meta[\"derm\"].apply(lambda x: os.path.join(IMAGE_ROOT_DIR, x))\n",
    "\n",
    "# =========================================================\n",
    "# LABEL GROUPING (as per Derm7pt paper)\n",
    "# =========================================================\n",
    "# Group diagnoses into 5 clinically meaningful categories (EXACTLY 5 classes)\n",
    "# BCC | MEL | MISC | NEV | SEK\n",
    "diagnosis_grouping = {\n",
    "    # MEL - Melanoma\n",
    "    \"melanoma\": \"MEL\",\n",
    "    \"melanoma in situ\": \"MEL\",\n",
    "    \"melanoma (in situ)\": \"MEL\",\n",
    "    \"melanoma (less than 0.76 mm)\": \"MEL\",\n",
    "    \"melanoma (0.76 to 1.5 mm)\": \"MEL\",\n",
    "    \"melanoma (more than 1.5 mm)\": \"MEL\",\n",
    "    \"melanoma metastasis\": \"MEL\",\n",
    "    \"melanosis\": \"MEL\",  # melanosis ‚Üí MEL\n",
    "    \n",
    "    # NEV - Nevus (all nevus types)\n",
    "    \"blue nevus\": \"NEV\",\n",
    "    \"Clark nevus\": \"NEV\",\n",
    "    \"clark nevus\": \"NEV\",  # lowercase variant\n",
    "    \"combined nevus\": \"NEV\",\n",
    "    \"congenital nevus\": \"NEV\",\n",
    "    \"dermal nevus\": \"NEV\",\n",
    "    \"recurrent nevus\": \"NEV\",\n",
    "    \"Reed nevus\": \"NEV\",\n",
    "    \"Spitz nevus\": \"NEV\",\n",
    "    \"reed or spitz nevus\": \"NEV\",  # combined Reed/Spitz ‚Üí NEV\n",
    "    \"nevus\": \"NEV\",\n",
    "    \n",
    "    # BCC - Basal Cell Carcinoma\n",
    "    \"basal cell carcinoma\": \"BCC\",\n",
    "    \n",
    "    # SEK - Seborrheic Keratosis (SK in paper)\n",
    "    \"seborrheic keratosis\": \"SEK\",\n",
    "    \"solar lentigo\": \"SEK\",\n",
    "    \"lentigo\": \"SEK\",\n",
    "    \"lichenoid keratosis\": \"SEK\",\n",
    "    \n",
    "    # MISC - Miscellaneous (dermatofibroma, vascular lesions, etc.)\n",
    "    \"dermatofibroma\": \"MISC\",\n",
    "    \"vascular lesion\": \"MISC\",\n",
    "    \"miscellaneous\": \"MISC\",\n",
    "}\n",
    "\n",
    "# Apply grouping\n",
    "meta[\"diagnosis_grouped\"] = meta[\"diagnosis\"].map(diagnosis_grouping)\n",
    "\n",
    "# Check for unmapped diagnoses (should be none now)\n",
    "unmapped = meta[meta[\"diagnosis_grouped\"].isna()][\"diagnosis\"].unique()\n",
    "if len(unmapped) > 0:\n",
    "    print(f\"‚ö†Ô∏è  ERROR: {len(unmapped)} unmapped diagnoses found:\")\n",
    "    for d in unmapped:\n",
    "        print(f\"   - '{d}'\")\n",
    "    raise ValueError(f\"Unmapped diagnoses found! Please add them to diagnosis_grouping dict.\")\n",
    "\n",
    "# Encode grouped labels\n",
    "label_encoder = LabelEncoder()\n",
    "meta[\"label\"] = label_encoder.fit_transform(meta[\"diagnosis_grouped\"])\n",
    "\n",
    "label_mapping = {cls: int(i) for i, cls in enumerate(label_encoder.classes_)}\n",
    "\n",
    "# Verify we have exactly 5 classes\n",
    "num_classes = len(label_encoder.classes_)\n",
    "if num_classes != 5:\n",
    "    raise ValueError(f\"Expected 5 classes, but got {num_classes}: {list(label_encoder.classes_)}\")\n",
    "\n",
    "print(\"\\n‚úÖ Grouped Label mapping (5 classes):\")\n",
    "for k, v in label_mapping.items():\n",
    "    print(f\"  {k} -> {v}\")\n",
    "    \n",
    "print(\"\\nDiagnosis distribution after grouping:\")\n",
    "class_dist = meta[\"diagnosis_grouped\"].value_counts().sort_index()\n",
    "for cls, count in class_dist.items():\n",
    "    print(f\"  {cls}: {count} samples\")\n",
    "print(f\"\\nTotal: {len(meta)} samples across {num_classes} classes\")\n",
    "\n",
    "# =========================================================\n",
    "# METADATA PREPROCESSING (as per paper)\n",
    "# =========================================================\n",
    "# Paper uses ONLY: sex, location, elevation (all categorical, one-hot encoded)\n",
    "# NO numeric fields, NO 7-point checklist features for metadata\n",
    "categorical_cols = [\"sex\", \"location\", \"elevation\"]\n",
    "\n",
    "# Clean categoricals (fill missing with 'unknown')\n",
    "for col in categorical_cols:\n",
    "    meta[col] = meta[col].fillna(\"unknown\")\n",
    "\n",
    "print(\"\\nMetadata feature distributions:\")\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(meta[col].value_counts())\n",
    "\n",
    "# One-hot encode categoricals (no drop_first to keep all categories)\n",
    "X_cat = pd.get_dummies(meta[categorical_cols], drop_first=False)\n",
    "\n",
    "print(f\"\\nOne-hot encoded metadata shape: {X_cat.shape}\")\n",
    "print(f\"Metadata features: {list(X_cat.columns)}\")\n",
    "\n",
    "# Final metadata feature matrix (only categorical features)\n",
    "X_meta = X_cat.reset_index(drop=True)\n",
    "\n",
    "y = meta[\"label\"].astype(int)\n",
    "img_paths_all = meta[\"ImagePath\"]\n",
    "\n",
    "print(f\"\\nMetadata feature shape: {X_meta.shape}\")\n",
    "\n",
    "# Check class distribution\n",
    "print(\"\\nClass distribution:\")\n",
    "class_counts = y.value_counts().sort_index()\n",
    "for cls, count in class_counts.items():\n",
    "    cls_name = label_encoder.classes_[cls]\n",
    "    print(f\"  {cls_name} (label {cls}): {count} samples\")\n",
    "\n",
    "# Find minimum class size\n",
    "min_class_size = class_counts.min()\n",
    "print(f\"\\nMinimum class size: {min_class_size}\")\n",
    "\n",
    "# Set k_neighbors based on smallest class (must be < min_class_size)\n",
    "k_neighbors = max(1, min(5, min_class_size - 1))\n",
    "print(f\"Using k_neighbors={k_neighbors} for SMOTETomek\")\n",
    "\n",
    "print(\"\\nApplying SMOTETomek on full metadata (before split)...\")\n",
    "\n",
    "# =========================================================\n",
    "# SMOTETomek (on full dataset, like your ISIC code)\n",
    "# =========================================================\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Convert to numpy array (SMOTE needs numeric data, not boolean DataFrame)\n",
    "X_meta_array = X_meta.values.astype(np.float32)\n",
    "\n",
    "smote = SMOTETomek(\n",
    "    smote=SMOTE(k_neighbors=k_neighbors, random_state=seed),\n",
    "    random_state=seed\n",
    ")\n",
    "X_meta_res, y_res = smote.fit_resample(X_meta_array, y)\n",
    "\n",
    "orig_len = len(X_meta)\n",
    "# Convert back to DataFrame for consistency\n",
    "X_meta_res = pd.DataFrame(X_meta_res, columns=X_meta.columns)\n",
    "y_res = pd.Series(y_res)\n",
    "\n",
    "X_synth = X_meta_res.iloc[orig_len:]\n",
    "y_synth = y_res.iloc[orig_len:]\n",
    "\n",
    "print(f\"‚úÖ Generated {len(X_synth)} synthetic metadata samples via SMOTETomek\")\n",
    "\n",
    "# =========================================================\n",
    "# OFFLINE AUGMENTATION FOR SYNTHETIC METADATA\n",
    "# =========================================================\n",
    "offline_aug = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomApply([\n",
    "        transforms.ColorJitter(0.3, 0.3, 0.3, 0.1),\n",
    "        transforms.RandomRotation(45),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "    ], p=1.0),\n",
    "])\n",
    "\n",
    "# Map label -> available real images from ORIGINAL data\n",
    "label_to_imgs = {lbl: img_paths_all[y == lbl].tolist() for lbl in sorted(y.unique())}\n",
    "\n",
    "aug_paths   = []\n",
    "aug_meta    = []\n",
    "aug_labels  = []\n",
    "aug_hashes  = []   # pHash of augmented images to avoid near-duplicates within aug set\n",
    "\n",
    "print(f\"\\nGenerating {len(X_synth)} synthetic image‚Äìmetadata pairs using offline augmentation...\")\n",
    "for i, (m_row, lbl) in enumerate(zip(X_synth.values, y_synth)):\n",
    "    candidates = label_to_imgs[lbl]\n",
    "    img_path = random.choice(candidates)\n",
    "\n",
    "    try:\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "    aug_img = offline_aug(img)\n",
    "\n",
    "    # Avoid near-duplicate augmented images\n",
    "    h = imagehash.phash(aug_img)\n",
    "    if any(h - hh <= 5 for hh in aug_hashes):\n",
    "        continue\n",
    "\n",
    "    out_path = os.path.join(AUG_DIR, f\"aug_{lbl}_{i}.jpg\")\n",
    "    aug_img.save(out_path, \"JPEG\", quality=95)\n",
    "\n",
    "    aug_hashes.append(h)\n",
    "    aug_paths.append(out_path)\n",
    "    aug_meta.append(m_row)\n",
    "    aug_labels.append(lbl)\n",
    "\n",
    "aug_meta_df = pd.DataFrame(aug_meta, columns=X_meta.columns)\n",
    "aug_labels_df = pd.DataFrame({\"ImagePath\": aug_paths, \"Label\": aug_labels})\n",
    "\n",
    "print(f\"‚úÖ Saved {len(aug_labels_df)} augmented images ‚Üí {AUG_DIR}\")\n",
    "\n",
    "# Save raw augmented data (optional, before cleaning)\n",
    "aug_meta_df.to_csv(os.path.join(SAVE_DIR, \"augmented_metadata_raw.csv\"), index=False)\n",
    "aug_labels_df.to_csv(os.path.join(SAVE_DIR, \"augmented_labels_raw.csv\"), index=False)\n",
    "\n",
    "# =========================================================\n",
    "# BUILD OFFICIAL TRAIN / VAL / TEST SPLITS (NO RESPLITTING)\n",
    "# =========================================================\n",
    "X_train_meta = X_meta.iloc[train_idx].reset_index(drop=True)\n",
    "X_val_meta   = X_meta.iloc[val_idx].reset_index(drop=True)\n",
    "X_test_meta  = X_meta.iloc[test_idx].reset_index(drop=True)\n",
    "\n",
    "X_train_img_paths = img_paths_all.iloc[train_idx].reset_index(drop=True)\n",
    "X_val_img_paths   = img_paths_all.iloc[val_idx].reset_index(drop=True)\n",
    "X_test_img_paths  = img_paths_all.iloc[test_idx].reset_index(drop=True)\n",
    "\n",
    "y_train = y.iloc[train_idx].reset_index(drop=True)\n",
    "y_val   = y.iloc[val_idx].reset_index(drop=True)\n",
    "y_test  = y.iloc[test_idx].reset_index(drop=True)\n",
    "\n",
    "print(f\"\\n‚úÖ Train: {len(X_train_meta)} samples\")\n",
    "print(f\"‚úÖ Val:   {len(X_val_meta)} samples\")\n",
    "print(f\"‚úÖ Test:  {len(X_test_meta)} samples\")\n",
    "\n",
    "# =========================================================\n",
    "# FILTER AUGMENTED DATA AGAINST VAL/TEST ‚Äì IMAGE pHASH (RELAXED)\n",
    "# =========================================================\n",
    "PHASH_THRESHOLD = 12   # Relaxed from 5 to allow more augmented variations\n",
    "\n",
    "def compute_hashes(paths, desc):\n",
    "    hashes = {}\n",
    "    for p in tqdm(paths, desc=desc):\n",
    "        try:\n",
    "            img = Image.open(p).convert(\"RGB\").resize((224, 224))\n",
    "            hashes[p] = imagehash.phash(img)\n",
    "        except Exception:\n",
    "            continue\n",
    "    return hashes\n",
    "\n",
    "print(\"\\nüìä Computing perceptual hashes for val/test/aug images...\")\n",
    "val_hashes  = compute_hashes(X_val_img_paths.tolist(), \"Val hashes\")\n",
    "test_hashes = compute_hashes(X_test_img_paths.tolist(), \"Test hashes\")\n",
    "aug_hashes_dict = compute_hashes(aug_labels_df[\"ImagePath\"].tolist(), \"Aug hashes\")\n",
    "\n",
    "print(f\"\\nüîç Checking augmented vs validation/test similarity (pHash ‚â§ {PHASH_THRESHOLD})...\")\n",
    "to_drop = set()\n",
    "\n",
    "for aug_path, h_aug in aug_hashes_dict.items():\n",
    "    # vs validation\n",
    "    if len(val_hashes) > 0:\n",
    "        if any(h_aug - hv <= PHASH_THRESHOLD for hv in val_hashes.values()):\n",
    "            to_drop.add(aug_path)\n",
    "    \n",
    "    # vs test\n",
    "    if len(test_hashes) > 0:\n",
    "        if any(h_aug - ht <= PHASH_THRESHOLD for ht in test_hashes.values()):\n",
    "            to_drop.add(aug_path)\n",
    "\n",
    "print(f\"üßπ Removing {len(to_drop)} augmented images visually similar to val/test\")\n",
    "\n",
    "mask_keep = ~aug_labels_df[\"ImagePath\"].isin(to_drop)\n",
    "aug_labels_df = aug_labels_df[mask_keep].reset_index(drop=True)\n",
    "aug_meta_df   = aug_meta_df[mask_keep].reset_index(drop=True)\n",
    "\n",
    "print(f\"Remaining augmented samples after pHash filtering: {len(aug_labels_df)}\")\n",
    "\n",
    "# =========================================================\n",
    "# METADATA-BASED FILTERING (RELAXED FOR ONE-HOT ENCODED DATA)\n",
    "# =========================================================\n",
    "if len(aug_labels_df) > 0:\n",
    "    print(\"\\nüîç Checking metadata similarity between augmented and val/test sets‚Ä¶\")\n",
    "\n",
    "    X_aug      = aug_meta_df.values\n",
    "    X_val_arr  = X_val_meta.values\n",
    "    X_test_arr = X_test_meta.values\n",
    "\n",
    "    # Relaxed thresholds for one-hot encoded categorical metadata\n",
    "    # (one-hot vectors have discrete jumps, not smooth continuous values)\n",
    "    eucl_thresh = 0.02   # Very small threshold for one-hot data\n",
    "    cos_thresh  = 0.005  # Very small threshold for one-hot data\n",
    "\n",
    "    # Euclidean distance\n",
    "    eucl_val  = euclidean_distances(X_aug, X_val_arr).min(axis=1)\n",
    "    eucl_test = euclidean_distances(X_aug, X_test_arr).min(axis=1)\n",
    "\n",
    "    # Cosine distance (1 - similarity)\n",
    "    cos_val  = 1 - cosine_similarity(X_aug, X_val_arr).max(axis=1)\n",
    "    cos_test = 1 - cosine_similarity(X_aug, X_test_arr).max(axis=1)\n",
    "\n",
    "    mask_del = (eucl_val < eucl_thresh) | (cos_val < cos_thresh) | \\\n",
    "               (eucl_test < eucl_thresh) | (cos_test < cos_thresh)\n",
    "\n",
    "    print(f\"\\nüßπ Removing {mask_del.sum()} augmented samples with nearly identical metadata to val/test\")\n",
    "\n",
    "    aug_meta_df   = aug_meta_df[~mask_del].reset_index(drop=True)\n",
    "    aug_labels_df = aug_labels_df[~mask_del].reset_index(drop=True)\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No augmented samples remaining after pHash filtering. Skipping metadata filtering.\")\n",
    "\n",
    "print(f\"‚úÖ Final augmented samples after all filtering: {len(aug_labels_df)}\")\n",
    "\n",
    "# Save cleaned augmented data (optional)\n",
    "aug_meta_df.to_csv(os.path.join(SAVE_DIR, \"augmented_metadata_clean.csv\"), index=False)\n",
    "aug_labels_df.to_csv(os.path.join(SAVE_DIR, \"augmented_labels_clean.csv\"), index=False)\n",
    "\n",
    "# =========================================================\n",
    "# COMBINE CLEAN AUGMENTED DATA WITH TRAIN SET ONLY\n",
    "# =========================================================\n",
    "if len(aug_labels_df) > 0:\n",
    "    print(\"\\nüì¶ Combining cleaned augmented data with TRAIN set only...\")\n",
    "\n",
    "    X_train_meta_final = pd.concat(\n",
    "        [X_train_meta.reset_index(drop=True),\n",
    "         aug_meta_df.reset_index(drop=True)],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    X_train_img_paths_final = pd.concat(\n",
    "        [X_train_img_paths.reset_index(drop=True),\n",
    "         aug_labels_df[\"ImagePath\"]],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    y_train_final = pd.concat(\n",
    "        [y_train.reset_index(drop=True),\n",
    "         aug_labels_df[\"Label\"]],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    print(f\"\\n‚úÖ Final training samples (real + augmented): {len(X_train_meta_final)}\")\n",
    "    print(f\"   - Real training samples: {len(X_train_meta)}\")\n",
    "    print(f\"   - Augmented samples:     {len(aug_meta_df)}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No augmented samples survived filtering. Using original training data only.\")\n",
    "    X_train_meta_final = X_train_meta\n",
    "    X_train_img_paths_final = X_train_img_paths\n",
    "    y_train_final = y_train\n",
    "    \n",
    "    print(f\"\\n‚úÖ Final training samples: {len(X_train_meta_final)}\")\n",
    "print(f\"‚úÖ Final validation samples: {len(X_val_meta)}\")\n",
    "print(f\"‚úÖ Final test samples:       {len(X_test_meta)}\")\n",
    "\n",
    "# Build final DataFrames\n",
    "train_final = X_train_meta_final.copy()\n",
    "train_final[\"ImagePath\"] = X_train_img_paths_final\n",
    "train_final[\"label\"]     = y_train_final\n",
    "\n",
    "val_final = X_val_meta.copy()\n",
    "val_final[\"ImagePath\"] = X_val_img_paths\n",
    "val_final[\"label\"]     = y_val\n",
    "\n",
    "test_final = X_test_meta.copy()\n",
    "test_final[\"ImagePath\"] = X_test_img_paths\n",
    "test_final[\"label\"]     = y_test\n",
    "\n",
    "# =========================================================\n",
    "# SAVE FINAL CSVs + PREPROCESSING ARTIFACTS\n",
    "# =========================================================\n",
    "train_final_path = os.path.join(SAVE_DIR, \"train_metadata_final.csv\")\n",
    "val_final_path   = os.path.join(SAVE_DIR, \"val_metadata_final.csv\")\n",
    "test_final_path  = os.path.join(SAVE_DIR, \"test_metadata_final.csv\")\n",
    "\n",
    "train_final.to_csv(train_final_path, index=False)\n",
    "val_final.to_csv(val_final_path, index=False)\n",
    "test_final.to_csv(test_final_path, index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Saved training metadata to: {train_final_path}\")\n",
    "print(f\"‚úÖ Saved validation metadata to: {val_final_path}\")\n",
    "print(f\"‚úÖ Saved test metadata to: {test_final_path}\")\n",
    "\n",
    "# Save preprocessing info (no scaler needed - only categorical features)\n",
    "preprocessing_info = {\n",
    "    \"categorical_cols\": list(X_cat.columns),\n",
    "    \"label_mapping\": label_mapping,\n",
    "    \"diagnosis_grouping\": diagnosis_grouping,\n",
    "    \"original_metadata_cols\": categorical_cols\n",
    "}\n",
    "info_path = os.path.join(SAVE_DIR, \"preprocessing_info.json\")\n",
    "with open(info_path, \"w\") as f:\n",
    "    json.dump(preprocessing_info, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Saved preprocessing info to: {info_path}\")\n",
    "print(\"\\nüéâ Derm7pt SMOTETomek + offline augmentation pipeline complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87e63f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "meta = pd.read_csv(\"G:\\\\Downloads\\\\Dermp7\\\\release_v0\\\\meta\\\\meta.csv\")\n",
    "train_idx = pd.read_csv(\"G:\\\\Downloads\\\\Dermp7\\\\release_v0\\\\meta\\\\train_indexes.csv\")\n",
    "valid_idx = pd.read_csv(\"G:\\\\Downloads\\\\Dermp7\\\\release_v0\\\\meta\\\\valid_indexes.csv\")\n",
    "test_idx  = pd.read_csv(\"G:\\\\Downloads\\\\Dermp7\\\\release_v0\\\\meta\\\\test_indexes.csv\")\n",
    "\n",
    "print(\"===== META HEAD =====\")\n",
    "print(meta.head())\n",
    "\n",
    "print(\"\\n===== META INFO =====\")\n",
    "print(meta.info())\n",
    "\n",
    "print(\"\\n===== TRAIN IDX =====\")\n",
    "print(train_idx.head())\n",
    "\n",
    "print(\"\\n===== VALID IDX =====\")\n",
    "print(valid_idx.head())\n",
    "\n",
    "print(\"\\n===== TEST IDX =====\")\n",
    "print(test_idx.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d66136",
   "metadata": {},
   "source": [
    "# üìã Loading Code for Other Derm7pt Notebooks\n",
    "\n",
    "**Use the code below in your training notebooks** after running the preprocessing pipeline above.\n",
    "\n",
    "This code will:\n",
    "- Load the preprocessed train/val/test CSVs\n",
    "- Load the saved scaler and preprocessing info\n",
    "- Provide ready-to-use data for your models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e135365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# DERM7PT DATA LOADER FOR TRAINING NOTEBOOKS\n",
    "# =========================================================\n",
    "# Run this code in your training notebooks to load the preprocessed Derm7pt dataset\n",
    "# Make sure the preprocessing pipeline above has been executed first!\n",
    "\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "# =========================================================\n",
    "# PATHS TO PREPROCESSED DATA\n",
    "# =========================================================\n",
    "PREPROCESSED_DIR = r\"augmented\"\n",
    "\n",
    "TRAIN_CSV = os.path.join(PREPROCESSED_DIR, \"train_metadata_final.csv\")\n",
    "VAL_CSV   = os.path.join(PREPROCESSED_DIR, \"val_metadata_final.csv\")\n",
    "TEST_CSV  = os.path.join(PREPROCESSED_DIR, \"test_metadata_final.csv\")\n",
    "\n",
    "INFO_PATH = os.path.join(PREPROCESSED_DIR, \"preprocessing_info.json\")\n",
    "\n",
    "# =========================================================\n",
    "# LOAD PREPROCESSED DATA\n",
    "# =========================================================\n",
    "print(\"Loading preprocessed Derm7pt data...\")\n",
    "\n",
    "# Load CSVs\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "val_df   = pd.read_csv(VAL_CSV)\n",
    "test_df  = pd.read_csv(TEST_CSV)\n",
    "\n",
    "# Load preprocessing info\n",
    "with open(INFO_PATH, \"r\") as f:\n",
    "    preprocessing_info = json.load(f)\n",
    "\n",
    "categorical_cols = preprocessing_info[\"categorical_cols\"]\n",
    "label_mapping = preprocessing_info[\"label_mapping\"]\n",
    "\n",
    "print(f\"\\n‚úÖ Training samples:   {len(train_df)}\")\n",
    "print(f\"‚úÖ Validation samples: {len(val_df)}\")\n",
    "print(f\"‚úÖ Test samples:       {len(test_df)}\")\n",
    "print(f\"\\nLabel mapping: {label_mapping}\")\n",
    "\n",
    "# =========================================================\n",
    "# EXTRACT FEATURES AND LABELS\n",
    "# =========================================================\n",
    "def extract_features(df):\n",
    "    \"\"\"Extract image paths, metadata features, and labels from dataframe\"\"\"\n",
    "    img_paths = df[\"ImagePath\"].values\n",
    "    labels = df[\"label\"].values\n",
    "    \n",
    "    # Metadata features (all columns except ImagePath and label)\n",
    "    metadata_cols = [col for col in df.columns if col not in [\"ImagePath\", \"label\"]]\n",
    "    metadata = df[metadata_cols].values\n",
    "    \n",
    "    return img_paths, metadata, labels\n",
    "\n",
    "X_train_img, X_train_meta, y_train = extract_features(train_df)\n",
    "X_val_img, X_val_meta, y_val       = extract_features(val_df)\n",
    "X_test_img, X_test_meta, y_test    = extract_features(test_df)\n",
    "\n",
    "num_classes = len(label_mapping)\n",
    "print(f\"\\nNumber of classes: {num_classes}\")\n",
    "\n",
    "# =========================================================\n",
    "# PYTORCH DATASET CLASS\n",
    "# =========================================================\n",
    "class Derm7ptDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for Derm7pt with images + metadata\n",
    "    \"\"\"\n",
    "    def __init__(self, img_paths, metadata, labels, transform=None):\n",
    "        self.img_paths = img_paths\n",
    "        self.metadata = metadata\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img_path = self.img_paths[idx]\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            # Fallback to black image if loading fails\n",
    "            print(f\"Warning: Failed to load {img_path}, using placeholder\")\n",
    "            image = Image.new(\"RGB\", (224, 224), color=\"black\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Get metadata and label\n",
    "        metadata = self.metadata[idx].astype(np.float32)\n",
    "        label = int(self.labels[idx])\n",
    "        \n",
    "        return image, metadata, label\n",
    "\n",
    "# =========================================================\n",
    "# DATA TRANSFORMS\n",
    "# =========================================================\n",
    "# Training transforms (with augmentation) - REDUCED for small Derm7pt dataset\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Validation/Test transforms (no augmentation)\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# =========================================================\n",
    "# CREATE DATASETS\n",
    "# =========================================================\n",
    "train_dataset = Derm7ptDataset(X_train_img, X_train_meta, y_train, transform=train_transform)\n",
    "val_dataset   = Derm7ptDataset(X_val_img, X_val_meta, y_val, transform=val_test_transform)\n",
    "test_dataset  = Derm7ptDataset(X_test_img, X_test_meta, y_test, transform=val_test_transform)\n",
    "\n",
    "print(f\"\\n‚úÖ Created PyTorch Datasets\")\n",
    "print(f\"   - Train: {len(train_dataset)} samples\")\n",
    "print(f\"   - Val:   {len(val_dataset)} samples\")\n",
    "print(f\"   - Test:  {len(test_dataset)} samples\")\n",
    "\n",
    "# =========================================================\n",
    "# CREATE DATALOADERS (EXAMPLE - ADJUST BATCH SIZE AS NEEDED)\n",
    "# =========================================================\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0,  # Set to 0 for Windows, increase for Linux/Mac\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Created DataLoaders (batch_size={BATCH_SIZE})\")\n",
    "print(f\"   - Train batches: {len(train_loader)}\")\n",
    "print(f\"   - Val batches:   {len(val_loader)}\")\n",
    "print(f\"   - Test batches:  {len(test_loader)}\")\n",
    "\n",
    "# =========================================================\n",
    "# EXAMPLE: TEST LOADING A BATCH\n",
    "# =========================================================\n",
    "print(\"\\nüîç Testing batch loading...\")\n",
    "for images, metadata, labels in train_loader:\n",
    "    print(f\"   - Image batch shape:    {images.shape}\")\n",
    "    print(f\"   - Metadata batch shape: {metadata.shape}\")\n",
    "    print(f\"   - Labels batch shape:   {labels.shape}\")\n",
    "    break\n",
    "\n",
    "print(\"\\n‚úÖ Derm7pt data loading complete! Ready for training.\")\n",
    "print(\"\\nüí° Usage in your model:\")\n",
    "print(\"   for images, metadata, labels in train_loader:\")\n",
    "print(\"       # images: torch.Tensor of shape (batch_size, 3, 224, 224)\")\n",
    "print(\"       # metadata: torch.Tensor of shape (batch_size, num_metadata_features)\")\n",
    "print(\"       # labels: torch.Tensor of shape (batch_size,)\")\n",
    "print(\"       # Your training code here...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "085881ebbcda4def929a7afeba1586a2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "22ac965251184c9d92878d0fc141f9bf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "528ccd9132af44b3a2d08c17a371a74f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b525369342324ae79912f44225a5684f",
       "max": 22058321,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_8713a5c8e429475f80d66c0a3b0a4c5f",
       "tabbable": null,
       "tooltip": null,
       "value": 22058321
      }
     },
     "6eb4d407a2e14d3398ca31e8ef765ac9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_dcc805abc57648118fa371add11e6e53",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_085881ebbcda4def929a7afeba1586a2",
       "tabbable": null,
       "tooltip": null,
       "value": "model.safetensors:‚Äá100%"
      }
     },
     "8713a5c8e429475f80d66c0a3b0a4c5f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "98428bdcd8904e79aebaebf1a0adcf75": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a9cc0e6fdbd04830970252fecda17487": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_6eb4d407a2e14d3398ca31e8ef765ac9",
        "IPY_MODEL_528ccd9132af44b3a2d08c17a371a74f",
        "IPY_MODEL_b94952efad8c4fefbc83bd28ec9e1c51"
       ],
       "layout": "IPY_MODEL_22ac965251184c9d92878d0fc141f9bf",
       "tabbable": null,
       "tooltip": null
      }
     },
     "ac7f57fd95414ddaa4e393f2a78ee5d9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b525369342324ae79912f44225a5684f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b94952efad8c4fefbc83bd28ec9e1c51": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ac7f57fd95414ddaa4e393f2a78ee5d9",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_98428bdcd8904e79aebaebf1a0adcf75",
       "tabbable": null,
       "tooltip": null,
       "value": "‚Äá22.1M/22.1M‚Äá[00:05&lt;00:00,‚Äá3.77MB/s]"
      }
     },
     "dcc805abc57648118fa371add11e6e53": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
