{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf1cf7d-e12a-469e-b3c6-c37dee98db7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)  \n",
    "torch.backends.cudnn.deterministic = True  \n",
    "torch.backends.cudnn.benchmark = False  \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "IMAGE_DIR = 'D:\\\\PAD-UFES\\\\images'  \n",
    "METADATA_PATH = 'D:\\\\PAD-UFES\\\\metadata.csv'\n",
    "\n",
    "metadata = pd.read_csv(METADATA_PATH)\n",
    "\n",
    "def preprocess_metadata(metadata):\n",
    "    metadata = metadata.fillna('UNK')\n",
    "\n",
    "    boolean_cols = [\n",
    "        'smoke',\n",
    "        'drink',\n",
    "        'pesticide',\n",
    "        'skin_cancer_history',\n",
    "        'cancer_history',\n",
    "        'has_piped_water',\n",
    "        'has_sewage_system',\n",
    "        'itch',\n",
    "        'grew',\n",
    "        'hurt',\n",
    "        'changed',\n",
    "        'bleed',\n",
    "        'elevation',\n",
    "        'biopsed',\n",
    "    ]\n",
    "    # Ensure columns are strings and lowercase\n",
    "    for col in boolean_cols:\n",
    "        metadata[col] = metadata[col].astype(str).str.lower()\n",
    "    \n",
    "    # Map boolean columns to 1/0/-1\n",
    "    boolean_mapping = {'true': 1, 'false': 0, 'unk': -1}\n",
    "    for col in boolean_cols:\n",
    "        metadata[col] = metadata[col].map(boolean_mapping)\n",
    "    \n",
    "    # Handle categorical variables\n",
    "    categorical_cols = [\n",
    "        'background_father',\n",
    "        'background_mother',\n",
    "        'gender',\n",
    "        'region',\n",
    "        'diagnostic',\n",
    "    ]\n",
    "    # Convert categorical columns to string\n",
    "    for col in categorical_cols:\n",
    "        metadata[col] = metadata[col].astype(str)\n",
    "    \n",
    "    # One-hot encode categorical variables\n",
    "    metadata_encoded = pd.get_dummies(metadata[categorical_cols])\n",
    "    \n",
    "    # Normalize numerical variables\n",
    "    numerical_cols = ['age', 'fitspatrick', 'diameter_1', 'diameter_2']\n",
    "    # Ensure numerical columns are numeric\n",
    "    for col in numerical_cols:\n",
    "        metadata[col] = pd.to_numeric(metadata[col], errors='coerce')\n",
    "    # Fill NaNs in numerical columns with the mean\n",
    "    metadata[numerical_cols] = metadata[numerical_cols].fillna(metadata[numerical_cols].mean())\n",
    "    # Scale numerical columns\n",
    "    scaler = StandardScaler()\n",
    "    metadata_numeric = metadata[numerical_cols]\n",
    "    metadata_numeric_scaled = pd.DataFrame(\n",
    "        scaler.fit_transform(metadata_numeric), columns=numerical_cols\n",
    "    )\n",
    "    \n",
    "    # Combine all metadata features\n",
    "    metadata_processed = pd.concat(\n",
    "        [metadata_numeric_scaled.reset_index(drop=True),\n",
    "         metadata_encoded.reset_index(drop=True),\n",
    "         metadata[boolean_cols].reset_index(drop=True)], axis=1\n",
    "    )\n",
    "    \n",
    "    return metadata_processed\n",
    "\n",
    "# Preprocess metadata\n",
    "metadata_processed = preprocess_metadata(metadata)\n",
    "\n",
    "def get_image_paths(metadata, image_dir):\n",
    "    image_paths = []\n",
    "    for idx, row in metadata.iterrows():\n",
    "        filename = row['img_id']\n",
    "        # Ensure filename is a string\n",
    "        filename = str(filename)\n",
    "        # Check if filename has an extension\n",
    "        if not filename.endswith(('.jpg', '.jpeg', '.png')):\n",
    "            # Try common extensions\n",
    "            possible_extensions = ['.jpg', '.jpeg', '.png']\n",
    "            found = False\n",
    "            for ext in possible_extensions:\n",
    "                filepath = os.path.join(image_dir, filename + ext)\n",
    "                if os.path.isfile(filepath):\n",
    "                    image_paths.append(filepath)\n",
    "                    found = True\n",
    "                    break\n",
    "            if not found:\n",
    "                print(f\"Image file not found for ID: {filename}\")\n",
    "                image_paths.append(None)\n",
    "        else:\n",
    "            filepath = os.path.join(image_dir, filename)\n",
    "            if os.path.isfile(filepath):\n",
    "                image_paths.append(filepath)\n",
    "            else:\n",
    "                print(f\"Image file not found: {filepath}\")\n",
    "                image_paths.append(None)\n",
    "    metadata['ImagePath'] = image_paths\n",
    "    return metadata\n",
    "\n",
    "metadata = get_image_paths(metadata, IMAGE_DIR)\n",
    "\n",
    "# Remove entries with missing images\n",
    "metadata = metadata[metadata['ImagePath'].notnull()]\n",
    "metadata_processed = metadata_processed.loc[metadata.index].reset_index(drop=True)\n",
    "metadata = metadata.reset_index(drop=True)\n",
    "\n",
    "# Drop diagnostic-related columns from features\n",
    "diagnostic_cols = ['diagnostic_ACK', 'diagnostic_BCC', 'diagnostic_MEL', 'diagnostic_NEV', 'diagnostic_SCC', 'diagnostic_SEK']\n",
    "metadata_processed = metadata_processed.drop(columns=diagnostic_cols)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(metadata['diagnostic'])\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# Split data into features and labels\n",
    "X_meta = metadata_processed.reset_index(drop=True)\n",
    "X_img_paths = metadata['ImagePath'].reset_index(drop=True)\n",
    "y = pd.Series(y_encoded)\n",
    "\n",
    "X_train_meta, X_temp_meta, X_train_img_paths, X_temp_img_paths, y_train, y_temp = train_test_split(\n",
    "    X_meta,\n",
    "    X_img_paths,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "X_val_meta, X_test_meta, X_val_img_paths, X_test_img_paths, y_val, y_test = train_test_split(\n",
    "    X_temp_meta,\n",
    "    X_temp_img_paths,\n",
    "    y_temp,\n",
    "    test_size=0.5,\n",
    "    random_state=42,\n",
    "    stratify=y_temp\n",
    ")\n",
    "\n",
    "# Load augmented metadata + image paths\n",
    "aug_meta_df   = pd.read_csv(\"D:/PAD-UFES/augmented_metadata.csv\")\n",
    "aug_labels_df = pd.read_csv(\"D:/PAD-UFES/augmented_labels.csv\")\n",
    "\n",
    "# Combine augmented samples with training set\n",
    "X_train_meta_final = pd.concat([X_train_meta, aug_meta_df], ignore_index=True)\n",
    "X_train_img_paths_final = pd.concat([X_train_img_paths.reset_index(drop=True),\n",
    "                                     aug_labels_df['ImagePath']], ignore_index=True)\n",
    "y_train_final = pd.concat([y_train.reset_index(drop=True),\n",
    "                           aug_labels_df['Label']], ignore_index=True)\n",
    "\n",
    "class PADUFESDataset(Dataset):\n",
    "    def __init__(self, img_paths, meta_data, labels, transform=None):\n",
    "        self.img_paths = img_paths.reset_index(drop=True)\n",
    "        self.meta_data = meta_data.reset_index(drop=True)\n",
    "        self.labels = pd.Series(labels).reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        meta = torch.tensor(self.meta_data.iloc[idx].values.astype(np.float32))\n",
    "        label = torch.tensor(self.labels.iloc[idx], dtype=torch.long)\n",
    "        return image, meta, label\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # Random horizontal flip\n",
    "    transforms.RandomRotation(70),          \n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Color jitter\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = PADUFESDataset(X_train_img_paths_final, X_train_meta_final, y_train_final, transform=train_transform)\n",
    "val_dataset = PADUFESDataset(X_val_img_paths, X_val_meta, y_val, transform=val_test_transform)\n",
    "test_dataset = PADUFESDataset(X_test_img_paths, X_test_meta, y_test, transform=val_test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"\\n✅ Loaded Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948f75c2-ea9b-4ab3-b6af-910f0d213eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_processed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd643e33-86a3-4de9-b06f-9c702a124a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360bfa12-b774-4b8a-bae9-c0c698db5f98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm  \n",
    "import torch.nn.functional as F\n",
    "\n",
    "class EarlyFusionModel(nn.Module):\n",
    "    def __init__(self, input_dim_meta, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embed metadata to smaller spatial dimensions first\n",
    "        self.meta_embed = nn.Sequential(\n",
    "            nn.Linear(input_dim_meta, 64 * 64),  # Updated for mobilevit's smaller receptive field\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64 * 64),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        # Load MobileViT model\n",
    "        self.mobilevit = timm.create_model(\"mobilevit_s.cvnets_in1k\", pretrained=True, num_classes=num_classes)\n",
    "        \n",
    "        # Inspect the model to identify the first conv layer\n",
    "        # Modify the first conv layer to accept additional channel\n",
    "        first_conv = self.mobilevit.stem.conv  # `stem.conv` is the correct initial layer\n",
    "        self.mobilevit.stem.conv = nn.Conv2d(4, first_conv.out_channels, \n",
    "                                             kernel_size=first_conv.kernel_size, \n",
    "                                             stride=first_conv.stride, \n",
    "                                             padding=first_conv.padding, \n",
    "                                             bias=first_conv.bias)\n",
    "        \n",
    "        # Initialize new channel weights\n",
    "        with torch.no_grad():\n",
    "            self.mobilevit.stem.conv.weight.data[:, :3] = first_conv.weight.data\n",
    "            # Initialize the new channel with smaller weights to prevent dominating\n",
    "            self.mobilevit.stem.conv.weight.data[:, 3:] = first_conv.weight.data.mean(dim=1, keepdim=True) * 0.1\n",
    "\n",
    "    def forward(self, img, meta):\n",
    "        # Reshape metadata to image-like format\n",
    "        batch_size = img.shape[0]\n",
    "        meta_reshaped = self.meta_embed(meta).view(batch_size, 1, 64, 64)\n",
    "        \n",
    "        # Upsample to match image dimensions\n",
    "        meta_upsampled = F.interpolate(meta_reshaped, \n",
    "                                       size=(224, 224),  # MobileViT expects 256x256\n",
    "                                       mode='bilinear', \n",
    "                                       align_corners=False)\n",
    "        \n",
    "        # Early fusion\n",
    "        combined_input = torch.cat([img, meta_upsampled], dim=1)\n",
    "        \n",
    "        # Process through modified MobileViT\n",
    "        out = self.mobilevit(combined_input)\n",
    "        return out\n",
    "\n",
    "# Assuming X_train_meta and other variables are defined\n",
    "input_dim_meta = X_train_meta.shape[1]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = EarlyFusionModel(input_dim_meta, num_classes).to(device)\n",
    "\n",
    "from torchinfo import summary\n",
    "summary(model=model, \n",
    "        input_size=[(16, 3, 224, 224), (16, input_dim_meta)],  # Updated for MobileViT input size\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a8b844-ef1a-44cd-8ad1-9b6d630b935a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_dim_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7279bf2a-d008-4abe-b887-366efe15d700",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from PIL import Image\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "def test(model, loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, metas, labels in loader:\n",
    "            imgs, metas, labels = imgs.to(device), metas.to(device), labels.to(device)\n",
    "            outputs = model(imgs, metas)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    return all_labels, all_preds\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt'):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_accuracy_max = -np.inf\n",
    "        \n",
    "    def __call__(self, val_acc, model):\n",
    "        score = val_acc\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_acc, model)\n",
    "        elif score <= self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_acc, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_acc, model):\n",
    "        if self.verbose:\n",
    "            print(f'Validation accuracy increased ({self.val_accuracy_max:.6f} --> {val_acc:.6f}). Saving model...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_accuracy_max = val_acc\n",
    "\n",
    "def train(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc='Training')\n",
    "    for images, meta, labels in pbar:\n",
    "        images, meta, labels = images.to(device), meta.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images, meta)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Optional: Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{running_loss/total:.4f}',\n",
    "            'acc': f'{100.*correct/total:.2f}%'\n",
    "        })\n",
    "    \n",
    "    return running_loss/len(train_loader), correct/total\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, meta, labels in val_loader:\n",
    "            images, meta, labels = images.to(device), meta.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images, meta)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    return running_loss/len(val_loader), correct/total\n",
    "\n",
    "def train_model_with_scheduler_and_checkpoint(\n",
    "    model, train_loader, val_loader, optimizer, criterion, device, \n",
    "    epochs=20, patience=5, scheduler_patience=5, checkpoint_dir='checkpoints'):\n",
    "    \n",
    "    # Create checkpoint directory if it doesn't exist\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, 'mobilevit1.pt')\n",
    "    \n",
    "    early_stopping = EarlyStopping(\n",
    "        patience=patience, \n",
    "        verbose=True, \n",
    "        path=checkpoint_path\n",
    "    )\n",
    "    scheduler = ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='max',  # Changed to max since we're monitoring accuracy\n",
    "        patience=scheduler_patience, \n",
    "        verbose=True,\n",
    "        factor=0.1,\n",
    "        min_lr=1e-6\n",
    "    )\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [], 'val_loss': [],\n",
    "        'train_acc': [], 'val_acc': [],\n",
    "        'lr': []\n",
    "    }\n",
    "    \n",
    "    best_model_epoch = None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f'\\nEpoch {epoch+1}/{epochs}')\n",
    "        \n",
    "        # Training phase\n",
    "        train_loss, train_acc = train(model, train_loader, optimizer, criterion, device)\n",
    "        \n",
    "        # Validation phase\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        # Update history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['lr'].append(optimizer.param_groups[0]['lr'])\n",
    "        \n",
    "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
    "        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "        \n",
    "        # Update scheduler based on validation accuracy\n",
    "        scheduler.step(val_acc)\n",
    "        \n",
    "        # Early stopping check\n",
    "        early_stopping(val_acc, model)\n",
    "        if val_acc > early_stopping.val_accuracy_max:\n",
    "            best_model_epoch = epoch + 1\n",
    "            \n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load(checkpoint_path))\n",
    "    \n",
    "    # Plot training curves\n",
    "    plot_training_curves_with_checkpoint(history, best_model_epoch)\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "def plot_training_curves_with_checkpoint(history, best_model_epoch):\n",
    "    epochs_range = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Loss curves\n",
    "    ax1.plot(epochs_range, history['train_loss'], label='Training Loss')\n",
    "    ax1.plot(epochs_range, history['val_loss'], label='Validation Loss')\n",
    "    if best_model_epoch:\n",
    "        ax1.axvline(best_model_epoch, color='r', linestyle='--', label='Best Model')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Training and Validation Loss')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Accuracy curves\n",
    "    ax2.plot(epochs_range, history['train_acc'], label='Training Accuracy')\n",
    "    ax2.plot(epochs_range, history['val_acc'], label='Validation Accuracy')\n",
    "    if best_model_epoch:\n",
    "        ax2.axvline(best_model_epoch, color='r', linestyle='--', label='Best Model')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.set_title('Training and Validation Accuracy')\n",
    "    ax2.legend()\n",
    "    \n",
    "    # Learning rate curve\n",
    "    ax3.plot(epochs_range, history['lr'], label='Learning Rate')\n",
    "    if best_model_epoch:\n",
    "        ax3.axvline(best_model_epoch, color='r', linestyle='--', label='Best Model')\n",
    "    ax3.set_xlabel('Epoch')\n",
    "    ax3.set_ylabel('Learning Rate')\n",
    "    ax3.set_title('Learning Rate Schedule')\n",
    "    ax3.set_yscale('log')\n",
    "    ax3.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672ef575-4ad7-478b-afc2-51750bd750ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "def set_random_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# Function to evaluate test metrics\n",
    "def evaluate_test_metrics(model, test_loader, device):\n",
    "    true_labels, pred_labels = test(model, test_loader, device)\n",
    "    acc = accuracy_score(true_labels, pred_labels)\n",
    "    precision = precision_score(true_labels, pred_labels, average='macro')\n",
    "    recall = recall_score(true_labels, pred_labels, average='macro')\n",
    "    f1 = f1_score(true_labels, pred_labels, average='macro')\n",
    "    return acc, precision, recall, f1\n",
    "\n",
    "# Placeholder for results\n",
    "results = {\n",
    "    \"accuracy\": [],\n",
    "    \"precision\": [],\n",
    "    \"recall\": [],\n",
    "    \"f1_score\": []\n",
    "}\n",
    "\n",
    "best_accuracy = 0.0\n",
    "best_model_state = None\n",
    "\n",
    "# Run experiment for 3 random seeds\n",
    "seeds = [42, 123, 569]  # Example random seeds\n",
    "for seed in seeds:\n",
    "    print(f\"\\nTraining with random seed: {seed}\")\n",
    "    set_random_seed(seed)\n",
    "    \n",
    "    # Reinitialize model, optimizer, and criterion\n",
    "    model = EarlyFusionModel(input_dim_meta, num_classes).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Train the model\n",
    "    model, history = train_model_with_scheduler_and_checkpoint(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        device=device,\n",
    "        epochs=100,\n",
    "        patience=7,\n",
    "        scheduler_patience=3,\n",
    "        checkpoint_dir='D:\\\\PAD-UFES\\\\checkpoints'\n",
    "    )\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    acc, precision, recall, f1 = evaluate_test_metrics(model, test_loader, device)\n",
    "    print(f\"Seed {seed}: Accuracy={acc:.4f}, Precision={precision:.4f}, Recall={recall:.4f}, F1 Score={f1:.4f}\")\n",
    "    \n",
    "    # Save metrics\n",
    "    results[\"accuracy\"].append(acc)\n",
    "    results[\"precision\"].append(precision)\n",
    "    results[\"recall\"].append(recall)\n",
    "    results[\"f1_score\"].append(f1)\n",
    "    \n",
    "    # Update the best model\n",
    "    if acc > best_accuracy:\n",
    "        best_accuracy = acc\n",
    "        best_model_state = model.state_dict()\n",
    "\n",
    "# Compute average and standard deviation\n",
    "metrics_summary = {}\n",
    "for metric, values in results.items():\n",
    "    avg = np.mean(values)\n",
    "    std_dev = np.std(values)\n",
    "    metrics_summary[metric] = (avg, std_dev)\n",
    "    print(f\"{metric.capitalize()}: Mean={avg:.4f}, StdDev={std_dev:.4f}\")\n",
    "\n",
    "# Save the best model\n",
    "print(f\"Best model achieved an accuracy of {best_accuracy:.4f}\")\n",
    "torch.save(best_model_state, 'D:\\\\PAD-UFES\\\\best_early_fusion_mobilevitDA.pth')\n",
    "\n",
    "model = EarlyFusionModel(input_dim_meta, num_classes).to(device)\n",
    "model.load_state_dict(torch.load('D:\\\\PAD-UFES\\\\best_early_fusion_mobilevitDA.pth'))\n",
    "model.eval()\n",
    "print(\"Best model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1ce06a-3d1c-48b3-9ff9-6417843e378f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EarlyFusionModel(input_dim_meta, num_classes).to(device)\n",
    "model.load_state_dict(torch.load('D:\\\\PAD-UFES\\\\best_early_fusion_mobilevitDA.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e445513-187f-4d9a-a981-8a88908ab80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "def test(model, loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, metas, labels in loader:\n",
    "            imgs, metas, labels = imgs.to(device), metas.to(device), labels.to(device)\n",
    "            outputs = model(imgs, metas)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    return all_labels, all_preds\n",
    "\n",
    "true_labels, pred_labels = test(model, test_loader, device)\n",
    "\n",
    "class_names = label_encoder.classes_\n",
    "report = classification_report(true_labels, pred_labels, digits=4,target_names=class_names)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "cm = confusion_matrix(true_labels, pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7576d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import numpy as np\n",
    "from ptflops import get_model_complexity_info\n",
    "\n",
    "\n",
    "\n",
    "meta_dim = 59\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "params_m = count_parameters(model) / 1e6\n",
    "print(f\"Total Trainable Parameters: {params_m:.3f} M\")\n",
    "\n",
    "\n",
    "\n",
    "class FusionWrapper(nn.Module):\n",
    "    \"\"\"\n",
    "    Wraps the multimodal model so ptflops sees a single image input.\n",
    "    Metadata is synthesized inside forward().\n",
    "    \"\"\"\n",
    "    def __init__(self, model, meta_dim=59):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.meta_dim = meta_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch = x.shape[0]\n",
    "        dummy_meta = torch.randn(batch, self.meta_dim).to(x.device)\n",
    "        return self.model(x, dummy_meta)\n",
    "\n",
    "wrapper = FusionWrapper(model, meta_dim).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    flops, params = get_model_complexity_info(\n",
    "        wrapper,\n",
    "        (3, 224, 224),          # single input: image only\n",
    "        as_strings=False,\n",
    "        print_per_layer_stat=False,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "flops_g = flops / 1e9\n",
    "print(f\"FLOPs: {flops_g:.3f} GFLOPs\")\n",
    "\n",
    "\n",
    "\n",
    "def measure_gpu_latency(model, device, meta_dim=59, runs=200, warmup=30):\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"CUDA not available, skipping GPU latency.\")\n",
    "        return None, None, None\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # Fixed dummy inputs\n",
    "    dummy_img = torch.randn(1, 3, 224, 224, device=device)\n",
    "    dummy_meta = torch.randn(1, meta_dim, device=device)\n",
    "\n",
    "    # Warm-up\n",
    "    for _ in range(warmup):\n",
    "        _ = model(dummy_img, dummy_meta)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    times = []\n",
    "\n",
    "    start_event = torch.cuda.Event(enable_timing=True)\n",
    "    end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    for _ in range(runs):\n",
    "        start_event.record()\n",
    "        _ = model(dummy_img, dummy_meta)\n",
    "        end_event.record()\n",
    "        torch.cuda.synchronize()\n",
    "        elapsed_ms = start_event.elapsed_time(end_event)  # ms\n",
    "        times.append(elapsed_ms)\n",
    "\n",
    "    times = np.array(times)\n",
    "    mean = times.mean()\n",
    "    std = times.std()\n",
    "    fps = 1000.0 / mean\n",
    "    return mean, std, fps\n",
    "\n",
    "gpu_mean, gpu_std, gpu_fps = measure_gpu_latency(model, device, meta_dim=meta_dim)\n",
    "\n",
    "if gpu_mean is not None:\n",
    "    print(f\"GPU Latency: {gpu_mean:.3f} ± {gpu_std:.3f} ms\")\n",
    "    print(f\"GPU FPS: {gpu_fps:.2f}\")\n",
    "else:\n",
    "    print(\"GPU metrics not computed (no CUDA).\")\n",
    "\n",
    "\n",
    "\n",
    "def measure_cpu_latency(model, meta_dim=59, runs=100, warmup=20):\n",
    "    model_cpu = model.cpu()\n",
    "    model_cpu.eval()\n",
    "\n",
    "    dummy_img = torch.randn(1, 3, 224, 224)\n",
    "    dummy_meta = torch.randn(1, meta_dim)\n",
    "\n",
    "    # Warm-up\n",
    "    for _ in range(warmup):\n",
    "        _ = model_cpu(dummy_img, dummy_meta)\n",
    "\n",
    "    times = []\n",
    "    for _ in range(runs):\n",
    "        start = time.perf_counter()\n",
    "        _ = model_cpu(dummy_img, dummy_meta)\n",
    "        end = time.perf_counter()\n",
    "        times.append((end - start) * 1000.0)  # ms\n",
    "\n",
    "    times = np.array(times)\n",
    "    mean = times.mean()\n",
    "    std = times.std()\n",
    "    fps = 1000.0 / mean\n",
    "    return mean, std, fps\n",
    "\n",
    "cpu_mean, cpu_std, cpu_fps = measure_cpu_latency(model, meta_dim=meta_dim)\n",
    "print(f\"CPU Latency: {cpu_mean:.3f} ± {cpu_std:.3f} ms\")\n",
    "print(f\"CPU FPS: {cpu_fps:.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "efficiency_stats = {\n",
    "    \"params_M\": params_m,\n",
    "    \"flops_G\": flops_g,\n",
    "    \"gpu_latency_ms_mean\": gpu_mean,\n",
    "    \"gpu_latency_ms_std\": gpu_std,\n",
    "    \"gpu_fps\": gpu_fps,\n",
    "    \"cpu_latency_ms_mean\": cpu_mean,\n",
    "    \"cpu_latency_ms_std\": cpu_std,\n",
    "    \"cpu_fps\": cpu_fps,\n",
    "}\n",
    "\n",
    "print(\"\\nEfficiency stats dict (for your table):\")\n",
    "for k, v in efficiency_stats.items():\n",
    "    print(f\"{k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cd483f-39d8-4f5b-82fe-a8756484ad73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "conf_matrix = confusion_matrix(true_labels, pred_labels, normalize=\"true\")\n",
    "\n",
    "# Display confusion matrix (black and white)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, cmap=\"gray\", fmt=\".2f\", xticklabels=class_names, yticklabels=class_names, cbar=True)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Normalized Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c59dcc-4a74-43c5-804e-9cb0835d533b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "n_class = 6\n",
    "\n",
    "for i in range(n_class):\n",
    "    fpr[i], tpr[i], _ = roc_curve(np.array(true_labels) == i, np.array(pred_labels) == i)\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "colors = ['orange', 'green', 'red', 'blue', 'purple', 'pink']\n",
    "\n",
    "for i in range(n_class):\n",
    "    plt.plot(fpr[i], tpr[i], color=colors[i], lw=2, label=f'ROC curve (AUC = {roc_auc[i]:.2f}) for {class_names[i]}')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('(ROC)Receiver Operating Characteristic')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
